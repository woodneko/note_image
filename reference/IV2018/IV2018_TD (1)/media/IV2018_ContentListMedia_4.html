<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
 <head>
  <title>IV 2018</title>
  
  <link href="style.css" rel="stylesheet" type="text/css" media="screen" />

<script language="JavaScript">

function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
</script>


</head>

<body leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">
			   
<div class="c" id="TheTop"><br></div>
<table border="0" cellspacing="0" cellpadding="1" width="85%" nowrap style="margin: auto">
<tr><td>
<br>
<h2>Technical Program for Friday June 29, 2018</h2>
<hr>
<br>
</td></tr>
</table>

<div class="c">

                  <span style="color:gray ">To show or hide the keywords and abstract of a paper (if available), click on the paper title</span><br>
                  <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">Open all abstracts</a>&nbsp;&nbsp;
                  <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">Close all abstracts</a>
               
</div>

<div class="c">
<table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frre"><b>FrRE</b></a></td>
               <td class="r">Dining Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frre" title="Click to go to the Program at a Glance"><b>Registration-29June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frkn"><b>FrKN</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frkn" title="Click to go to the Program at a Glance"><b>Keynote-29June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#38688" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Waterloo Univ</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frb1b"><b>FrB1B</b></a></td>
               <td class="r">Lobby</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frb1b" title="Click to go to the Program at a Glance"><b>Break1-29June</b></a></td>
               <td class="r">Break</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frosal"><b>FrOSAL</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frosal" title="Click to go to the Program at a Glance"><b>Sensor and Data Fusion</b></a></td>
               <td class="r">Plenary Session</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#12021" title="Click to go to the Author Index">Yang, Ming</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>

<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosal_01">10:20-10:40, Paper FrOSAL.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0172.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('172'); return false" title="Click to show or hide the keywords and abstract">A Speed Guide Model for Collision Avoidance in Non-Signalized Intersections Based on Reduplicate Game Theory</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36020" title="Click to go to the Author Index">Cheng, Chijung</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32600" title="Click to go to the Author Index">Yang, Zhuo</a></td><td class="r">Beijing Inst. of Aerospace Control Devices</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14648" title="Click to go to the Author Index">Yao, Danya</a></td><td class="r">Tsinghua Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab172" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> In consideration of the convenience of drivers’ operations, a velocity guide driving model for collision avoidance in non-signalized intersection is proposed. Based on reduplicate game theory, a profit function consists of safety, rapidity, controlling indicators is redefined. Pareto optimality is adopted to obtain the strategies which maximize the profit for the whole game. Simulation results show that the model is effective and gives a more comfortable vehicle-cross process.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosal_02">10:40-11:00, Paper FrOSAL.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0435.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('435'); return false" title="Click to show or hide the keywords and abstract">Robust Camera Lidar Sensor Fusion Via Deep Gated Information Fusion Network</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34944" title="Click to go to the Author Index">Kim, Jaekyum</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37134" title="Click to go to the Author Index">Choi, Jaehyung</a></td><td class="r">Phantom AI, Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37132" title="Click to go to the Author Index">Kim, YaeCheol</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37131" title="Click to go to the Author Index">Koh, Junho</a></td><td class="r">Hanyang Univ. Signal Processing Machine Learning Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23315" title="Click to go to the Author Index">Chung, Chung Choo</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34467" title="Click to go to the Author Index">Choi, Jun Won</a></td><td class="r">Hanyang Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab435" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> In this paper, we introduce a new deep learning architecture for camera and Lidar sensor fusion. The proposed scheme performs 2D object detection using the RGB camera image and the depth, height, and intensity images generated by projecting the 3D Lidar point cloud into camera image plane. The proposed object detector consists of two convolutional neural networks (CNNs) that process the RGB and Lidar images separately as well as the fusion network that combines the feature maps produced at the intermediate layers of the CNNs. We aim to develop a robust object detector that maintains good object detection accuracy even when the quality of the sensor signals is degraded for object detection. Towards this end, we devise the gated fusion unit (GFU) that adjusts the contribution of the feature maps generated by two CNN structures via gating mechanism. Using the GFU, the proposed object detector can fuse the high level feature maps drawn from two modalities with appropriate weights to achieve robust performance. Experiments conducted on the challenging KITTI benchmark show that the proposed camera and Lidar fusion network outperforms the conventional sensor fusion methods even when either of the camea and Lidar sensor signals is corrupted by missing data, occlusion, noise, and illumination change.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosal_03">11:00-11:20, Paper FrOSAL.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0064.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('64'); return false" title="Click to show or hide the keywords and abstract">End-To-End Driving Activities and Secondary Tasks Recognition Using Deep Convolutional Neural Network and Transfer Learning (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35959" title="Click to go to the Author Index">Xing, Yang</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30472" title="Click to go to the Author Index">Lv, Chen</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38688" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Waterloo Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36022" title="Click to go to the Author Index">Velenis, Efstathios</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10337" title="Click to go to the Author Index">Wang, Fei-Yue</a></td><td class="r">Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#41209" title="Click to go to the Author Index">Tang, Jianlin</a></td><td class="r">Jiangsu Xcmg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#41210" title="Click to go to the Author Index">Liu, Hong</a></td><td class="r">Jiangsu Xcmg</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab64" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a></span><br>
                              <strong>Abstract:</strong> Drivers’ decision and their corresponding behaviors are important aspects that can affect the driving safety and it is beneficial to understand the driver behaviors in real time. In this study, an end-to-end driving-related tasks recognition system is proposed. Specifically, seven common driving activities are identified, which are normal driving, right mirror checking, rear mirror checking, left mirror checking, using in-vehicle video device, texting, and answering a mobile phone. Among these, the first four activities are regarded as normal driving tasks, while the rest three are divided into distraction. The images are collected using a consumer range camera, namely, Kinect. In total, five drivers are involved in the naturalistic data collection. Before training the identification model, the raw images are first segmented using a Gaussian mixture model (GMM) to extract the driver from the background. Then, a pre-trained deep convolutional neural network (CNN) model is trained to classify the behaviors, which directly take the processed RGB images as input and outputs the identified label. In this work, the AlexNet is selected as the pre-trained CNN model. Then, to reduce the training cost, the transfer learning mechanism is applied to the CNN model. An average of 79% detection accuracy is achieved for the seven driving tasks. The proposed integration model can be used as a low-cost driver distraction and dangerous tasks recognition model.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosal_04">11:20-11:40, Paper FrOSAL.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0346.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('346'); return false" title="Click to show or hide the keywords and abstract">Online Camera LiDAR Fusion and Object Detection on Hybrid Data for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35015" title="Click to go to the Author Index">Banerjee, Koyel</a></td><td class="r">BMW</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36576" title="Click to go to the Author Index">Notz, Dominik</a></td><td class="r">BMW of North America</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36882" title="Click to go to the Author Index">Windelen, Johannes</a></td><td class="r">BMW</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36885" title="Click to go to the Author Index">Gavarraju, Sumanth Nirmal</a></td><td class="r">BMW of North America</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36883" title="Click to go to the Author Index">He, Mingkang</a></td><td class="r">BMW of North America, Johns Hopkins Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab346" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Environment perception for autonomous driving traditionally uses sensor fusion to combine the object detections from various sensors mounted on the car into a single representation of the environment. Non-calibrated sensors result in artifacts and aberration in the environment model, which makes tasks like free-space detection more challenging. In this study, we improve the LiDAR and camera fusion approach of Levinson and Thrun. We rely on intensity discontinuities and erosion and dilation of the edge image for increased robustness against shadows and visual patterns, which is a recurring problem in point cloud related work. Furthermore, we use a gradient-free optimizer instead of an exhaustive grid search to find the extrinsic calibration. Hence, our fusion pipeline is lightweight and able to run in real-time on a computer in the car. For the detection task, we modify the Faster R-CNN architecture to accommodate hybrid LiDAR-camera data for improved object detection and classification. We test our algorithms on the KITTI data set and locally collected urban scenarios. We also give an outlook on how radar can be added to the fusion pipeline via velocity matching.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosal_05">11:40-12:00, Paper FrOSAL.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0596.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('596'); return false" title="Click to show or hide the keywords and abstract">Predicting Trajectories of Vehicles Using Large-Scale Motion Priors</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37550" title="Click to go to the Author Index">M S, Suraj</a></td><td class="r">Georgia Inst. of Tech. Blue Vision Labs</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37618" title="Click to go to the Author Index">Grimmett, Hugo</a></td><td class="r">Blue Vision Labs</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37617" title="Click to go to the Author Index">Platinsky, Lukas</a></td><td class="r">Blue Vision Labs</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27616" title="Click to go to the Author Index">Ondruska, Peter</a></td><td class="r">Univ. of Oxford</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab596" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> We present a simple yet effective paradigm to accurately predict the future trajectories of observed vehicles in dense city environments. We equipped a large fleet of cars with cameras and performed city-scale structure-from-motion to accurately reconstruct 10M positions frin their trajectories spanning over 1000h of driving.<p>We demonstrate that this information can be used as a powerful high-fidelity prior to predict future trajectories of newly observed vehicles in the area without the need for any knowledge of road infrastructure or vehicle motion models. By relating the current position of the observed car to a large dataset of the previously exhibited motion in the area we can directly perform prediction of its future position.<p>We evaluate our method on two large-scale data sets from San Francisco and New York City and demonstrate an order of magnitude improvement compared to a linear-motion based method. We also demonstrate that the performance naturally improves with the amount of data and ultimately yields a system that can accurately predict vehicle motion in challenging situations across extremes in traffic, time, and weather conditions.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frlb"><b>FrLB</b></a></td>
               <td class="r">Dining Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frlb" title="Click to go to the Program at a Glance"><b>Lunch-29June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frosbl"><b>FrOSBL</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frosbl" title="Click to go to the Program at a Glance"><b>Decision and Control</b></a></td>
               <td class="r">Plenary Session</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#20181" title="Click to go to the Author Index">Li, Shengbo Eben</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td>Co-Chair: <a href="IV2018_AuthorIndexMedia.html#34467" title="Click to go to the Author Index">Choi, Jun Won</a></td><td class="r">Hanyang Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosbl_01">13:30-13:50, Paper FrOSBL.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0068.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('68'); return false" title="Click to show or hide the keywords and abstract">Optimization of the Braking Strategy for an Emergency Braking System by the Application of Machine Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35891" title="Click to go to the Author Index">Schratter, Markus</a></td><td class="r">Virtual Vehicle Res. Center</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36049" title="Click to go to the Author Index">Amler, Sabine</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36050" title="Click to go to the Author Index">Daman, Paul</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36051" title="Click to go to the Author Index">Watzenig, Daniel</a></td><td class="r">Virtual Vehicle Res. Center</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab68" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> This paper explores a methodology whereby accident data is directly used to develop a braking strategy for an autonomous emergency braking system. Future vehicles will be equipped with additional technologies. Detailed information about accidents or critical situations can be recorded. If enough recorded data from critical situations are available, such data could be used to improve Active Safety Systems. In our approach, we do not model the behavior of pedestrians or drivers. The idea is to use the capability of machine learning to get the behaviors out of traffic data. Machine learning is used to derive the function design for an emergency braking system for pedestrians. Generated traffic scenarios are used to review the methodology. Random Forests and Neural Networks are used for the function designs and the learned function designs are compared with a reference implementation.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosbl_02">13:50-14:10, Paper FrOSBL.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0302.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('302'); return false" title="Click to show or hide the keywords and abstract">Continuous Decision Making for On-Road Autonomous Driving under Uncertain and Interactive Environments</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35170" title="Click to go to the Author Index">Chen, Jianyu</a></td><td class="r">UC Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36754" title="Click to go to the Author Index">Tang, Chen</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36784" title="Click to go to the Author Index">Xin, Long</a></td><td class="r">1 Tsinghua Univ. 2 Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20181" title="Click to go to the Author Index">Li, Shengbo Eben</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab302" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> Although autonomous driving techniques have achieved great improvements, challenges still exist in decision making for variety of different scenarios under uncertain and interactive environments. A good decision maker must satisfy the following requirements: (1) Be in a generic and unified form to cover as more scenarios as possible. (2) Be able to interact properly with other moving obstacles under the uncertainty of their motions. In this paper, the continuous decision making (CDM) framework is proposed to formulate different driving scenarios in a unified way, which encodes the high level decision making information into a continuous reference trajectory that can be naturally combined with a lower level trajectory planner. Within the framework, a maximum interaction defensive policy (MIDP) is proposed, which calculates the best action to interact with stochastic moving obstacles while guaranteeing safety. The method is applied to a ramp merging scenario and the stochastic behavior models of the surrounding vehicles are learned from the NGSIM dataset. Simulations are shown to visualize and analyze the results.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosbl_03">14:10-14:30, Paper FrOSBL.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0483.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('483'); return false" title="Click to show or hide the keywords and abstract">Turn-By-Turn Intelligent Manoeuvring of Driverless Taxis: A Recursive Value Model Enhanced by Reinforcement Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31575" title="Click to go to the Author Index">Yang, Bo</a></td><td class="r">Inst. of High Performance Computing</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37307" title="Click to go to the Author Index">Li, Qianxiao</a></td><td class="r">Inst. of High Performance Computing</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab483" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> We develop a protocol for constructing intelligent manoeuvring of driverless taxis - especially for empty taxis - with the aim of optimising the efficiency of the taxi system (e.g. minimising the average waiting time of the commuters). The manoeuvring is formulated as a stochastic policy matrix giving optimal choices on which way to go for taxis at any locations, based on the historical patterns of commuter origin and destination distribution. A recursive value (RV) model is formulated for turn-by-turn navigation. The policy matrix is solved by a combination of the RV model and reinforcement learning, together with a comprehensive agent-based microscopic simulation platform we have developed. We show that even for very large road networks, very efficient manoeuvring algorithms can be readily found with reinforcement learning, especially with our RV model as the initial state. Such algorithms are indispensable for the fleet of driverless taxis, and can also give helpful recommendations to human drivers operating taxis. The general framework of our protocol allows the construction of optimal manoeuvring algorithms for a wide range of road networks in a systematic way.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosbl_04">14:30-14:50, Paper FrOSBL.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0584.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('584'); return false" title="Click to show or hide the keywords and abstract">Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37539" title="Click to go to the Author Index">Ma, Xiaobai</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28999" title="Click to go to the Author Index">Driggs-Campbell, Katherine</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18816" title="Click to go to the Author Index">Kochenderfer, Mykel</a></td><td class="r">Stanford Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab584" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosbl_05">14:50-15:10, Paper FrOSBL.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0575.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('575'); return false" title="Click to show or hide the keywords and abstract">Sequence-To-Sequence Prediction of Vehicle Trajectory Via LSTM Encoder-Decoder Architecture</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35945" title="Click to go to the Author Index">Park, Seong Hyeon</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34946" title="Click to go to the Author Index">Kim, ByeoungDo</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25222" title="Click to go to the Author Index">Kang, Chang Mook</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23315" title="Click to go to the Author Index">Chung, Chung Choo</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34467" title="Click to go to the Author Index">Choi, Jun Won</a></td><td class="r">Hanyang Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab575" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> In this paper, we propose a deep learning based vehicle trajectory prediction technique which can generate the future trajectory sequence of surrounding vehicles in real time. We employ the encoder-decoder architecture which analyzes the pattern underlying in the past trajectory using the long short-term memory (LSTM) based encoder and generates the future trajectory sequence using the LSTM based decoder. This structure produces the K most likely trajectory candidates over occupancy grid map by employing the {it beam search} technique which keeps the K locally best candidates from the decoder output. The experiments conducted on highway traffic scenarios show that the prediction accuracy of the proposed method is significantly higher than the conventional trajectory prediction techniques.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frosbl_06">15:10-15:30, Paper FrOSBL.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0388.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('388'); return false" title="Click to show or hide the keywords and abstract">Visual Place Recognition in Long-Term and Large-Scale Environment Based on CNN Feature</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33022" title="Click to go to the Author Index">Zhu, Jianliang</a></td><td class="r">Univ. of Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13802" title="Click to go to the Author Index">Ai, Yunfeng</a></td><td class="r">Univ. of Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22302" title="Click to go to the Author Index">Tian, Bin</a></td><td class="r">Chinese Acad. of Sciences Inst. of Automation</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24691" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Lancaster Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab388" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> With the universal application of camera in intelligent vehicles, visual place recognition has become a major problem in intelligent vehicle localization. The traditional solution is to make visual description of place images using hand-crafted feature for matching places, but this description method is not very good for extreme variability, especially for seasonal transformation. In this paper, we propose a new method based on convolutional neural network (CNN), by putting images into the pre-trained network model to get automatically learned image descriptors, and through some operations of pooling, fusion and binarization to optimize them, then the similarity result of place recognition is presented with the Hamming distance of the place sequence. In the experimental part, we compare our method with some state-of-the-art algorithms, FABMAP, ABLE-M and SeqSLAM, to illustrate its advantages. The experimental results show that our method based on CNN achieves better performance than other methods on the representative public datasets.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frb2b"><b>FrB2B</b></a></td>
               <td class="r">Lobby</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frb2b" title="Click to go to the Program at a Glance"><b>Break2-29June</b></a></td>
               <td class="r">Break</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst3"><b>FrPS-SST3</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst3" title="Click to go to the Program at a Glance"><b>Autonomous Driving Control-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_01">16:00-18:00, Paper FrPS-SST3.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0631.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('631'); return false" title="Click to show or hide the keywords and abstract">Synchronizing Multiple Data Streams by Time for Vehicle Control</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36909" title="Click to go to the Author Index">Masih, Siddharth</a></td><td class="r">UC Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36910" title="Click to go to the Author Index">Hart, Jordan</a></td><td class="r">Univ. of California Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37611" title="Click to go to the Author Index">Singhal, Parth</a></td><td class="r">1997</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36922" title="Click to go to the Author Index">Hornauer, Sascha</a></td><td class="r">International Computer Science Inst. UC Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab631" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> New research on autonomous driving is beginning to incorporate remote, relative-to-vehicle, environmental data. Few protocols exist to deliver remote data to autonomous vehicles --- for example observation meta-data or sensor streams such as camera, LIDAR, and etc. Researchers bypass this problem by using a single Robot OS controller or rudimentary streaming pipes. However, with recent advancements in vehicle to everything (V2X) protocols, including better LTE and millimeter wave (MMwave) communications in mobile environments, a dedicated multiple-source streaming protocol is increasingly necessary. For this reason, we developed a protocol that allows multiple and distinct transmitters of data to stream sensor information to a single receiver with features including session management, encryption, and jitter regulation. The Synchronized Buffer proposed herein seeks to synchronize data frames by their time stamps, in the spirit of the Real-Time Transport Protocol (RTP) used in VoIP. We validated the proposed time synchronization algorithm by streaming data at different simulated data frames per second. Quality of service (QoS) and potential security flaws are also explored. Such a dedicated streaming protocol addresses current problems by allowing vehicles to integrate previously unavailable remote data sources into critical autonomous decision making.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_02">16:00-18:00, Paper FrPS-SST3.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0515.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('515'); return false" title="Click to show or hide the keywords and abstract">Smooth Behavioral Estimation for Ramp Merging Control in Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31608" title="Click to go to the Author Index">Dong, Chiyu</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15953" title="Click to go to the Author Index">Dolan, John</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18691" title="Click to go to the Author Index">Litkouhi, Bakhtiar</a></td><td class="r">General Motor R&D Center</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab515" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Cooperative driving behavior is essential for driving in traffic, especially for ramp merging, lane changing or navigating intersections. Autonomous vehicles should also manage these situations by behaving cooperatively and naturally. In this paper, we enhance our previous learning-based method to efficiently estimate other vehicles' intentions and interact with them in ramp merging scenarios, without over-the-air communication between vehicles. The proposed approach inherits our previous PGM and ACC framework. Real driving trajectories are used to learn transition models in the PGM. Thus, besides the structure of the PGM, our method does not require human-designed reward or cost functions. The PGM-based intention estimation is followed by an off-the-shelf ACC distance keeping model to generate proper acceleration/deceleration commands. The PGM plays a plug-in role in our self-driving framework. The new model eliminates two assumptions in the previous model: 1) a fixed merging point for all merging agents, which is hard to obtain before the merging vehicles make the lane change; 2) Perfect velocity measurement, which requires sophisticated perception systems. We validate the performance of our method both on real merging data and using a designed merging strategy in simulation, and show significant improvements compared with previous methods. Parameter design is also discussed by experiments. The new method is computationally efficient, and exhibits better robustness against sensing uncertainty.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_03">16:00-18:00, Paper FrPS-SST3.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0099.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('99'); return false" title="Click to show or hide the keywords and abstract">Automated Driving: Interactive Automation Control System to Enhance Situational Awareness in Conditional Automation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18542" title="Click to go to the Author Index">Olaverri-Monreal, Cristina</a></td><td class="r">UAS Tech. Wien</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36179" title="Click to go to the Author Index">Kumar, Satyarth</a></td><td class="r">UAS Tech. Wien</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34930" title="Click to go to the Author Index">Diaz, Alberto</a></td><td class="r">Tech. Univ. of Madrid</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab99" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Human drivers in autonomous vehicles will monitor the system and be ready to resume control in ambiguous or emergency situations. As a driver’s reaction time to intervene after having realized a problem has occurred can be critical, we present the Interactive Automation Control System (IACS) to assist the driver when their takeover is required. The system displays manual or automated mode in an unobstrusive location in the vehicle, signaling when a TOR is required. We evaluate the systems performance during a situation in which the automation has not been defined to operate and study its impact on the overall driving performance, specifically the driver’s reaction time to a Take Over Request (TOR). Results showed significant improvements in driving performance with the proposed system. Both the response time to the TOR and the number of collisions decreased when the IACS was activated. Subjective ratings of the system regarding its performance showed high satisfaction levels.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_04">16:00-18:00, Paper FrPS-SST3.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0212.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('212'); return false" title="Click to show or hide the keywords and abstract">Optimization of Velocity Ramps with Survival Analysis for Intersection Merge-Ins</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35466" title="Click to go to the Author Index">Puphal, Tim</a></td><td class="r">Honda Res. Inst. Europe GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36091" title="Click to go to the Author Index">Probst, Malte</a></td><td class="r">Honda Res. Inst. Europe</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24238" title="Click to go to the Author Index">Li, Yiyang</a></td><td class="r">Honda R&D Co., Ltd</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16344" title="Click to go to the Author Index">Sakamoto, Yosuke</a></td><td class="r">HONDA R&D Co., Ltd</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19762" title="Click to go to the Author Index">Eggert, Julian</a></td><td class="r">Honda Res. Inst. Europe GmbH</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab212" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> We consider the problem of correct motion planning for T-intersection merge-ins of arbitrary geometry and vehicle density. A merge-in support system has to estimate the chances that a gap between two consecutive vehicles can be taken successfully. In contrast to previous models based on heuristic gap size rules, we present an approach which optimizes the integral risk of the situation using parametrized velocity ramps. It accounts for the risks from curves and all involved vehicles (front and rear on all paths) with a so-called survival analysis. For comparison, we also introduce a specially designed extension of the Intelligent Driver Model (IDM) for entering intersections. We show in a quantitative statistical evaluation that the survival method provides advantages in terms of lower absolute risk (i.e., no crash happens) and better risk-utility tradeoff (i.e., making better use of appearing gaps). Furthermore, our approach generalizes to more complex situations with additional risk sources.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_05">16:00-18:00, Paper FrPS-SST3.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0418.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('418'); return false" title="Click to show or hide the keywords and abstract">Development of BP Neural Network PID Controller and Its Application on Autonomous Emergency Braking System</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37004" title="Click to go to the Author Index">Wang, Liuhui</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37092" title="Click to go to the Author Index">Zhan, Zhenfei</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37006" title="Click to go to the Author Index">Yang, Xin</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39930" title="Click to go to the Author Index">Wang, Qingmiao</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36934" title="Click to go to the Author Index">Zhang, Yufeng</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37022" title="Click to go to the Author Index">Zheng, Ling</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39932" title="Click to go to the Author Index">Gang, Guo</a></td><td class="r">Chongqing Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab418" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Rear-end collision is one of the most common collision modes in China, which often leads to severe accident consequences. Autonomous Emergency Braking (AEB) system which can avoid or mitigate rear-end collision is one of the Advanced Driver Assistance System (ADAS) technologies. Traditional PID controller cannot effectively control the AEB system with strong nonlinear characteristics. Therefore, Back Propagation (BP) neural network PID controller is proposed in this paper. The PID parameters can be adjusted in real time based on the self-learning property and self-adapting property of BP neural network. The dynamics model is built in CarSim, and the inverse dynamics model is built in Simulink. Through the coordination control of the throttle angle and brake pressure, the host vehicle can brake automatically to avoid collisions in case of emergency. In addition, three kinds of test scenarios for the target car, stationary, slight braking, emergency braking, are setup based on complex environment in China. Finally, the simulations are conducted in these scenarios. And the simulation results indicate the feasibility and effectiveness of BP neural network PID controller in AEB system.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_06">16:00-18:00, Paper FrPS-SST3.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0329.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('329'); return false" title="Click to show or hide the keywords and abstract">Model Predictive Enhanced Adaptive Cruise Control for Multiple Driving Situations</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36225" title="Click to go to the Author Index">Ding, Yongqiang</a></td><td class="r">Res. Center for Itelligent Vehicles, Beijing Inst. of Te</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15898" title="Click to go to the Author Index">Chen, Huiyan</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15879" title="Click to go to the Author Index">Gong, Jianwei</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13755" title="Click to go to the Author Index">Xiong, Guangming</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35636" title="Click to go to the Author Index">Wang, Gang</a></td><td class="r">Shenzhen Inst. of Advanced Tech. Chinese Acad. of S</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab329" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> This paper presents an Enhanced Adaptive Cruise Control (EACC) framework that can work in different modes according to the forward targets. The EACC system, which was proposed in this paper, is based on a unified model and can achieve speed tracking, stop & go and autonomous emergency braking (AEB).Notably, speed tracking does not require a real preceding vehicle, a virtual vehicle can be set in front of the EACC vehicle. The mathematical method of setting the virtual preceding vehicle and the switching logic between the different working modes of the EACC system were given. Employing a constraints softening method to avoid computing infeasibility, an optimal control law is numerically calculated using the CVXGEN solver. Finally, real vehicle tests show that the EACC framework provides significant benefits in terms of speed-tracking capability, safety and comfort requirements while satisfying driver desired car following characteristics for different driving situations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst3_07">16:00-18:00, Paper FrPS-SST3.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0583.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('583'); return false" title="Click to show or hide the keywords and abstract">Extended Predictive Model-Mediated Teleoperation of Mobile Robots through Multilateral Control</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31039" title="Click to go to the Author Index">Panzirsch, Michael</a></td><td class="r">German Aerospace Center (DLR), Inst. for Robotics and Mechat</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37405" title="Click to go to the Author Index">Singh, Harsimran</a></td><td class="r">German Aerospace Center (DLR), Inst. for Robotics and Mechat</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37538" title="Click to go to the Author Index">Stelzer, Martin</a></td><td class="r">German Aerospace Center (DLR), Inst. for Robotics and Mechat</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37407" title="Click to go to the Author Index">Schuster, Martin J.</a></td><td class="r">German Aerospace Center (DLR), Inst. for Robotics and Mechat</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37404" title="Click to go to the Author Index">Ott, Christian</a></td><td class="r">German Aerospace Center (DLR), Inst. for Robotics and Mechat</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37398" title="Click to go to the Author Index">Ferre, Manuel</a></td><td class="r">Centre for Automation and Robotics (CAR) UPM-CSIC, Univ. P</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab583" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Telematics" title="Click to go to the Keyword Index">Telematics</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Despite the substantial progression of autonomous driving systems, their application is often limited e.g. due to safety margins which can be caused by uncertainties in the environment reconstruction. Then, via teleoperation as a fallback solution, a human-in-the-loop can be introduced as the main decision maker. However, high delay in the communication channel distorts the performance of direct force feedback teleoperation for example in space or disaster scenarios. On the other hand, model-mediated teleoperation can provide instantaneous and even predictive force feedback to the user, but the performance is limited due to state mismatches, incomplete models, model errors and the modeling challenges of complex wheel-ground contacts. Therefore, in this paper we introduce the concept of extended model-mediated teleoperation with a car like interface for mobile robots by fusing local fictitious and remote force feedback, which can be measured, computed or fictitious. We provide a method to guarantee stability of the extended model-mediated teleoperation (involving time delay, multilateral coupling, fictitious force feedback and permanent updates of the local model) based on the passivity theorem. The benefits of the approach are highlighted by human-in-the-loop experiments with a wheeled mobile robot.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst9"><b>FrPS-SST9</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst9" title="Click to go to the Program at a Glance"><b>Visual Sensing-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_01">16:00-18:00, Paper FrPS-SST9.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0429.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('429'); return false" title="Click to show or hide the keywords and abstract">Artificial Intelligence Course Design: Istream-Based Visual Cognitive Smart Vehicles (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11999" title="Click to go to the Author Index">Gong, Xiaoyan</a></td><td class="r">Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37113" title="Click to go to the Author Index">Wu, Yilin</a></td><td class="r">Beijing NO. 13 School, Beijing, China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37114" title="Click to go to the Author Index">Ye, Zifan</a></td><td class="r">Affiliated NO.2 High School of Beijing Normal Univ. Beijin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30058" title="Click to go to the Author Index">Liu, Xiwei</a></td><td class="r">Chinese Acad. of Sciences</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab429" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> New intelligent era calls for new learners and thus urgently needs a series of artificial intelligence. As a good educational platform for teaching artificial intelligence, smart cars have aroused concern and practices of all parties. However, at present, most courses and training pay more attention to basic knowledge and technology of smart cars, seldom to training based on artificial intelligence curriculum system and comprehensive competency integrating science, technology, art and management. Therefore, based on concept of iSTREAM (intelligence for Science, Technology, Robotics, Engineering, Art, and Management) and Raspberry intelligent vehicle teaching platform, this paper introduced a smart car-themed artificial intelligence courses including basic courses, specialized courses, specialized technical courses and elective courses. This course can guide learners to develop smart cars based on visual cognition, in-depth learning, VR and 3D printing integrated artistic creativity. It combines disciplines such as science, technology, art, games and management to upgrade a single knowledge and technology course into a comprehensive competency course that integrates knowledge, skills, emotion and management. Practice in Beijing NO.13 and NO.101 High School shows that this course allows students to experience scientific research process, learn artificial intelligence related knowledge and skills, understand scientific way of thinking and scientific research methods, stimulate learners’ responsibility and scientific passion, and cultivate leadership skills through self-learning and partly project management.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_02">16:00-18:00, Paper FrPS-SST9.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0426.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('426'); return false" title="Click to show or hide the keywords and abstract">Continuous Point Cloud Stitch Based on Image Feature Matching Constraint and Score</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36767" title="Click to go to the Author Index">Fangchao, Hu</a></td><td class="r">Chongqing Univ. of Posts and Telecommunications</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40390" title="Click to go to the Author Index">Li, Yinguo</a></td><td class="r">Chongqing Univ. of Posts and Telecommunications</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40391" title="Click to go to the Author Index">Tian, Zhen</a></td><td class="r">Chongqing Univ. of Posts and Telecommunications, Department</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40532" title="Click to go to the Author Index">Huang, Wei</a></td><td class="r">Chongqing Univ. of Posts and Telecommunications</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab426" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> The purpose of environment perception for autonomous vehicle is to get the entire information around the ego-vehicle and understand the environment where the autonomous vehicle runs. In this paper, we attempt to rebuild a 3D environment for the autonomous vehicle using the vehicle mounted cameras. First, we generate the continuous 3D point clouds with the vehicle moving. Then we stitch the continuous point clouds by using the image feature matching constraint and scores. At last, we obtained an accuracy and efficiency dense point cloud of the environment. After the numerical experiments, the proposed stitching algorithm has been verified that the MSE is lower than the average of other algorithms. And number of points decreases than the conventional algorithm.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_03">16:00-18:00, Paper FrPS-SST9.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0407.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('407'); return false" title="Click to show or hide the keywords and abstract">Spatio-Temporal Depth Interpolation (STDI)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31167" title="Click to go to the Author Index">Ochs, Matthias</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32367" title="Click to go to the Author Index">Bradler, Henry</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16129" title="Click to go to the Author Index">Mester, Rudolf</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab407" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> In the area of autonomous driving, sensing the environment is most important for self-localization and egomotion estimation. Visual odometry/SLAM methods have proven capable to achieve good results, even in real-time applications by operating in a sparse mode. Running on a sequence, these methods need to continuously incorporate new features well distributed over the image. Therefore, the performance of these methods can be further improved, if they are supplied with coarse but dense initial depth information, that can be utilized at arbitrary sparse image positions. Previously triangulated depths and even high quality depth measurements of a LIDAR sensor are not suitable for this task, since they only provide a sparse depth map. To solve this issue, we introduce a novel interpolation method called Spatio-Temporal Depth Interpolation (STDI), which exploits spatial and temporal correlations of the data (e.g. sequences of sparse depth maps) to give a consistent dense output including associated uncertainties. STDI is a fused approach, which makes use of the most important components of a principal component analysis (PCA) (spatial information) and additionally is capable to re-use information of previously interpolated depth maps in a regression based approach (temporal information). We evaluate the quality of STDI on the KITTI visual odometry benchmark, where a sequence of extremely sparsely sampled depth maps (approx. 40 depth values) is densified and on the KITTI depth completion benchmark. The latter deals with the densification of sparse LIDAR input. Of course, our method is not limited to these applications and can be used for any densification of sparse sequential data which is expected to contain spatial and/or temporal correlations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_04">16:00-18:00, Paper FrPS-SST9.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0143.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('143'); return false" title="Click to show or hide the keywords and abstract">Real-Time Stereo Disparity Quality Improvement for Challenging Traffic Environments</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31367" title="Click to go to the Author Index">Xu, Yuquan</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15903" title="Click to go to the Author Index">Mita, Seiichi</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18429" title="Click to go to the Author Index">Tehrani Nik Nejad, Hossein</a></td><td class="r">DENSO Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36284" title="Click to go to the Author Index">Chin, Hakusho</a></td><td class="r">DENSO Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29556" title="Click to go to the Author Index">Ishimaru, Kazuhisa</a></td><td class="r">Nippon Soken Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34794" title="Click to go to the Author Index">Nishino, Sakiko</a></td><td class="r">Nippon Soken Inc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab143" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> The stereo vision system is one of the most important sensors for 3D reconstruction of the environment and scene understanding. It plays an essential role for the automated driving and ADAS applications. Although recently great progress has been made in this field both in the density and accuracy, the stereo vision in the challenging environment, such as rain, snow and low light conditions, are still open problems and need to be improved. To address this issue, we propose a novel real-time stereo vision algorithm, we transform the rough confidence score of the matching cost to the outlier probability rate and use it to guide the multi-path Viterbi method to estimate the disparity. Our proposed algorithm costs less than 40ms and improves the accuracy of stereo results in difficult scenarios. Real-world experiments show that the proposed method can significantly improve the disparity results in the various challenging environment.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_05">16:00-18:00, Paper FrPS-SST9.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0164.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('164'); return false" title="Click to show or hide the keywords and abstract">Continuous Stereo Self-Calibration on Planar Roads</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27537" title="Click to go to the Author Index">Mueller, Georg Rupert</a></td><td class="r">Univ. of the Bundeswehr Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34322" title="Click to go to the Author Index">Burger, Patrick</a></td><td class="r">Univ. of the Bundeswehr Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15680" title="Click to go to the Author Index">Wuensche, Hans Joachim Joe</a></td><td class="r">Univ. BW Munich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab164" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> This paper presents an algorithm for continuous online estimation of the twelve degrees-of-freedom (12-DoF) extrinsic calibration of a stereo camera system for an autonomous car. An Extended Kalman Filter (EKF) recursively estimates the stereo camera calibration by tracking salient points in 3D space that are visible in both cameras. All extrinsic parameters of the stereo camera calibration are only observable under translation and two independent rotations of the vehicle. However, when driving on urban, paved roads the vehicle performs only limited pitch or roll movement. An analysis of the Fisher information matrix reveals that in these situations especially the installation height is difficult to estimate. For these scenarios a further measurement model is added to the algorithm that utilizes the homography of salient points. This homography is induced by the planar road surface in consecutive camera images. Recognition of the road surface, when the position and orientation of the cameras is unknown, can be difficult. Therefore, a convolutional neural network (CNN) is developed that segments camera images pixelwisely into three classes: ’road’, ’static (other)’ and ’potentially dynamic’. Only salient points that are segmented as ’road’ are considered for the homography measurement model. Points that are segmented as ’potentially dynamic’ are not taken into account for the calibration algorithm. The structure of the CNN has been chosen carefully to enable segmentation of camera images on a mid-range GPU on-board of our autonomous vehicle. An evaluation of the extended algorithm, based on a recorded dataset, shows a considerably faster estimation of the installation height.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_06">16:00-18:00, Paper FrPS-SST9.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0292.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('292'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Field-Based Representation of Surrounding Vehicle Motion from a Monocular Camera</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36739" title="Click to go to the Author Index">Zhu, Lifeng</a></td><td class="r">Southeast Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23114" title="Click to go to the Author Index">Li, Xuanpeng</a></td><td class="r">Southeast Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27397" title="Click to go to the Author Index">Lu, Wenjie</a></td><td class="r">Univ. Paris Sud</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36741" title="Click to go to the Author Index">Zhang, Yongjie Jessica</a></td><td class="r">CMU</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab292" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0292.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Sensing and presenting on-road information of moving vehicles is essential for fully and semi-automated driving. It is challenging to track vehicles from affordable on-board cameras in crowded scenes. The mismatch or missing data are unavoidable and it is ineffective to directly present uncertain cues to support the decision-making.In this paper, we propose a physical model based on incompressible fluid dynamics to represent the vehicle's motion, which provides hints of possible collision as a continuous scalar riskmap. We estimate the position and velocity of other vehicles from a monocular on-board camera located in front of the ego-vehicle. The noisy trajectories are then modeled as the boundary conditions in the simulation of advection and diffusion process. We then interactively display the animating distribution of substances, and show that the continuous scalar riskmap well matches the perception of vehicles even in presence of the tracking failures. We test our method on real-world scenes and discuss about its application for driving assistance and autonomous vehicle in the future.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_07">16:00-18:00, Paper FrPS-SST9.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0270.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('270'); return false" title="Click to show or hide the keywords and abstract">VH-HFCN Based Parking Slot and Lane Markings Segmentation on Panoramic Surround View</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34510" title="Click to go to the Author Index">Wu, Yan</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34511" title="Click to go to the Author Index">Yang, Tao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34519" title="Click to go to the Author Index">Zhao, Junqiao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34513" title="Click to go to the Author Index">Guan, Linting</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36226" title="Click to go to the Author Index">Jiang, Wei</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab270" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> The automatic parking is being massively developed by car manufacturers and providers. Until now, there are two problems with the automatic parking. First, there is no openly-available segmentation labels of parking slot on panoramic surround view (PSV) dataset. Second, how to detect parking slot and road structure robustly. Therefore, in this paper, we build up a public PSV dataset. At the same time, we proposed a highly fused convolutional network (HFCN) based segmentation method for parking slot and lane markings based on the PSV dataset. A surround-view image is made of four calibrated images captured from four fisheye cameras. We collect and label more than 4,200 surround view images for this task, which contain various illuminated scenes of different types of parking slots. A VH-HFCN network is proposed, which adopts an HFCN as the base, with an extra efficient VH-stage for better segmenting various markings. The VH-stage consists of two independent linear convolution paths with vertical and horizontal convolution kernels respectively. This modification enables the network to robustly and precisely extract linear features. We evaluated our model on the PSV dataset and the results showed outstanding performance in ground markings segmentation. Based on the segmented markings, parking slots and lanes are acquired by skeletonization, hough line transform and line arrangement.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst9_08">16:00-18:00, Paper FrPS-SST9.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0091.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('91'); return false" title="Click to show or hide the keywords and abstract">Benchmarking Image Sensors under Adverse Weather Conditions for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36070" title="Click to go to the Author Index">Bijelic, Mario</a></td><td class="r">RD Daimler Ulm and Univ. Ulm</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36069" title="Click to go to the Author Index">Gruber, Tobias</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14030" title="Click to go to the Author Index">Ritter, Werner</a></td><td class="r">Daimler AG</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab91" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Adverse weather conditions are very challenging for autonomous driving because most of the state-of-the-art sensors stop working reliably under these conditions. In order to develop robust sensors and algorithms, tests with current sensors in defined weather conditions are crucial for determining the impact of bad weather for each sensor. This work describes a testing and evaluation methodology that helps to benchmark novel sensor technologies and compare them to state-of-the-art sensors. As an example, gated imaging is compared to standard imaging under foggy conditions. It is shown that gated imaging outperforms state-of-the-art standard passive imaging due to time-synchronized active illumination.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst10"><b>FrPS-SST10</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst10" title="Click to go to the Program at a Glance"><b>Software for Intelligent Vehicle</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_01">16:00-18:00, Paper FrPS-SST10.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0563.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('563'); return false" title="Click to show or hide the keywords and abstract">A Modular Software Framework for Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37149" title="Click to go to the Author Index">Lim, Kai Li</a></td><td class="r">The Univ. of Western Australia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37150" title="Click to go to the Author Index">Drage, Thomas</a></td><td class="r">The Univ. of Western Australia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37151" title="Click to go to the Author Index">Podolski, Roman</a></td><td class="r">Tech. Univ. of Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37154" title="Click to go to the Author Index">Meyer-Lee, Gabriel</a></td><td class="r">Swarthmore Coll</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37152" title="Click to go to the Author Index">Evans-Thompson, Samuel</a></td><td class="r">The Univ. of Western Australia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37155" title="Click to go to the Author Index">Lin, Jason Yao-Tsu</a></td><td class="r">The Univ. of Western Australia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37156" title="Click to go to the Author Index">Channon, Geoffrey</a></td><td class="r">The Univ. of Western Australia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37157" title="Click to go to the Author Index">Poole, Mitchell</a></td><td class="r">The Univ. of Western Australia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15866" title="Click to go to the Author Index">Braunl, Thomas</a></td><td class="r">The Univ. of Western Australia</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab563" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Software frameworks for autonomous vehicles are required to interface and process data from several different sensors on board the vehicle, in addition to performing navigational processes such as path planning and lane keeping. These can include a combination of cameras, LIDARs, GPS, IMU, and odometric sensors to achieve positioning and localisation for the vehicle and can be challenging to integrate. In this paper, we present a unified software framework that combines sensor and navigational processing for autonomous driving. Our framework is modular and scalable whereby the use of protocol buffers enables segregating each sensor and navigation subroutine individual classes, which can then be independently modified or tested. It is redesigned to replace the existing software on our Formula SAE vehicle, which we use for testing autonomous driving. Our testing results verify the suitability of our framework to be used for fully autonomous drives.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_02">16:00-18:00, Paper FrPS-SST10.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0500.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('500'); return false" title="Click to show or hide the keywords and abstract">Semi-Automatic High-Accuracy Labelling Tool for Multi-Modal Long-Range Sensor Dataset</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37347" title="Click to go to the Author Index">Izquierdo, Rubén</a></td><td class="r">Univ. of Alcalá</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12438" title="Click to go to the Author Index">Parra Alonso, Ignacio</a></td><td class="r">Univ. De Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33405" title="Click to go to the Author Index">Salinas Maldonado, Carlota</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12437" title="Click to go to the Author Index">Fernandez Llorca, David</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12591" title="Click to go to the Author Index">Sotelo, Miguel A.</a></td><td class="r">Univ. of Alcala</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab500" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a></span><br>
                              <strong>Abstract:</strong> Many research works have contributed to achieve SAE levels 3 and 4 in some pre-defined areas under certain restrictions. A deeper scene understanding and precise predictions of drivers intentions are needed to continue improving autonomous driving capabilities to reach higher SAE levels. Deployment of accurate and detailed datasets could be considered as one of the most pressing needs to enhance autonomous driving capabilities. This work presents a novel data acquisition methodology for on-road vehicle trajectory collection. The proposed sensor setup improves the range and detection accuracy by using a high accuracy laser scanner covering 360^circ and two high-speed and high-resolution cameras. The sensor fusion increases the labelling resolution and extends the detection range sporting the best of each sensor. A Median Flow tracking algorithm and a Convolutional Neural Network enable a semi-automatic labelling process, which reduces the effort to create detailed annotated datasets. High accurate trajectories are reconstructed with few manual annotations up to 60 m with a mean error below 2 cm. This methodology has been developed with a view to creating a dataset which enables the development of advanced vehicle trajectory prediction systems, and thus to contribute to human-like automated driving.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_03">16:00-18:00, Paper FrPS-SST10.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0453.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('453'); return false" title="Click to show or hide the keywords and abstract">Failure Prediction for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37185" title="Click to go to the Author Index">Hecker, Simon</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35649" title="Click to go to the Author Index">Dai, Dengxin</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36878" title="Click to go to the Author Index">Van Gool, Luc S. J.</a></td><td class="r">ETH Zurich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab453" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> The primary focus of autonomous driving research is to improve driving accuracy. While great progress has been made, state-of-the-art algorithms still fail at times. Such failures may have catastrophic consequences. It therefore is important that automated cars foresee problems ahead as early as possible. This is also of paramount importance if the driver will be asked to take over. We conjecture that failures do not occur randomly. For instance, driving models may fail more likely at places with heavy traffic, at complex intersections, and/or under adverse weather/illumination conditions. This work presents a method to learn to predict the occurrence of these failures, i.e. to assess how difficult a scene is to a given driving model and to possibly give the human driver an early headsup. A camera-based driving model is developed and trained over real driving datasets. The discrepancies between the model's predictions and the human `ground-truth' maneuvers were then recorded, to yield the `failure' scores. Experimental results show that the failure score can indeed be learned and predicted. Thus, our prediction method is able to improve the overall safety of an automated driving model by alerting the human driver timely, leading to better human-vehicle collaborative driving.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_04">16:00-18:00, Paper FrPS-SST10.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0625.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('625'); return false" title="Click to show or hide the keywords and abstract">A 3D Dynamic Scene Analysis Framework for Development of Intelligent Transportation Systems</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36452" title="Click to go to the Author Index">Wang, Chien-Yi</a></td><td class="r">Honda Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37596" title="Click to go to the Author Index">Narayanan Athma, Athma</a></td><td class="r">Honda Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37597" title="Click to go to the Author Index">Patil, Abhishek</a></td><td class="r">Honda Res. Inst. USA Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33408" title="Click to go to the Author Index">Zhan, Wei</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35453" title="Click to go to the Author Index">Chen, Yi-Ting</a></td><td class="r">Honda Res. Inst</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab625" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Holistic driving scene understanding is a critical step toward intelligent transportation systems. It involves different levels of analysis, interpretation, reasoning and decision making. In this paper, we propose a 3D dynamic scene analysis framework as the first step toward driving scene understanding. Specifically, given a sequence of synchronized 2D and 3D sensory data, the framework systematically integrates different perception modules to obtain 3D position, orientation, velocity and category of traffic participants and the ego car in a reconstructed 3D semantically labeled traffic scene. We implement this framework and demonstrate the effectiveness in challenging urban driving scenarios. The proposed framework builds a foundation for higher level driving scene understanding problems such as intention and motion prediction of surrounding entities, ego motion planning, and decision making.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_05">16:00-18:00, Paper FrPS-SST10.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0410.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('410'); return false" title="Click to show or hide the keywords and abstract">A Fallback Approach for an Automated Vehicle Encountering Sensor Failure in Monitoring Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36911" title="Click to go to the Author Index">Xue, Wei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30888" title="Click to go to the Author Index">Yang, Bo</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34445" title="Click to go to the Author Index">Kaizuka, Tsutomu</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27100" title="Click to go to the Author Index">Nakano, Kimihiko</a></td><td class="r">Univ. of Tokyo</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab410" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Dynamic driving task (DDT) fallback turns to be an essential part in level 3 or higher driving automation systems, which is responsible to either perform the DDT or achieve a minimal risk condition after encountering automated driving system (ADS) failure. As a typical ADS failure, sensor failure can prevent ADS from performing on-road driving safely, and thus a minimal risk condition can be achieved when the failed vehicle stops away from the active lane. Therefore, this paper considers a level 4 ADS-dedicated vehicle encountering front sensor failure in highway traffic. The proposed fallback approach is designed to avoid potential collision with surrounding vehicles while bringing the vehicle to a stop on the designated parking zone, thus achieve a minimal risk condition. Safety constraint is proposed based on the assumption that the failed vehicle is remarkable by surrounding vehicles, and enforced using model predictive control. As a result, simulation is conducted in Carsim/Simulink environment.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_06">16:00-18:00, Paper FrPS-SST10.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0183.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('183'); return false" title="Click to show or hide the keywords and abstract">Ontology Based Scene Creation for the Development of Automated Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27371" title="Click to go to the Author Index">Bagschik, Gerrit</a></td><td class="r">Tech. Univ. Braunschweig</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36436" title="Click to go to the Author Index">Menzel, Till</a></td><td class="r">TU Braunschweig - Inst. of Control Engineering</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13854" title="Click to go to the Author Index">Maurer, Markus</a></td><td class="r">TU Braunschweig</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab183" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> The introduction of automated vehicles without permanent human supervision demands a functional system description, including functional system boundaries and a comprehensive safety analysis. These inputs to the technical development can be identified and analyzed by a scenario-based approach. Furthermore, to establish an economical test and release process, a large number of scenarios must be identified to obtain meaningful test results. Experts are doing well to identify scenarios that are difficult to handle or unlikely to happen. However, experts are unlikely to identify all scenarios possible based on the knowledge they have on hand. Expert knowledge modeled for computer aided processing may help for the purpose of providing a wide range of scenarios. This contribution reviews ontologies as knowledge-based systems in the field of automated vehicles, and proposes a generation of traffic scenes in natural language as a basis for a scenario creation.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_07">16:00-18:00, Paper FrPS-SST10.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0184.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('184'); return false" title="Click to show or hide the keywords and abstract">Scenarios for Development, Test and Validation of Automated Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36436" title="Click to go to the Author Index">Menzel, Till</a></td><td class="r">TU Braunschweig - Inst. of Control Engineering</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27371" title="Click to go to the Author Index">Bagschik, Gerrit</a></td><td class="r">Tech. Univ. Braunschweig</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13854" title="Click to go to the Author Index">Maurer, Markus</a></td><td class="r">TU Braunschweig</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab184" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> The latest version of the ISO 26262 standard from 2016 represents the state of the art for a safety-guided development of safety-critical electric/electronic vehicle systems. These vehicle systems include advanced driver assistance systems and vehicle guidance systems. The development process proposed in the ISO 26262 standard is based upon multiple V-models, and defines activities and work products for each process step. In many of these process steps, scenario based approaches can be applied to achieve the defined work products for the development of automated driving functions. To accomplish the work products of different process steps, scenarios have to focus on various aspects like a human understandable notation or a description via time-space variables. This leads to contradictory requirements regarding the level of detail and way of notation for the representation of scenarios. In this paper, the authors discuss requirements for the representation of scenarios in different process steps defined by the ISO 26262 standard, propose a consistent terminology based on prior publications for the identified levels of abstraction, and demonstrate how scenarios can be systematically evolved along the phases of the development process outlined in the ISO 26262 standard.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst10_08">16:00-18:00, Paper FrPS-SST10.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0349.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('349'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Train Here, Deploy There: Robust Segmentation in Unseen Domains</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31401" title="Click to go to the Author Index">Romera, Eduardo</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12570" title="Click to go to the Author Index">Bergasa, Luis M.</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16008" title="Click to go to the Author Index">Alvarez, José M.</a></td><td class="r">NICTA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10332" title="Click to go to the Author Index">Trivedi, Mohan M.</a></td><td class="r">Univ. of California at San Diego</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab349" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0349.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Semantic Segmentation methods play a key role in today's Autonomous Driving research, since they provide a global understanding of the traffic scene for upper-level tasks like navigation. However, main research efforts are being put on enlarging deep architectures to achieve marginal accuracy boosts in existing datasets, forgetting that these algorithms must be deployed in a real vehicle with images that weren't seen during training. On the other hand, achieving robustness in any domain is not an easy task, since deep networks are prone to overfitting even with thousands of training images. In this paper, we study in a systematic way what is the gap between the concepts of &quot;accuracy&quot; and &quot;robustness&quot;. A comprehensive set of experiments demonstrates the relevance of using data augmentation to yield models that can produce robust semantic segmentation outputs in any domain. Our results suggest that the existing domain gap can be significantly reduced when appropiate augmentation techniques regarding geometry (position and shape) and texture (color and illumination) are applied. In addition, the proposed training process results in better calibrated models, which is of special relevance to assess the robustness of current systems.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst11"><b>FrPS-SST11</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst11" title="Click to go to the Program at a Glance"><b>Odometry</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst11_01">16:00-18:00, Paper FrPS-SST11.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0207.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('207'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Structureless Approach for Visual Odometry</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36527" title="Click to go to the Author Index">Chih-Chung, Chou</a></td><td class="r">National Taiwan Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36529" title="Click to go to the Author Index">Chun-Kai, Chang</a></td><td class="r">Independent Roboticis Res</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26522" title="Click to go to the Author Index">Seo, YoungWoo</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab207" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0207.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> A local, bundle-adjustment is an important procedure to improve the accuracy of a visual odometry solution. However, it is computationally very expensive as it jointly optimize all the poses of cameras and locations of map points. To reduce the computational complexity of such a procedure without much sacrificing the accuracy of a solution, the state-of-the-art algorithms were proposed to eliminate the map point variables, using extra matrix operations, from their linearized optimization solutions. Instead of eliminating the map points, this paper proposes a novel way of addressing this complexity issue -- we represent a map point as a function of two camera poses, and uses the triangulated location of the map point when needed. Our method is more efficient than a full-SLAM formulation in solving the visual odometry problem in that 1) the complexity of our solution is lower than those of the state-of-the-art methods, 2) no extra matrix operations required to eliminate map points, 3) no need guesses on map points' initial locations. Experiemental results, through simulated experiments and experiments with KITTI data, demonstrated that our results are more accurate that those of a full-SLAM approach with lower runtime complexities.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst11_02">16:00-18:00, Paper FrPS-SST11.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0062.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('62'); return false" title="Click to show or hide the keywords and abstract">Monocular Visual-Inertial Odometry Based on Sparse Feature Selection with Adaptive Grid</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35519" title="Click to go to the Author Index">Cai, Zhiao</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12021" title="Click to go to the Author Index">Yang, Ming</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15150" title="Click to go to the Author Index">Wang, Chunxiang</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13407" title="Click to go to the Author Index">Wang, Bing</a></td><td class="r">Shanghai Jiao Tong Univ. SEIEE</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab62" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> For sparse feature based visual-inertial odometry, feature selection is vital to the performance. The selected features should be evenly distributed in the image and be appropriate for tracking. This paper presents a visual-inertial odometry approach based on sparse feature selection with the adaptive grid to improve the performance in different environments. In the proposed approach, FAST corner detection is employed in every grid, the size of which is adaptively adjusted. Features in the same grid are ranked and selected based on scores to ensure good feature quality. Subsequently, the selected features are tracked by the KLT sparse optical flow with local intensity normalization. Finally, the initial and visual measurements are jointly optimized by sliding window based nonlinear optimization to achieve six degrees of freedom motion estimation. The proposed method is validated on the public dataset and real-world experiments, which shows our approach reaches state-of-the-art performance.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst11_03">16:00-18:00, Paper FrPS-SST11.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0585.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('585'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Integrating Odometry and Inter-Vehicular Communication for Adaptive Cruise Control with Target Detection Loss</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32303" title="Click to go to the Author Index">Lin, Yuan</a></td><td class="r">Virginia Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37537" title="Click to go to the Author Index">Wu, Chaoxian</a></td><td class="r">Wuhan Univ. of Tech. Virginia Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10381" title="Click to go to the Author Index">Eskandarian, Azim</a></td><td class="r">Virginia Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab585" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0585.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Adaptive Cruise Control (ACC) systems utilize distance sensors and control algorithms to enable a vehicle to follow its preceding vehicle with desired headways. There are situations when distance sensors may lose the target, i.e., the preceding vehicle, when unfavorable weather conditions result in low reflectance or the preceding vehicle escapes the sensor's angle of view on curvy roads. In these situations, a common strategy is to suppress acceleration and maintain current speed (Cruise Control) until the sensor detects the preceding vehicle again. In this paper, we propose a new solution which integrates odometry and inter-vehicular communication to enable vehicle following for the period of target detection loss. With inter-vehicular communication, a following vehicle can obtain the odometry data of its preceding vehicle to compute the inter-vehicle distance. This work focuses on the design of an ACC system that can enable vehicle following during short-term target detection loss. The feasibility of the system design is validated through mobile robot experiments. Autonomous lane keeping capability is developed such that the mobile robots move according to the center of the lane. The robot following experiment results show that with accurate odometry and lane centering, the proposed system design can maintain desired headway following.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst11_04">16:00-18:00, Paper FrPS-SST11.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0209.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('209'); return false" title="Click to show or hide the keywords and abstract">Toward Autonomous Driving in Highway and Urban Environment: HQ3 and IVFC 2017</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36002" title="Click to go to the Author Index">Qian, Lilin</a></td><td class="r">NUDT</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31235" title="Click to go to the Author Index">Fu, Hao</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27175" title="Click to go to the Author Index">Li, Xiaohui</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36306" title="Click to go to the Author Index">Cheng, Bang</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30434" title="Click to go to the Author Index">Hu, Tingbo</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20698" title="Click to go to the Author Index">Sun, Zhenping</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25865" title="Click to go to the Author Index">Wu, Tao</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27361" title="Click to go to the Author Index">Dai, Bin</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31283" title="Click to go to the Author Index">Xu, Xin</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36327" title="Click to go to the Author Index">Tang, Jin</a></td><td class="r">Central South Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab209" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> The 2017 Intelligent Vehicle Future Challenge of China (IVFC) was held in Changshu between 24th November and 26th November, 2017. As the ninth series of this event, last year’s competition has introduced many new features and has attracted 21 teams to join this competition. The HQ3 autonomous vehicle, jointly developed by National University of Defense Technology, Jilin University and Central South University, took part in this competition. This paper mainly describes the key modules of HQ3, including GPS-free local- ization, environment perception and behavior planning. All of these modules together enable HQ3 to perform well during the competition.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst12"><b>FrPS-SST12</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst12" title="Click to go to the Program at a Glance"><b>Simulation and Case Analysis-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_01">16:00-18:00, Paper FrPS-SST12.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0338.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('338'); return false" title="Click to show or hide the keywords and abstract">Estimating the Uniqueness of Test Scenarios Derived from Recorded Real-World-Driving-Data Using Autoencoders</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36087" title="Click to go to the Author Index">Langner, Jacob</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31244" title="Click to go to the Author Index">Bach, Johannes</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36603" title="Click to go to the Author Index">Ries, Lennart</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31839" title="Click to go to the Author Index">Otten, Stefan</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26193" title="Click to go to the Author Index">Holzaepfel, Marc</a></td><td class="r">Dr. Ing. H.c. F. Porsche AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31844" title="Click to go to the Author Index">Sax, Eric</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab338" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> Advanced Driver Assistant Systems (ADAS) use a multitude of input signals for tasks like trajectory planning and control of vehicle dynamics provided by a large variety of information sources such as sensors and digital maps. To assure the feature’s valid behavior all realistically possible environmental situations have to be tested. The test scenarios used for simulation can be derived from real-world-driving-data. However, the significance of derived scenarios is weakened by repetitive similar situations within the driving data, which increase the test efforts without providing new insights regarding the test of the ADAS. In this contribution, an automated selection algorithm for test scenarios based on relevant environmental parameters is presented. Starting with a randomly selected initial testset, the machine-learning concept of autoencoders is utilized to recognize novel scenarios within the data pool, which are iteratively added to the initial testset. Furthermore, the key parameters for the autoencoder’s performance are shown in depths. The approach is fully automated, so that the identified novel scenarios within an entire testset are automatically combined to a reduced testset of unique relevant scenarios. The achieved testset reduction and thereby the saving potential in simulation time is demonstrated on a dataset including several thousand test kilometers.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_02">16:00-18:00, Paper FrPS-SST12.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0294.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('294'); return false" title="Click to show or hide the keywords and abstract">Evaluating Model Mismatch Impacting CACC Controllers in Mixed Traffic Using a Driving Simulator</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34364" title="Click to go to the Author Index">Patel, Raj Haresh</a></td><td class="r">EURECOM</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30869" title="Click to go to the Author Index">Aramrattana, Maytheewat</a></td><td class="r">Halmstad Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30877" title="Click to go to the Author Index">Englund, Cristofer</a></td><td class="r">Viktoria Swedish ICT</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32285" title="Click to go to the Author Index">Haerri, Jerome</a></td><td class="r">EURECOM</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26837" title="Click to go to the Author Index">Jansson, Jonas</a></td><td class="r">Res. Department Traffic and Road-Users VTI - Linköping, Swed</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35120" title="Click to go to the Author Index">Bonnet, Christian</a></td><td class="r">EURECOM</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab294" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> At early market penetration, automated vehicles will share the road with legacy vehicles. For a safe transportation system, automated vehicle controllers therefore need to estimate the behavior of the legacy vehicles. However, mismatches between the estimated and real human behaviors can lead to inefficient control inputs, and even collisions in the worst case. In this paper, we propose a framework for evaluating the impact of model mismatch by interfacing a controller under test with a driving simulator. As a proof-of-concept, an algorithm based on Model Predictive Control (MPC) is evaluated in a braking scenario. We show how model mismatch between estimated and real human behavior can lead to a decrease in avoided collisions by almost 46%, and an increase in discomfort by almost 91%. Model mismatch is therefore non-negligible and the proposed framework is a unique method to evaluate them.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_03">16:00-18:00, Paper FrPS-SST12.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0153.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('153'); return false" title="Click to show or hide the keywords and abstract">X-In-The-Loop Advanced Driving Simulation Platform for the Design, Development, Testing and Validation of ADAS</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36128" title="Click to go to the Author Index">Moten, Sikandar</a></td><td class="r">Siemens PLM Software</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36126" title="Click to go to the Author Index">Celiberti, Francesco</a></td><td class="r">Siemens PLM Software</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36251" title="Click to go to the Author Index">Grottoli, Marco</a></td><td class="r">Siemens PLM Software</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36344" title="Click to go to the Author Index">van der Heide, Anne</a></td><td class="r">TU Delft</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36111" title="Click to go to the Author Index">Lemmens, Yves</a></td><td class="r">Siemens PLM Software</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab153" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper presents a X-in-the-loop (where X: Model, Software, Hardware, Driver/Human, etc.) driving simulation platform, developed at Siemens PLM Software, that facilitates the design, development, testing and validation of Advanced Driver Assistance System (ADAS). The paper outlines the essential components of the simulator and demonstrates the usefulness by two autonomous driving functionalities i.e. adaptive cruise control and autonomous intersection crossing. In addition, the paper highlights the key features of the platform.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_04">16:00-18:00, Paper FrPS-SST12.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0232.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('232'); return false" title="Click to show or hide the keywords and abstract">CoInCar-Sim: An Open-Source Simulation Framework for Cooperatively Interacting Automobiles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33468" title="Click to go to the Author Index">Naumann, Maximilian</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29528" title="Click to go to the Author Index">Poggenhans, Fabian</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17952" title="Click to go to the Author Index">Lauer, Martin</a></td><td class="r">Karlsruher Inst. Für Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10216" title="Click to go to the Author Index">Stiller, Christoph</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab232" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> While motion planning techniques for automated vehicles in a reactive and anticipatory manner have already been widely presented, cooperative motion planning has only been addressed recently. For the latter, interaction between traffic participants is crucial and has to be considered. Consequently, simulations where other traffic participants follow simple behavioral rules can no longer be used for development and evaluation. To close this gap, we present a multi vehicle simulation framework. Conventional simulation agents, using a simple, rule-based behavior, are replaced by multiple instances of a sophisticated behavior generation. Thus, the development, test and simulative evaluation of cooperative planning approaches is facilitated. The framework is implemented using the Robot Operating System (ROS) and its code will be released open source.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_05">16:00-18:00, Paper FrPS-SST12.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0542.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('542'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Overtaking Maneuvers in Simulated Highway Driving Using Deep Reinforcement Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37360" title="Click to go to the Author Index">Kaushik, Meha</a></td><td class="r">International Inst. of Information Tech. Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37293" title="Click to go to the Author Index">Prasad, Vignesh</a></td><td class="r">TCS Innovation Labs, Kolkata</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23891" title="Click to go to the Author Index">Krishna, K Madhava</a></td><td class="r">IIIT Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37361" title="Click to go to the Author Index">Ravindran, Balaraman</a></td><td class="r">IIT Madras</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab542" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0542.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> Most methods that attempt to tackle the problem of Autonomous Driving and overtaking usually try to either directly minimize an objective function or iteratively in a Reinforcement Learning like framework to generate motor actions given a set of inputs. We follow a similar trend but train the agent in a way similar to a curriculum learning approach where the agent is first given an easier problem to solve, followed by a harder problem. We use Deep Deterministic Policy Gradients to learn overtaking maneuvers for a car, in presence of multiple other cars, in a simulated highway scenario. The novelty of our approach lies in the training strategy used where we teach the agent to drive in a manner similar to the way humans learn to drive and the fact that our reward function uses only the raw sensor data at the current time step. This method, which resembles a curriculum learning approach is able to learn smooth maneuvers, largely collision free, wherein the agent overtakes all other cars, independent of the track and number of cars in the scene.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_06">16:00-18:00, Paper FrPS-SST12.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0221.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('221'); return false" title="Click to show or hide the keywords and abstract">Worst-Case Analysis of the Time-To-React Using Reachable Sets</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30474" title="Click to go to the Author Index">Söntges, Sebastian</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34567" title="Click to go to the Author Index">Koschi, Markus</a></td><td class="r">Tech. Univ. of Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13878" title="Click to go to the Author Index">Althoff, Matthias</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab221" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Collision mitigation and collision avoidance systems in intelligent vehicles reduce the severity and number of accidents. To determine the optimal point in time at which such systems should intervene, time-based criticality metrics such as the Time-To-React (TTR) are commonly used. The TTR describes the last point in time along the current trajectory at which an evasive trajectory exists. In this paper, we present a novel approach to determine the point in time after which it is guaranteed that no evasive maneuver exists, i.e., by using reachable sets, we over-approximate the TTR. Our deterministic upper bound of the TTR can be used to trigger a collision mitigation system or to find a feasible emergency maneuver which avoids the collision. We demonstrate the efficient computation of the tight over-approximated TTR in different urban and rural traffic scenarios, and compare our results to an estimated TTR using an optimization-based trajectory planner.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst12_07">16:00-18:00, Paper FrPS-SST12.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0627.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('627'); return false" title="Click to show or hide the keywords and abstract">Adaptive Stress Testing for Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37190" title="Click to go to the Author Index">Koren, Mark</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37193" title="Click to go to the Author Index">Alsaif, Saud</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37551" title="Click to go to the Author Index">Lee, Ritchie</a></td><td class="r">Carnegie Mellon Univ. Silicon Valley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18816" title="Click to go to the Author Index">Kochenderfer, Mykel</a></td><td class="r">Stanford Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab627" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper presents a method for testing the decision making systems of autonomous vehicles. Our approach involves perturbing stochastic elements in the vehicle's environment until the vehicle is involved in a collision. Instead of applying direct Monte Carlo sampling to find collision scenarios, we formulate the problem as a Markov decision process and use reinforcement learning algorithms to find the most likely failure scenarios. This paper presents Monte Carlo Tree Search (MCTS) and Deep Reinforcement Learning (DRL) solutions that can scale to large environments. We show that DRL can find more likely failure scenarios than MCTS with fewer calls to the simulator. A simulation scenario involving a vehicle approaching a crosswalk is used to validate the framework. Our proposed approach is very general and can be easily applied to other scenarios given the appropriate models of the vehicle and the environment.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst13"><b>FrPS-SST13</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst13" title="Click to go to the Program at a Glance"><b>Simulation and Case Analysis-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_01">16:00-18:00, Paper FrPS-SST13.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0648.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('648'); return false" title="Click to show or hide the keywords and abstract">Autonomous RC-Car for Education Purpose in Istem Projects (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14341" title="Click to go to the Author Index">Huang, Wu-Ling</a></td><td class="r">Inst. of Automation, Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37646" title="Click to go to the Author Index">WeiHang, Feng</a></td><td class="r">Beijing NO.161 High School</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11999" title="Click to go to the Author Index">Gong, Xiaoyan</a></td><td class="r">Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12766" title="Click to go to the Author Index">Zhu, Fenghua</a></td><td class="r">Inst. of Automation, Chinese Acad. of Sciences</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab648" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Software simulation and real environment running parallel execution is a lately proposed method, which also provides full coverage and convenience to accomplish autonomous driving education purpose. This paper introduces a new high-school iSTEM program of autonomous vehicles education. This program uses a Scaled RC-Car platform with several sensors and Raspberry Pi embedded platform, to build an autonomous driving car in scaled indoor simulation environment. The RC-Car is capable of safely autonomous driving. Many existing algorithms are put together to provide the necessary functions of autonomous driving, such lane detection, obstacle detection, lane following, vehicle control etc. In this paper, we provide the details of this program, hardware and software components of the RC-Car, Deep learning end-to-end approaching of algorithm deployment, and future works.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_02">16:00-18:00, Paper FrPS-SST13.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0619.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('619'); return false" title="Click to show or hide the keywords and abstract">An Automated Vehicle Safety Concept Based on Runtime Restriction of the Operational Design Domain</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36081" title="Click to go to the Author Index">Colwell, Ian</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37592" title="Click to go to the Author Index">Phan, Buu</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37590" title="Click to go to the Author Index">Saleem, Shahwar</a></td><td class="r">WISE LAB, Univ. OF WATERLOO</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37593" title="Click to go to the Author Index">Salay, Rick</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35731" title="Click to go to the Author Index">Czarnecki, Krzysztof</a></td><td class="r">Univ. of Waterloo</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab619" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> Automated vehicles need to operate safely in a wide range of environments and hazards. The complex systems that make up an automated vehicle must also ensure safety in the event of system failures. This paper proposes an approach and architectural design for achieving maximum functionality in the case of system failures. The Operational Design Domain (ODD) defines the domain over which the automated vehicle can operate safely. We propose modifying a runtime representation of the ODD based on current system capabilities. This enables the system to react with context-appropriate responses depending on the remaining degraded functionality. In addition to proposing an architectural design, we have implemented the approach to prove its viability. The proof of concept has shown promising directions for future work and moved our automated vehicle research platform closer to achieving level 4 automation.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_03">16:00-18:00, Paper FrPS-SST13.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0443.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('443'); return false" title="Click to show or hide the keywords and abstract">Taming Functional Deficiencies of Automated Driving Systems: A Methodology Framework Toward Safety Validation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36575" title="Click to go to the Author Index">Chen, Meng</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36602" title="Click to go to the Author Index">Knapp, Andreas</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36973" title="Click to go to the Author Index">Pohl, Martin</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10481" title="Click to go to the Author Index">Dietmayer, Klaus</a></td><td class="r">Univ. of Ulm</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab443" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Safety is one of the key aspects of road vehicles. With applications of machine learning and artificial intelligence (AI) technologies, driver assistance and automated driving systems have been rapidly developed. This paper identifies one of the emerging safety issues of automated driving systems: functional deficiencies resulting from limited sensing abilities and algorithmic performance. Safety validation problem and challenges for some methodologies provided by ISO 26262 are addressed. To this end, we provide a methodology framework for identifying functional deficiencies during system development. A novel methodology based on possibility theory and a fuzzy relation model, Causal Scenario Analysis (CSA), is introduced as one essential part in this framework. A traffic light handling case study is presented.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_04">16:00-18:00, Paper FrPS-SST13.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0560.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('560'); return false" title="Click to show or hide the keywords and abstract">Comparison of Observers for Vehicle Yaw Rate Estimation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37485" title="Click to go to the Author Index">Monot, Nolwenn</a></td><td class="r">IMS Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37488" title="Click to go to the Author Index">Moreau, Xavier</a></td><td class="r">Univ. De Bordeaux</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18475" title="Click to go to the Author Index">Benine-Neto, André</a></td><td class="r">Univ. De Bordeaux</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37489" title="Click to go to the Author Index">Rizzo, Audrey</a></td><td class="r">Groupe PSA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31507" title="Click to go to the Author Index">Aioun, Francois</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab560" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper presents a comparison of observers for vehicle lateral dynamics. Three model-based observers are compared: firstly Luenberger observer with a linear stationary model, secondly Kalman filter with a non stationary model and lastly Luenberger observers for each subsystems of a Takagi-Sugeno multi-model. Moreover, an analysis of the lateral dynamics shows that the model discretisation may be problematic according to the signal sampling theorem and a solution is proposed, such that all the observers can be used on board of the vehicle.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_05">16:00-18:00, Paper FrPS-SST13.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0134.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('134'); return false" title="Click to show or hide the keywords and abstract">Benefit Assessment of New Ecological and Safe Driving Algorithm Using Naturalistic Driving Data</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35973" title="Click to go to the Author Index">Ghasemi Dehkordi, Sepehr</a></td><td class="r">Queensland Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26144" title="Click to go to the Author Index">Larue, Gregoire Sebastien</a></td><td class="r">Queensland Univ. of Tech. (QUT)</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35983" title="Click to go to the Author Index">Cholette, Michael</a></td><td class="r">Queensland Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10648" title="Click to go to the Author Index">Rakotonirainy, Andry</a></td><td class="r">Queensland Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab134" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> A new Ecological and Safe (EcoSafe) driving control algorithm has been recently developed by the authors for controlling the longitudinal motion of the vehicle to minimize fuel consumption while respecting safety constraints. The algorithm uses a Model predictive control framework augmented with enhanced safety constraints based on Intervehicular Time (TIV) and the Time to Collision (TTC). This algorithm requires tuning to adapt to traffic condition. In this paper we propose a tuning method for EcoSafe algorithm which is deduced from driver preference and traffic flow information. In addition, to the best of our knowledge, the benefits of similar EcoSafe algorithms have not been tested with naturalistic data. Hence, we assessed the benefits of EcoSafe algorithm in terms of eco-driving and safety by using 1,100 km of naturalistic driving data. We use velocity profile extracted from the Australian Naturalistic Driving Study (ANDS) as the leading vehicle driving behaviour. The results show that our proposed strategy has a 14% reduction in fuel consumption on average while maintaining high safety levels without increasing travel time significantly.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_06">16:00-18:00, Paper FrPS-SST13.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0013.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('13'); return false" title="Click to show or hide the keywords and abstract">The Road Regional Hazard Level Evaluation Method Based on Ising Model</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28616" title="Click to go to the Author Index">Han, Qingwen</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35866" title="Click to go to the Author Index">Yin, Zhoutao</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35886" title="Click to go to the Author Index">Zeng, Lingqiu</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35888" title="Click to go to the Author Index">Liu, Xiaoying</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35885" title="Click to go to the Author Index">Ye, Lei</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35887" title="Click to go to the Author Index">Hu, Yifei</a></td><td class="r">Chongqing Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35884" title="Click to go to the Author Index">Lei, Jianmei</a></td><td class="r">State Key Lab. of Vehicle NVH and Safety Tech. & Chon</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab13" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Impact_on_Traffic_Flows" title="Click to go to the Keyword Index">Impact on Traffic Flows</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> On-road risk assessment is one of the important topics for intelligent vehicles research. Efficiency of risk assessment is affected by lack of on-road vehicles information, which should be solved by The Vehicular Ad-Hoc Network(VANET). In this paper, a regional vehicles hazard level estimation method, which employs Ising model to explain the intervehicles relationship, is proposed, an inter-vehicles relationship model is constructed, and corresponding parameters mapping relationship is studied. Then Ising model’s energy formula is used to illustrate regional hazard level. Moreover, based on the inter-vehicles relationship, we present an approach to build a regional threat distribution map, which indicates the passing threat of a certain ego-car. A series of experiments are done to examine the efficiency of the proposed method. Experiment results show that the proposed method can produce a reasonable danger evaluation for an ego-car.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst13_07">16:00-18:00, Paper FrPS-SST13.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0639.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('639'); return false" title="Click to show or hide the keywords and abstract">Multi-Model Traffic Scene Simulation with Road Image Sequences and GIS Information</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36390" title="Click to go to the Author Index">Cui, Zhichao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25095" title="Click to go to the Author Index">Liu, Yuehu</a></td><td class="r">Inst. of Artificial Intelligence and Robotics, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37100" title="Click to go to the Author Index">Ren, Fuji</a></td><td class="r">Tokushima Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37355" title="Click to go to the Author Index">Zhang, Qilin</a></td><td class="r">HERE Tech. Chicago, Illinois</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab639" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> In this paper, a new multi-modal traffic scene simulation framework with combined inputs of road image sequences and road information from Geographic Information Systems (GIS) is proposed. The proposed framework contains two major steps, with the first one being a preprocessing step, including 3D road model extraction, camera location and orientation estimation and lane extraction from both GIS and road image sequences. After such preprocessing, the traffic scene reconstruction is reformulated into a 6-degree of freedom (6DoF) pose estimation in the 3D road model. Subsequently, the iterative closest point (ICP) algorithm is exploited for coarse point registration by estimating the pose in the road model. In addition, an objective function is established to incorporate the image features (e.g., lanes) into the road model and to refine the pose estimation. In the experiments with the publicly available KITTI dataset, the proposed method achieves high average Intersection-over-Union (IoU) scores as compared to the ground truth image sequences.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst4"><b>FrPS-SST4</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst4" title="Click to go to the Program at a Glance"><b>Autonomous Driving Control-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst4_01">16:00-18:00, Paper FrPS-SST4.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0086.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('86'); return false" title="Click to show or hide the keywords and abstract">Training RNN Simulated Vehicle Controllers Using the SVD and Evolutionary Algorithms</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36116" title="Click to go to the Author Index">McNeill, Daniel Kyle</a></td><td class="r">Pol. Di Bari</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab86" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> We describe an approach to creating a controller for The Open Car Racing Simulator (TORCS), based on The Simulated Car Racing Championship (SCRC) client, using unsupervised evolutionary learning for recurrent neural networks. Our method of training the recurrent neural network controllers relies on combining the components of the singular value decomposition of two different neural network connection matrices.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst4_02">16:00-18:00, Paper FrPS-SST4.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0095.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('95'); return false" title="Click to show or hide the keywords and abstract">Efficient Mixed-Integer Programming for Longitudinal and Lateral Planning of Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36113" title="Click to go to the Author Index">Miller, Christina</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33055" title="Click to go to the Author Index">Pek, Christian</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13878" title="Click to go to the Author Index">Althoff, Matthias</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab95" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> The application of continuous optimization to motion planning of autonomous vehicles has enjoyed increasing popularity in recent years. In order to maintain low computation times, it is advantageous to have a convex formulation, in general requiring the planning problem to be separated into a longitudinal and lateral component. However, this decoupling of the motion often results in infeasible trajectories in situations in which both components need to be heavily linked, e.g., when planning swerving maneuvers to avoid a collision with obstacles. In this work, we propose an approach which extends the convex optimization problem of the longitudinal component to incorporate changing constraints, allowing us to guarantee feasibility of the resulting combined trajectory. Furthermore, we provide additional safety guarantees for the planned motion by integrating formal safety distances assuming infinite precision arithmetic. Our approach is demonstrated using simulated lane change maneuvers.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst4_03">16:00-18:00, Paper FrPS-SST4.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0115.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('115'); return false" title="Click to show or hide the keywords and abstract">A Minimum Swept Path Control Strategy for Reversing Articulated Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35853" title="Click to go to the Author Index">Liu, Xuanzuo</a></td><td class="r">Univ. of Cambridge</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29702" title="Click to go to the Author Index">Cebon, David</a></td><td class="r">Univ. of Cambridge</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab115" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Assistive_Mobility_Systems" title="Click to go to the Keyword Index">Assistive Mobility Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> This paper presents a new control strategy called Minimum Swept Path Control (MSPC) for reversing articulated heavy goods vehicles. It improves on previous Path Following Control (PFC) methods by minimising large excursions of the tractor unit. A preview distance is integrated into the MSPC algorithm to predict future vehicle states and feed them back into the control system to compensate state errors. The relationship between the length of preview distance and maximum lateral offsets is investigated, which can be used to optimise the preview distance. The relationship between maximum lateral offsets and the corresponding weights in the control cost function is also established in this preliminary study.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst4_04">16:00-18:00, Paper FrPS-SST4.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0362.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('362'); return false" title="Click to show or hide the keywords and abstract">Sliding Mode Control of a Dry-Type Two-Speed Dual Clutch Transmission for an Electric Vehicle During Optimal Power Transmission Process in Torque Phase</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36195" title="Click to go to the Author Index">Wu, Mingxiang</a></td><td class="r">Coll. of Engineering, Shanghai Normal Univ. TianHua Coll</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab362" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Electric_and_Hybrid_Technologies" title="Click to go to the Keyword Index">Electric and Hybrid Technologies</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> In order to properly control slip-to-slip shift process of electric vehicles (EVs) equipped with dry dual-clutch transmissions (DDCT), dynamic modelling of a dry-type two-speed dual-clutch transmission for an electric vehicle in torque phase is proposed at first. Then, an optimal control strategy is proposed to investigate feasibility of non-shock shift process of torque phase without power interruption and power circulation. Optimal solutions of electric motor torque and dual clutch friction torques are derived in analytical form. And then, a dynamic model of fork-lever actuator is integrated into DDCT driveline dynamic model of EV, and an affine nonlinear shift dynamic model for the whole DDCT system is proposed to describe dynamic behaviours in torque phases of shift. Further, to solve problems in tracking inaccuracy induced by strong nonlinearities and modelling uncertainties of the dynamic model, sliding mode control strategy based on feedback linearization control theory is proposed. Accurate electric motor torque as well as motor currents imposed to affine nonlinear system are calculated through nonlinear feedback control law. Finally, tracking control accuracy of the affine nonlinear dynamic system is investigated through numerical simulation on MATLAB/Simulink platform. The simulation results verify that not only non-shock shift process of torque phase without power interruption and power circulation is realized, but also torque phase time can be adjusted to an arbitrary value based on actual requirement. Besides, high-precision tracings to optimal angular velocities of electric motor and dual clutch are realized by accurately modulating motor control currents and electric motor torque.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst4_05">16:00-18:00, Paper FrPS-SST4.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0077.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('77'); return false" title="Click to show or hide the keywords and abstract">Accurate and Smooth Speed Control for an Autonomous Vehicle</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27347" title="Click to go to the Author Index">Xu, Shaobing</a></td><td class="r">Univ. of Michigan, Ann Arbor</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12018" title="Click to go to the Author Index">Peng, Huei</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36055" title="Click to go to the Author Index">Song, Ziyou</a></td><td class="r">Univ. of Michigan, Ann Arbor</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36077" title="Click to go to the Author Index">Chen, Kailiang</a></td><td class="r">SF Motors</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36076" title="Click to go to the Author Index">Tang, Yifan</a></td><td class="r">SF Motors</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab77" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> This paper presents a preview servo-loop speed control algorithm to achieve smooth, accurate, and computationally inexpensive speed tracking for connected automated vehicles (CAVs). Differing from methods neglecting the future road slope and target speed information, the proposed controller focuses on taking advantages of this accessible future information to achieve better speed tracking performance. It integrates the future slope and target speed into an augmented optimal control problem, by solving which we obtain the optimal control law in an analytical form. The brake/ throttle control laws consist of five parts, i.e., three feedback controls of system states and two feedforward items—preview of road slope and preview of target speed. This controller and its degenerate form, i.e., a classic PID, are implemented and applied to our automated vehicle platform, a Hybrid Lincoln MKZ. Experimental results show three major benefits of the proposed control—lower speed tracking errors, more gentle operations, and smoother brake/throttle behaviors.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst5"><b>FrPS-SST5</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst5" title="Click to go to the Program at a Glance"><b>ADAS-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_01">16:00-18:00, Paper FrPS-SST5.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0321.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('321'); return false" title="Click to show or hide the keywords and abstract">Driver Profiling by Using LSTM Networks with Kalman Filtering</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36630" title="Click to go to the Author Index">K&#322;usek, Adrian</a></td><td class="r">AGH Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36823" title="Click to go to the Author Index">Kurdziel, Marcin</a></td><td class="r">AGH Univ. of Science and Tech. Al. a Mickiewicza 30,</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36688" title="Click to go to the Author Index">Paciorek, Mateusz</a></td><td class="r">AGH Univ. of Science and Tech. Al. a Mickiewicza 30,</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36824" title="Click to go to the Author Index">Wawryka, Piotr</a></td><td class="r">AGH Univ. of Science and Tech. Al. a Mickiewicza 30,</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29307" title="Click to go to the Author Index">Turek, Wojciech</a></td><td class="r">AGH Univ. of Science and Tech. Faculty of Computer Sc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab321" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a></span><br>
                              <strong>Abstract:</strong> Nowadays, the most common way to model the driver behavior is to create, under some assumptions, a model of common patterns in driver maneuvers. These patterns are often modeled with averaged driver model. While this idea is very simple and intuitive in the context of driver classification by his/her patterns of maneuvers, our previous works demonstrated that assumptions underlying such models are often inaccurate and not applicable in general settings. In fact, it is very hard to express driving patterns with simple models. In this article we present a new way of modeling drivers: we employ Long-Short Term Memory networks to learn driver models from telematics data. In particular, our neural network models learn to predict driving-related signals, such as speed or acceleration, given the evolution of these signals up to the point of prediction. Solving this prediction task allows us to capture the behavioral model of the driver. We tested our models on several drivers, by predicting their future decisions. By learning our models on one driver and then evaluating them on another driver, we demonstrate that LSTM models are a powerful tool for driver profiling and detection of abnormal situations. We also evaluate the influence of data preprocessing on the quality of predictions. In this context we use Kalman filering, which can remove noise from uncertain dynamic measurements, in effect giving the best linearly estimated data.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_02">16:00-18:00, Paper FrPS-SST5.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0137.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('137'); return false" title="Click to show or hide the keywords and abstract">Driver Identification System Using Convolutional Neural Network with Background Removal-Based Infrared Data Augmentation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36277" title="Click to go to the Author Index">Kim, Sanghyuk</a></td><td class="r">Sogang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36279" title="Click to go to the Author Index">Lee, Yunsoo</a></td><td class="r">Sogang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36281" title="Click to go to the Author Index">Ahn, Namhyun</a></td><td class="r">Sogang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34875" title="Click to go to the Author Index">Kang, Suk-Ju</a></td><td class="r">Sogang Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab137" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> As the interest of the autonomous driving increases, techniques related to the advanced driver assistance system are evolving together. In this paper, we propose a novel driver identification system using convolutional neural network (CNN) with the background removal-based infrared image data augmentation. It helps to identify who a driver is, and provides the customized driving environment. The process for the proposed identification system is as follows. First, we acquire customized individual infrared images in a driving simulation environment. Second, we augment the large amount of data by using the background removal-based method and several image processing techniques. Third, the augmented data is trained by the low-complexity-based CNN method. Finally, we load all trained weights to the forward network for real-time processing. In the experimental results, the proposed system had the memory resource of 4,795 KB, which are up to 49.0822 times smaller than benchmark algorithms, and the average F1 score of 0.9418 for the driver identification accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_03">16:00-18:00, Paper FrPS-SST5.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0323.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('323'); return false" title="Click to show or hide the keywords and abstract">CG Benefited Driver Facial Landmark Localization across Large Rotation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36831" title="Click to go to the Author Index">Shi, Liang</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17467" title="Click to go to the Author Index">Dong, Yanchao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30603" title="Click to go to the Author Index">Yue, Jiguang</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36837" title="Click to go to the Author Index">Lin, Minjing</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36397" title="Click to go to the Author Index">Wang, Senbo</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36399" title="Click to go to the Author Index">Shen, Runjie</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36838" title="Click to go to the Author Index">Chang, Zhiming</a></td><td class="r">Datang Guoxin Binhai Offshore Wind Power Co., Ltd</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab323" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Facial landmark localization is a crucial initial step Driver Inattention Monitoring. The aim of this paper is to localize driver facial landmarks across large rotation, say [-90°, +90°] in yaw rotation, to cope with real driving conditions. The paper proposes a flexible pipe-line for creating automatically labeled face image to supply wanted dataset. The benefits of CG (Computer Graphics) techniques such as 3D face modelling and morphing, photorealistic rendering and ground truth generation are utilized. To the best of our knowledge this is the first time to combine CG rendering and automatic ground truth labelling techniques with face landmark localization algorithms. The effectiveness of the CG rendered data is proved by cross validation with Multi-PIE dataset. Landmark localization across large rotation is obtained by a system simply integrating the off-the-shelves algorithms and trained with the CG rendered data. The experiments of the implemented system on Multi-PIE and real persons show that it could localize facial landmarks across large rotation accurately and in real time.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_04">16:00-18:00, Paper FrPS-SST5.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0177.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('177'); return false" title="Click to show or hide the keywords and abstract">Improved Driving Behaviors Prediction Based on Fuzzy Logic-Hidden Markov Model (FL-HMM)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34370" title="Click to go to the Author Index">Deng, Qi</a></td><td class="r">Univ. of Duisburg-Essen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31666" title="Click to go to the Author Index">Soeffker, Dirk</a></td><td class="r">Univ. of Duisburg-Essen</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab177" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Research and development of human driving behaviors play an important role in the development of assistance systems. In this contribution, a driving behaviors prediction model is based on a newly developed approach combining different Hidden Markov Models (HMM) cooperatively combined by Fuzzy Logic (FL). Due to variations of individual human drivers decision behavior the task to classify related behaviors based on individually trained models is difficult. The FL approach will be used for additional distinction of driving scenes into very safe, safe, and dangerous driving scenarios. For each scenario corresponding HMMs will be trained. Three different driving behaviors including left/right lane change and lane keeping are modelled as hidden states for the HMM. Based on observations, the algorithm calculates the most possible driving behaviors through the observation sequences. Furthermore, the observed sequences are also used for training of HMM during modeling process. To improve the prediction performance of the model, a prefilter is proposed to quantize the collected signals into observed sequences with specific features.<p>To optimize the model performance NSGA-II was used to define the optimal thresholds of FL and the optimal prefilters of HMMs. Using experimental data from real human driving behaviors (taken from driving simulator) it can be concluded that selecting optimal thresholds will increase the performance of driving behaviors prediction. The effectiveness of the suggested fuzzy-based HMM has been successfully proved based on experiments.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_05">16:00-18:00, Paper FrPS-SST5.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0516.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('516'); return false" title="Click to show or hide the keywords and abstract">Multiclass Classification of Driver Perceived Workload Using Long Short-Term Memory Based Recurrent Neural Network</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31923" title="Click to go to the Author Index">Manawadu, Udara</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31926" title="Click to go to the Author Index">Kawano, Takahiro</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36521" title="Click to go to the Author Index">Murata, Shingo</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31921" title="Click to go to the Author Index">Kamezaki, Mitsuhiro</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36540" title="Click to go to the Author Index">Muramatsu, Junya</a></td><td class="r">TOYOTA Motor Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16766" title="Click to go to the Author Index">Sugano, Shigeki</a></td><td class="r">Waseda Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab516" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a></span><br>
                              <strong>Abstract:</strong> Human sensing enables intelligent vehicles to provide driver-adaptive support by classifying perceived workload into multiple levels. Objective of this study is to classify driver workload associated with traffic complexity into five levels. We conducted driving experiments in systematically varied traffic complexity levels in a simulator. We recorded driver physiological signals including electrocardiography, electrodermal activity, and electroencephalography. In addition, we integrated driver performance and subjective workload measures. Deep learning based models outperform statistical machine learning methods when dealing with dynamic time-series data with variable sequence lengths. We show that our long short-term memory based recurrent neural network model can classify driver perceived-workload into five classes with an accuracy of 74.5%. Since perceived workload differ between individual drivers for the same traffic situation, our results further highlight the significance of including driver characteristics such as driving style and workload sensitivity to achieve higher classification accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_06">16:00-18:00, Paper FrPS-SST5.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0608.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('608'); return false" title="Click to show or hide the keywords and abstract">Body Pose and Context Information for Driver Secondary Task Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34746" title="Click to go to the Author Index">Martin, Manuel</a></td><td class="r">Fraunhofer IOSB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37574" title="Click to go to the Author Index">Popp, Johannes</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37573" title="Click to go to the Author Index">Anneken, Mathias</a></td><td class="r">Fraunhofer IOSB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35021" title="Click to go to the Author Index">Voit, Michael</a></td><td class="r">Fraunhofer IOSB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18961" title="Click to go to the Author Index">Stiefelhagen, Rainer</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab608" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Distraction of the driver by secondary tasks is already dangerous while driving manually but especially in handover situations in an automated mode this can lead to critical situations. Currently, these tasks are not taken into account in most modern cars. We present a system that detects typical distracting secondary tasks in an efficient modular way. We first determine the body pose of the driver and afterwards use recurrent neuronal networks to estimate actions based on sequences of the captured body poses. Our system uses knowledge about the surroundings of the driver that is unique to the car environment. Our evaluation shows that this approach achieves better results than other state of the art systems for action recognition on our dataset.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_07">16:00-18:00, Paper FrPS-SST5.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0180.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('180'); return false" title="Click to show or hide the keywords and abstract">Personal Space of Autonomous Car's Passengers Sitting in the Driver's Seat</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36419" title="Click to go to the Author Index">Ferrier-Barbut, Eleonore</a></td><td class="r">Univ. Grenoble Alpes, Inria, Grenoble INP, 38000 Grenoble, Franc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34992" title="Click to go to the Author Index">Vaufreydaz, Dominique</a></td><td class="r">Univ. Grenoble-Alpes, CNRS, Inria, LIG, F-38000 Grenoble</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36420" title="Click to go to the Author Index">David, Jean-Alix</a></td><td class="r">Univ. Grenoble Alpes, Inria, Grenoble INP, 38000 Grenoble, Franc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36421" title="Click to go to the Author Index">Lussereau, Jérôme</a></td><td class="r">Univ. Grenoble Alpes, Inria, Grenoble INP, 38000 Grenoble, Franc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13536" title="Click to go to the Author Index">Spalanzani, Anne</a></td><td class="r">INRIA</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab180" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> This article deals with the specific context of an autonomous car navigating in an urban center within a shared space between pedestrians and cars. The driver delegates the control to the autonomous system while remaining seated in the driver's seat. The proposed study aims at giving a first insight into the definition of human perception of space applied to vehicles by testing the existence of a personal space around the car. It aims at measuring proxemic information about the driver's comfort zone in such conditions. Proxemics, or human perception of space, has been largely explored when applied to humans or to robots, leading to the concept of personal space, but poorly when applied to vehicles. In this article, we highlight the existence and the characteristics of a zone of comfort around the car which is not correlated to the risk of a collision between the car and other road users. Our experiment includes 19 volunteers using a virtual reality headset to look at 30 scenarios filmed in 360° from the point of view of a passenger sitting in the driver's seat of an autonomous car. They were asked to say &quot;stop&quot; when they felt discomfort visualizing the scenarios.<p>As said, the scenarios voluntarily avoid collision effect as we do not want to measure fear but discomfort. The scenarios involve one or three pedestrians walking past the car at different distances from the wings of the car, relative to the direction of motion of the car, on both sides. The car is either static or moving straight forward at different speeds. The results indicate the existence of a comfort zone around the car in which intrusion causes discomfort. The size of the comfort zone is sensitive neither to the side of the car where the pedestrian passes nor to the number of pedestrians...
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_08">16:00-18:00, Paper FrPS-SST5.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0385.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('385'); return false" title="Click to show or hide the keywords and abstract">Sparse Coding of Weather and Illuminations for ADAS and Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36961" title="Click to go to the Author Index">Cheng, Guo</a></td><td class="r">IUPUI</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18424" title="Click to go to the Author Index">Zheng, Jiang Yu</a></td><td class="r">IUPUI</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13895" title="Click to go to the Author Index">Murase, Hiroshi</a></td><td class="r">Nagoya Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab385" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Weather and illumination are critical factors in vision tasks such as road detection, vehicle recognition, and active lighting for autonomous vehicles and ADAS. Understanding the weather and illumination type in a vehicle driving view can guide visual sensing, control vehicle headlight and speed, etc. This paper uses sparse coding technique to identify weather types in driving video, given a set of bases from video samples covering a full spectrum of weather and illumination conditions. We sample traffic and architecture insensitive regions in each video frame for features and obtain clusters of weather and illuminations via unsupervised learning. Then, a set of keys are selected carefully according to the visual appearance of road and sky. For video input, sparse coding of each frame is calculated for representing the vehicle view robustly under a specific illumination. The linear combination of the basis from keys results in weather types for road recognition, active lighting, intelligent vehicle control, etc.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst5_09">16:00-18:00, Paper FrPS-SST5.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0109.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('109'); return false" title="Click to show or hide the keywords and abstract">Early Start Intention Detection of Cyclists Using Motion History Images and a Deep Residual Network</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25848" title="Click to go to the Author Index">Zernetsch, Stefan</a></td><td class="r">Univ. of Applied Sciences Aschaffenburg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36061" title="Click to go to the Author Index">Kress, Viktor</a></td><td class="r">Univ. of Applied Sciences Aschaffenburg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28429" title="Click to go to the Author Index">Sick, Bernhard</a></td><td class="r">Univ. of Kassel</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20548" title="Click to go to the Author Index">Doll, Konrad</a></td><td class="r">Univ. of Applied Sciences Aschaffenburg</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab109" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> In this article, we present a novel approach to detect starting motions of cyclists in real world traffic scenarios based on Motion History Images (MHIs). The method uses a deep Convolutional Neural Network (CNN) with a residual network architecture (ResNet), which is commonly used in image classification and detection tasks. By combining MHIs with a ResNet classifier and performing a frame by frame classification of the MHIs, we are able to detect starting motions in image sequences. The detection is performed using a wide angle stereo camera system at an urban intersection. We compare our algorithm to an existing method to detect movement transitions of pedestrians that uses MHIs in combination with a Histograms of Oriented Gradients (HOG) like descriptor and a Support Vector Machine (SVM), which we adapted to cyclists. To train and evaluate the methods a dataset containing MHIs of 394 cyclist starting motions was created. The results show that both methods can be used to detect starting motions of cyclists. Using the SVM approach, we were able to safely detect starting motions 0.506 s on average after the bicycle starts moving with an F1-score of 97.7%. The ResNet approach achieved an F1-score of 100% at an average detection time of 0.144 s. The ResNet approach outperformed the SVM approach in both robustness against false positive detections and detection time.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst6"><b>FrPS-SST6</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst6" title="Click to go to the Program at a Glance"><b>ADAS-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_01">16:00-18:00, Paper FrPS-SST6.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0609.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('609'); return false" title="Click to show or hide the keywords and abstract">Object of Fixation Estimation by Joint Analysis of Gaze and Object Dynamics</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24482" title="Click to go to the Author Index">Martin, Sujitha</a></td><td class="r">Honda Res. Inst. USA, Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18782" title="Click to go to the Author Index">Tawari, Ashish</a></td><td class="r">Honda Res. Inst. USA</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab609" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Determining object of fixation is an important factor in many application of intelligent vehicles including driver's situational awareness estimation. The objective of this work is to infer object of fixation given fixation is occurring. We propose a system architecture that identifies object tracks in the scene, derives object characteristics independent of and jointly with gaze behavior, and utilizes a spatio-temporal sensitive machine learning framework to estimate the likelihood of an object being the object of fixation. Performance evaluation is conducted on a dataset of on-road driving, centered around urban intersections, with manual annotations of object of fixation. Our proposed system can achieve up to 83% average precision accuracy when compared to baseline of 78%. Furthermore, comparing the effects of different combinations of object characteristics on precision and recall accuracy show promising insights on factors affecting reliable estimation of object of fixation.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_02">16:00-18:00, Paper FrPS-SST6.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0190.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('190'); return false" title="Click to show or hide the keywords and abstract">A Speech-Based On-Demand Intersection Assistant Prototype</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34701" title="Click to go to the Author Index">Orth, Dennis</a></td><td class="r">Ruhr-Univ. Bochum, Inst. of Communication Acoustics</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19757" title="Click to go to the Author Index">Bolder, Bram</a></td><td class="r">Honda Res. Inst. Europe GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36449" title="Click to go to the Author Index">Steinhardt, Nico</a></td><td class="r">Honda Res. Inst. Europe GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36450" title="Click to go to the Author Index">Dunn, Mark</a></td><td class="r">Honda Res. Inst. Europe GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34704" title="Click to go to the Author Index">Kolossa, Dorothea</a></td><td class="r">Ruhr-Univ. Bochum, Inst. of Communication Acoustics</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32634" title="Click to go to the Author Index">Heckmann, Martin</a></td><td class="r">Honda Res. Inst. Europe</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Novel_Interfaces_and_Displays" title="Click to go to the Keyword Index">Novel Interfaces and Displays</a></span><br>
                              <strong>Abstract:</strong> We have recently proposed a speech-based on-demand intersection assistant which helps the driver to handle urban intersections by informing him of the traffic situation on the right hand side and recommending suitable gaps in traffic. In a previous user study, conducted in a simulator, we could show that the system is in general well accepted and preferred by drivers compared to driving without assistance or with only visual support. In this paper, we report on an implementation of this system and its evaluation in real urban traffic. We use LIDAR sensors for the perception of the traffic environment. A scene analyzer estimates the gaps between the vehicles in real time. The result of this analysis is provided to a dialog manager, which uses it to inform the driver of approaching vehicles and suitable gaps. While approaching the intersection, the driver can activate the system via a wake-up-word and control it with subsequent speech commands. The design of the data analyzer and dialog manager is based on evaluations at real intersections. The resulting system can provide suitable support to the driver in a wide range of traffic situations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_03">16:00-18:00, Paper FrPS-SST6.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0261.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('261'); return false" title="Click to show or hide the keywords and abstract">Analysis of Driver Brake Behavior under Critical Cut-In Scenarios</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35868" title="Click to go to the Author Index">Feng, Zhiwei</a></td><td class="r">School of Automotive Studies, Tongji Univ. Shanghai</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36654" title="Click to go to the Author Index">Ma, Xuehan</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32201" title="Click to go to the Author Index">Zhu, Xichan</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32202" title="Click to go to the Author Index">Ma, Zhixiong</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a></span><br>
                              <strong>Abstract:</strong> Analysis of driver brake behavior parameters under critical cui-in scenarios is conducted based on naturalistic driving data collected in Shanghai, China. Time headway (THW) when brake initiation is chosen to evaluate driver’s timing of brake initiation. Average brake pressure change rate and maximum brake jerk are to evaluate driver brake response speed, average deceleration is to evaluate the effect of driver brake maneuver, maximum deceleration is to evaluate driver’s maximum brake strength. Effects of different factors on driver brake behavior parameters are analyzed by using one-way analysis of variance and linear regression analysis. Driver brake behavior parameters in this study are important for the development of longitudinal control systems of automated vehicles which are suitable for Chinese people.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_04">16:00-18:00, Paper FrPS-SST6.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0354.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('354'); return false" title="Click to show or hide the keywords and abstract">Joint Deep Neural Network Modelling and Statistical Analysis on Characterizing Driving Behaviors</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36768" title="Click to go to the Author Index">Wang, Yuhao</a></td><td class="r">The Hong Kong Pol. Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24604" title="Click to go to the Author Index">Ho, Ivan Wang-Hei</a></td><td class="r">The Hong Kong Pol. Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a></span><br>
                              <strong>Abstract:</strong> Google defines the concept of autonomous driving as one of the applications of big data. Specifically, with the input sensor data, the autonomous vehicles can be provided with the sematic-level driving characteristics for an accurate and safe driving control. However, both the enumeration of handcrafted driving features with expert knowledge and the feature classification with machine learning for characterizing driving behaviors is lack of practicability under a complex scale. Therefore, this study focuses on detecting the sematic-level driving behaviors from large-scale GPS sensor data. Specifically, we classified different driving maneuvers from a huge amount of dataset through a layer-by-layer statistical analysis method. The identified maneuver information with the corresponding driver ID is useful for the supervised learning of high-level feature abstraction with neural network. With the aim of analyzing the sensory data with deep learning in a consumable form, we propose a joint histogram feature map to regularize the shallow features in this paper. Besides, extensive simulation is conducted to evaluate different machine learning and deep learning methodologies for optimal driving behavior characterization. Overall, our results indicate that Deep Neural Network (DNN) is suitable for the driving maneuver classification task with more than 94% accuracy, while Long Short-term Memory (LSTM) neural network performs well with a 92% accuracy in identifying a specific driver. However, LSTM shows degraded accuracy when the scale of the identification task becomes larger. In this case, a hierarchical deep learning model is proposed, and simulation results show that the combination of DNN and LSTM in this hierarchical model can well maintain the prediction accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_05">16:00-18:00, Paper FrPS-SST6.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0123.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('123'); return false" title="Click to show or hide the keywords and abstract">Prediction of Human Driver Behaviors Based on an Improved HMM Approach</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34370" title="Click to go to the Author Index">Deng, Qi</a></td><td class="r">Univ. of Duisburg-Essen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31531" title="Click to go to the Author Index">Wang, Jiao</a></td><td class="r">Univ. of Duisburg-Essen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31666" title="Click to go to the Author Index">Soeffker, Dirk</a></td><td class="r">Univ. of Duisburg-Essen</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab123" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Research and development of predicting driving behaviors play an important role in the development of Advanced Driver Assistance Systems (ADAS) for assisting drivers. In this contribution, an approach is developed based on Hidden Markov Model (HMM) for predicting human driving behaviors. Three different driving maneuvers including left/right lane change and lane keeping are modelled as hidden states for the HMM. Based on observations (training), the HMM approach is able to calculate the most possible driving behaviors using observed sequences. Furthermore, the observed sequences are also used for training of HMM in the modeling process. To improve the prediction performance of the model, a prefilter is proposed to quantize the collected signals into observed sequences with specific features. <p>In this contribution the definition of a suitable prefilter will be discussed and finally optimized. The approach focuses on the definition of optimal prefilters. Here optimality is defined as the optimal segments describing a quantized prefilter mapping the vehicle's environment to quantized states. In combination with related HMM-based results in terms of accuracy, detection, and false alarm rates an optimal parameter set of the prefilter can be determined. Using experimental data from real human driving behaviors (taken from driving simulator) it can be concluded that the optimal definition of the prefilter can increase the detection rate and accuracy, and in the meanwhile decrease the false alarm rate. The effectiveness of driving behaviors prediction has been successfully proved by comparison with other methods in this contribution.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_06">16:00-18:00, Paper FrPS-SST6.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0006.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('6'); return false" title="Click to show or hide the keywords and abstract">Conceptual Design and Evaluation of a Human Machine Interface for Highly Automated Truck Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35856" title="Click to go to the Author Index">Richardson, Natalie</a></td><td class="r">Tech. Univ. of Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35858" title="Click to go to the Author Index">Lehmer, Christoph</a></td><td class="r">Tech. Univ. of Munich, Chair of Automotive Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35857" title="Click to go to the Author Index">Michel, Britta</a></td><td class="r">MAN Truck & Bus AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26551" title="Click to go to the Author Index">Lienkamp, Markus</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab6" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Vehicle automation is linked to various benefits such as an increase in fuel and transport efficiency, as well as an increase in driving comfort. Automation also comes with a variety of downsides e.g. loss of situation awareness, loss of skills as well as inappropriate trust levels regarding system functionality. Drawbacks differ between automation levels. As highly-automated driving (level 3) requires the driver to take over the driving task in critical situations within a limited period of time, the need for an appropriate human-machine interface (HMI) arises. To foster adequate and efficient human-machine interaction, this contribution presents a user-centered, iterative approach for HMI design for highly-automated truck driving. An expert workshop was conducted to develop first ideas and HMI sketches. Workshop results were combined with scientific findings regarding HMI design for highly-automated car driving. Based on those findings, a paper prototype was created and evaluated with experts, using an approach of mixed qualitative methods (heuristic evaluation, thinking aloud). The outcome was implemented to the HMI concept. In a third step, the HMI was conceptualized as video prototype enabling a more detailed evaluation. Again, experts were asked to assess the HMI using qualitative (thinking aloud) and quantitative methods (questionnaires). The result represents a video prototype showing a HMI strategy for highly-automated driving, aiming at fostering a successful human-machine interaction. Relevant issues such as drivers’ informational needs, situation awareness and trust were explicitly considered during HMI design. Next steps comprise HMI implementation and user evaluation in a driving simulator to let users experience the HMI in a semi-real driving context.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst6_07">16:00-18:00, Paper FrPS-SST6.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0541.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('541'); return false" title="Click to show or hide the keywords and abstract">Emergency Autonomous Vehicle Guidance under Steering Loss</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37443" title="Click to go to the Author Index">Boudali, Mohamed</a></td><td class="r">Univ. of Haute-Alsace</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23238" title="Click to go to the Author Index">Orjuela, Rodolfo</a></td><td class="r">Univ. De Haute-Alsace</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14036" title="Click to go to the Author Index">Basset, Michel</a></td><td class="r">Univ. of Haute-Alsace</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23237" title="Click to go to the Author Index">Attia, Rachid</a></td><td class="r">Univ. De Haute-Alsace</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab541" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> The autonomous vehicle guidance needs a steering system which is able to handle the lateral dynamics and a throttle/braking system to handle the longitudinal dynamics. However, a failure in the steering system leads the vehicle in dangerous situation. In order to manage this situation, an emergency guidance control architecture aims to guide and stop the vehicle in a safe area is proposed here. To that end, an emergency guidance controller (EGC) is developed to ensure the lateral guidance as well as the longitudinal guidance using braking torques. Since the same actuators (brakes) are employed for both control objectives a managing mechanism is proposed. Finally, simulation tests are carried out to show the effectiveness of the proposed approach.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst7"><b>FrPS-SST7</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst7" title="Click to go to the Program at a Glance"><b> Path Planning and Motion Classification</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_01">16:00-18:00, Paper FrPS-SST7.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0140.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('140'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Graph Search Based Local Path Planning with Adaptive Node Sampling</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36014" title="Click to go to the Author Index">Katsuki, Rie</a></td><td class="r">Toshiba Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36275" title="Click to go to the Author Index">Tasaki, Tsuyoshi</a></td><td class="r">Toshiba</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36268" title="Click to go to the Author Index">Watanabe, Tomoki</a></td><td class="r">Toshiba Corp</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab140" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0140.VD.mpg" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper describes a graph search–based local path planner with an adaptive node sampling according to positions of obstacles. Randomly sampled nodes of a graph in a traversable region for finding a local path can generate a winding path due to connection between the randomly sampling nodes. Node sampling with constant intervals can fail to find a proper path in a case that an interval between adjacent obstacles are smaller than the sampling interval. To solve these problems, our path planner changes node sampling strategy according to obstacle positions; it samples nodes along a reference path, e.g. the center line of the lane, if there are no obstacles, but densely samples nodes around obstacles. After sampling, the planner searches the graph using Dijkstra’s algorithm for finding an optimal trajectory. For efficient search for the optimal trajectory, the planner firstly generates a trajectory approximated by piecewise linear lines with minimum cost, and fine-tunes it by adjusting node positions with curved edges having smaller cost. We test the planner through simulations in which an ego vehicle drive along a reference path when there are no obstacles, and densely sample around obstacles to improve robustness against obstacle locations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_02">16:00-18:00, Paper FrPS-SST7.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0428.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('428'); return false" title="Click to show or hide the keywords and abstract">Combining Lattice-Based Planning and Path Optimization in Autonomous Heavy Duty Vehicle Applications</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35407" title="Click to go to the Author Index">Oliveira, Rui</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34431" title="Click to go to the Author Index">Cirillo, Marcello</a></td><td class="r">Scania</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22526" title="Click to go to the Author Index">Mårtensson, Jonas</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31595" title="Click to go to the Author Index">Wahlberg, Bo</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab428" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Lattice-based motion planners are an established method for generation of feasible motions for car-like vehicles. However the solution paths can only reach a discretized approximation of the intended goal pose. Moreover, the solutions are optimal only with respect to the actions available to the planner, which can result in paths with excessive steering. These drawbacks have a negative impact when used in real systems. In this paper we address both drawbacks by integrating a steering method into a state-of-the-art lattice-based motion planner. Unlike previous approaches, in which path optimization happens in an a posteriori step after the planner has found a solution, we propose an interleaved execution of path planning and path optimization. The proposed approach is real-time capable and implemented in a full-size autonomous truck, and we show experimentally that it is able to greatly improve the quality of solutions provided by a lattice planner.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_03">16:00-18:00, Paper FrPS-SST7.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0647.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('647'); return false" title="Click to show or hide the keywords and abstract">Path Optimization for a Wheel Loader Considering Construction Site Terrain</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37645" title="Click to go to the Author Index">Hong, Beichuan</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12426" title="Click to go to the Author Index">Ma, Xiaoliang</a></td><td class="r">Royal Inst. of Tech. (KTH)</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab647" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Wheel loader is one of the most widely used heavy-duty vehicles for transporting building materials in construction site. Improvement of its efficiency is important for sustainable transport and construction operations. This paper proposes a path optimization approach that allows us to plan loader trajectory and corresponding vehicle motions in construction site when the topological relief information is available. Vehicle dynamics is modeled for 3D motions considering the power balance of vehicle propulsion. The path planning problem is then formulated using a framework of constrained optimal control where vehicle dynamics is incorporated as system constraints. In order to solve the problem, a discrete search method is developed based on the principle of dynamic programming (DP), in which the states of the forward and backward movement paths of wheel loader are explored in parallel. A numerical study is then presented to demonstrate the application of the proposed approach for optimizing the loader path using terrain information.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_04">16:00-18:00, Paper FrPS-SST7.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0493.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('493'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Learning to Drive: Using Visual Odometry to Bootstrap Deep Learning for Off-Road Path Prediction</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30954" title="Click to go to the Author Index">Holder, Christopher</a></td><td class="r">Durham Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16017" title="Click to go to the Author Index">Breckon, Toby</a></td><td class="r">Durham Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab493" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0493.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Autonomous driving is a field currently gaining a lot of attention, and recently ‘end-to-end’ approaches, whereby a machine learning algorithm learns to drive by emulating human drivers, have demonstrated significant potential. However, recent work has focused on the on-road environment, rather than the much more challenging off-road environment. In this work we propose a new approach to this problem, whereby instead of learning to predict immediate driver control inputs, we train a deep convolutional neural network (CNN) to predict the future path that a vehicle will take through an off-road environment visually, addressing several limitations inherent in existing methods. We combine a novel approach to automatic training data creation, making use of stereoscopic visual odometry, with a state of the art CNN architecture to map a predicted route directly onto image pixels, and demonstrate the effectiveness of our approach using our own off-road data set.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_05">16:00-18:00, Paper FrPS-SST7.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0508.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('508'); return false" title="Click to show or hide the keywords and abstract">Exploring the Potential of Using Semantic Context and Common Sense in On-Road Vehicle Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33362" title="Click to go to the Author Index">Nan, Zhixiong</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35969" title="Click to go to the Author Index">Pan, Menghan</a></td><td class="r">Xi'an JiaoTong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30202" title="Click to go to the Author Index">Wang, Xiao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36183" title="Click to go to the Author Index">Wei, Ping</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30197" title="Click to go to the Author Index">Xu, Linhai</a></td><td class="r">Xi’an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30196" title="Click to go to the Author Index">Sun, Hongbin</a></td><td class="r">Xi’an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30143" title="Click to go to the Author Index">Xin, Jingmin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab508" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Vehicle detection is an important research topic for autonomous driving community. Since the great success of deep learning on object detection, almost all vehicle detection methods go along with this line. However, deep learning methods heavily rely on the training data, and the whole mechanism is like a ``black box&quot;. Therefore, in this paper, we explore a vehicle detection method using traffic semantic context and human common sense instead of relying on the training data. To verify our idea, we compare our method with two classic machine learning methods as well as three state-of-the-art deep learning methods on a dataset collected in real traffics. The results show that our method outperforms others on this dataset. The deep learning methods may exceed ours after enlarging the training data or testing on more complicated datasets. However, the main contribution of this paper is providing inspiration for learning methods, and we believe their performance can be greatly improved after considering the idea of this paper.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_06">16:00-18:00, Paper FrPS-SST7.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0316.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('316'); return false" title="Click to show or hide the keywords and abstract">MB-Net: MergeBoxes for Real-Time 3D Vehicles Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36820" title="Click to go to the Author Index">Gählert, Nils</a></td><td class="r">Daimler AG, Univ. of Jena</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36830" title="Click to go to the Author Index">Mayer, Marina</a></td><td class="r">Daimler AG, Hochschule Für Tech. Stuttgart</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31831" title="Click to go to the Author Index">Schneider, Lukas</a></td><td class="r">Daimler, ETH Zurich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10226" title="Click to go to the Author Index">Franke, Uwe</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15777" title="Click to go to the Author Index">Denzler, Joachim</a></td><td class="r">Friedrich-Schiller-Univ. Jena</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab316" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> RGB images is essential for driver assistance systems as well as for autonomous vehicles. Classical 2D box-based detection schemes allow roughly estimating the position of other vehicles, but not their orientation relative to the ego-vehicle. Recent approaches use 3D models to derive the pose of other vehicles from single monocular images but do not reach realtime performance. In this paper we present an approach that achieves competitive performance on the challenging KITTI Object Detection and Orientation Estimation benchmark while being the fastest approach with over 40 FPS. The key is a novel representation named MergeBox whose parameters can be estimated extremely efficiently. We extend SSD – a current fast state-of-the-art 2D box object detector – with this representation to our MB-Net. In contrast to all other current state-of-the-art methods we do not require explicit information on the object orientation for training our model. This reduces label costs significantly, a further advantage for practical applications that require labeling of databases that are much bigger than those used for research.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_07">16:00-18:00, Paper FrPS-SST7.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0440.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('440'); return false" title="Click to show or hide the keywords and abstract">Modeling and Predicting Vehicle Motion Activities by Using And-Or Graph</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37142" title="Click to go to the Author Index">Wang, Shuofeng</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11351" title="Click to go to the Author Index">Li, Li</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34596" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Cranfield Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab440" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> The ability of modeling and predicting vehicle motion activities is important for automated vehicles. In this paper, we propose an And-or Graph based model to give a simple and clear description of motion activities. Compared to other models, this new model relaxes the Markov property requirement in transition between activities and is thus more flexible. The parameters of this model can be easily learned from data. Using the trained new model, we can predict the on-going motion activity and label its corresponding probability. Experiments show that a high prediction accuracy 97 can be achieved by this new model.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst7_08">16:00-18:00, Paper FrPS-SST7.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0466.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('466'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Fast Multi Model Motion Segmentation on Road Scenes</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36204" title="Click to go to the Author Index">Sandhu, Mahtab</a></td><td class="r">International Inst. of Information Tech. Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37224" title="Click to go to the Author Index">Haque, Nazrul</a></td><td class="r">International Inst. of Information Tech. Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37226" title="Click to go to the Author Index">Sharma, Avinash</a></td><td class="r">International Inst. of Information Tech. Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23891" title="Click to go to the Author Index">Krishna, K Madhava</a></td><td class="r">IIIT Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37230" title="Click to go to the Author Index">Medasani, Shanti</a></td><td class="r">Mathworks</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab466" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0466.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> We propose a novel motion clustering formula- tion over spatio-temporal depth images obtained from stereo sequences that segments multiple motion models in the scene in an unsupervised manner. The motion models are obtained at frame rates that compete with the speed of the stereo depth computation. This is possible due to a decoupling framework that first delineates spatial clusters and subsequently assigns motion labels to each of these cluster with analysis of a novel motion graph model. A principled computation of the weights of the motion graph that signifies the relative shear and stretch between possible clusters lends itself to a high fidelity segmentation of the motion models in the scene. The fidelity is vindicated through accuracies reaching 89.61% on KITTI and complex native sequences.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst8"><b>FrPS-SST8</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst8" title="Click to go to the Program at a Glance"><b> Visual Sensing-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_01">16:00-18:00, Paper FrPS-SST8.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0372.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('372'); return false" title="Click to show or hide the keywords and abstract">Generalized B-Spline Camera Model</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22438" title="Click to go to the Author Index">Beck, Johannes</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10216" title="Click to go to the Author Index">Stiller, Christoph</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab372" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Previously proposed camera calibration methods either use a local camera model in a complex, cumbersome, time consuming and often manual calibration process or a lens specific global camera model, which can be automatically calibrated by simply recording images of chessboards. The drawback of using a global hand crafted camera model is its limited capability of modeling distortions caused by the mounted lens or optical devices in front of the lens like windshields.<p>Therefore, we propose a local camera model based on B-splines which can handle various distortions. Moreover, it will be shown how such a model can be calibrated in an easy-to-use calibration process which were up to now only applicable to global camera models.<p>We demonstrate the benefit of using the proposed local camera model by an extensive evaluation using single and multi-camera setups with different types of lenses, some mounted behind a windshield.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_02">16:00-18:00, Paper FrPS-SST8.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0374.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('374'); return false" title="Click to show or hide the keywords and abstract">CyDet: Improving Camera-Based Cyclist Recognition Accuracy with Known Cycling Jersey Patterns</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36921" title="Click to go to the Author Index">Masalov, Alexander</a></td><td class="r">WINKAM</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18416" title="Click to go to the Author Index">Ota, Jeffrey</a></td><td class="r">Intel</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36917" title="Click to go to the Author Index">Corbet, Heath Edwin</a></td><td class="r">Specialized Bicycle Components</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36918" title="Click to go to the Author Index">Lee, Eric</a></td><td class="r">Specialized Bicycles</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36919" title="Click to go to the Author Index">Pelley, Adam</a></td><td class="r">Specialized Bicycle Components</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab374" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> In this work, we propose CyDet, a hybrid approach to detection and classification based on human knowledge of the scenario context and statistical machine learning approaches to object detection, classification, and tracking. This approach enabled us to experiment with five different methods to maximize the accuracy of cyclist detection. These methods included detecting a cyclist as a single object, utilizing different classifiers for different view angles, using HOG and LBP with fewer descriptors and features, using a machine learning layer for classifier training, and uniquely, custom designed jerseys by Specialized Bicycles. By working closely with Specialized, we were able to ensure that designs were both optimized for computer vision algorithm detection and productizable, and we were able to train our algorithms to classify the known designs as cyclists which further improved our accuracy, especially when the cyclist is partially occluded. Our experiments show that our approach achieves state of the art accuracy on the KITTI benchmark, and we establish a baseline detection rate with our approach on a new, open Specialized Bicycles Cyclist Detection Dataset that include cyclists wearing jerseys with known patterns.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_03">16:00-18:00, Paper FrPS-SST8.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0188.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('188'); return false" title="Click to show or hide the keywords and abstract">Fast Multi-Pass 3D Point Segmentation Based on a Structured Mesh Graph for Ground Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34322" title="Click to go to the Author Index">Burger, Patrick</a></td><td class="r">Univ. of the Bundeswehr Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15680" title="Click to go to the Author Index">Wuensche, Hans Joachim Joe</a></td><td class="r">Univ. BW Munich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Point-cloud segmentation of 3D LiDAR scans is an important preprocessing task for autonomous vehicles in on-road and especially in off-road scenarios. Clustering point measurements with the same properties into multiple homogeneous regions is a challenging task due to an uneven sampling density and lack of explicit structural information. This paper presents a novel technique to achieve a robust and fast point-cloud segmentation using the characteristic intrinsic sensor pattern. This pattern is characterized by the mounting position of each laser diode. A structured mesh graph is created by taking the beam calibration and the chronology of incoming data packets into account. The proposed graph-based, multi-pass point segmentation algorithm compares this pattern with a flat-world model to detect discontinuities and to set label attributes such as obstacle or free space for each vertex. Furthermore, we directly detect missing measurements and therefore generate artificial vertices considering the laser beam intrinsics. Finally, a region-growing algorithm is applied in order to obtain cohesive objects. Experimental results show that we achieve a reliable overall performance and a good trade-off between segmentation quality and runtime of 15 ms in rough terrain as well as suburban areas.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_04">16:00-18:00, Paper FrPS-SST8.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0577.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('577'); return false" title="Click to show or hide the keywords and abstract">Evaluation of Synthetic Video Data in Machine Learning Approaches for Parking Space Classification</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33316" title="Click to go to the Author Index">Horn, Daniela</a></td><td class="r">Ruhr-Univ. Bochum</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20584" title="Click to go to the Author Index">Houben, Sebastian</a></td><td class="r">Ruhr-Univ. Bochum</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab577" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a></span><br>
                              <strong>Abstract:</strong> Most modern computer vision techniques rely on large amounts of meticulously annotated data for training and evaluation. In close-to-market development, this demand is even higher since numerous common and—more important— less common situations have to be tested and must hence be covered datawise. However, gathering the necessary amount of data ready-labeled for the task at hand is a challenge of its own. Depending on the complexity of the objective and the chosen approach, the required amount of data can be vast. At the same time, the effort to capture all possible cases of a given problem grows with their variability. This makes recording new video data unfeasible, even impossible at times. In this work, we regard parking space classification as an exemplary application to target the imbalance of cost and benefit w.r.t. image data creation for machine learning approaches. We rely on a fully-fledged park deck simulation created with Unreal Engine 4 for data creation and replace all conventionally recorded and hand-labeled training data by automatically-annotated synthetic video data. We train several of-the-shelf classifiers with a common choice of feature inputs on synthetic images only and evaluate them on two realworld sequences of different outdoor car parks. We reach a classification performance that matches our previous work on this task in which all classifiers were developed solely with real-life video data.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_05">16:00-18:00, Paper FrPS-SST8.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0525.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('525'); return false" title="Click to show or hide the keywords and abstract">Dual Viewpoint Passenger State Classification Using 3D CNNs</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37340" title="Click to go to the Author Index">Tu, Ian</a></td><td class="r">Univ. of Warwick</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37653" title="Click to go to the Author Index">Bhalerao, Abhir</a></td><td class="r">Univ. of Warwick</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37654" title="Click to go to the Author Index">Griffiths, Nathan</a></td><td class="r">Univ. of Warwick</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37655" title="Click to go to the Author Index">Delgado, Mauricio Muñoz</a></td><td class="r">Jaguar Land Rover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37656" title="Click to go to the Author Index">Thomason, Alasdair</a></td><td class="r">Jaguar Land Rover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35003" title="Click to go to the Author Index">Popham, Thomas</a></td><td class="r">Jaguar Land Rover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16351" title="Click to go to the Author Index">Mouzakitis, Alexandros</a></td><td class="r">Jaguar Land Rover</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab525" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> The rise of intelligent vehicle systems will lead to more human-machine interactions and so there is a need to create a bridge between the system and the actions and behaviours of the people inside the vehicle. In this paper, we propose a dual camera setup to monitor the actions and behaviour of vehicle passengers and a deep learning architecture which can utilise video data to classify a range of actions. The method incorporates two different views as input to a 3D convolutional network and uses transfer learning from other action recognition data. The performance of this method is evaluated using an in-vehicle dataset, which contains video recordings of people performing a range of common in-vehicle actions. We show that the combination of transfer learning and using dual viewpoints in a 3D action recognition network offers an increase in classification accuracy of action classes with distinct poses, e.g. mobile phone use and sleeping, whilst it does not apply as well for classifying those actions with small movements, such as talking and eating.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_06">16:00-18:00, Paper FrPS-SST8.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0467.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('467'); return false" title="Click to show or hide the keywords and abstract">Performance Evaluation of Local Image Features for Multinational Vehicle License Plate Verification</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34301" title="Click to go to the Author Index">Asif, Muhammad Rizwan</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34302" title="Click to go to the Author Index">Qi, Chun</a></td><td class="r">School of Electronics and Information Engineering, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34305" title="Click to go to the Author Index">Bibi, Irfana</a></td><td class="r">School of Computer Science and Tech. Xidian Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34304" title="Click to go to the Author Index">Fareed, Muhammad Sadiq</a></td><td class="r">School of Electronics and Information Engineering, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38615" title="Click to go to the Author Index">Zhang, Zhe</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38616" title="Click to go to the Author Index">Zhang, Zhaoqiang</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab467" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Smart_Infrastructure" title="Click to go to the Keyword Index">Smart Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a></span><br>
                              <strong>Abstract:</strong> The verification of vehicle License Plates (LPs) has not been given much importance in existing LP recognition systems as only a handful of methods deal with this problem explicitly. For an efficient system, it is imperative that a detected LP is validated first before the recognition of characters on it. Majority of the existing methods make use of geometrical constraints for the elimination of false LP regions which is not an effective way as multinational LPs have variable geometrical attributes and diversity in styles. To overcome these limitations, in this paper, we evaluate three kinds of representative local descriptors (SURF, HOG and LBP) and their combinations along with AlexNet CNN for the classification of LP and non-LP regions to provide a unique solution for the validation of multinational LPs. Experiments on 13490 LP and non-LP images show that the HOG feature individually gives the best recognition rate of 96.94% while considering collectively, best of 98.35% is achieved for SURF+HOG; whereas, the fine-tuned AlexNet outperform all others in terms of recognition accuracy of 99.27% but requires extensive processing. Furthermore, the proposed model is incorporated in one of the existing LP detection methods to demonstrate improved performance.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_07">16:00-18:00, Paper FrPS-SST8.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0593.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('593'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Monocular Depth Estimation by Learning from Heterogeneous Datasets</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36501" title="Click to go to the Author Index">Gurram, Akhil</a></td><td class="r">Huawei European Res. Center</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36612" title="Click to go to the Author Index">Urfalioglu, Onay</a></td><td class="r">Huawei Res. Center</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36499" title="Click to go to the Author Index">Halfaoui, Ibrahim</a></td><td class="r">Huawei</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36504" title="Click to go to the Author Index">Bouzaraa, Fahd</a></td><td class="r">Huawei Tech. Duesseldorf GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11454" title="Click to go to the Author Index">López, Antonio M.</a></td><td class="r">Univ. Autònoma De Barcelona</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab593" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0593.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Depth estimation provides essential information to perform autonomous driving and driver assistance. Especially, Monocular Depth Estimation is interesting from a practical point of view, since using a single camera is cheaper than many other options and avoids the need for continuous calibration strategies as required by stereo-vision approaches. State-of-the-art methods for Monocular Depth Estimation are based on Convolutional Neural Networks (CNNs). A promising line of work consists of introducing additional semantic information about the traffic scene when training CNNs for depth estimation. In practice, this means that the depth data used for CNN training is complemented with images having pixel-wise semantic labels, which usually are difficult to annotate (e.g. crowded urban images). Moreover, so far it is common practice to assume that the same raw training data is associated with both types of ground truth, i.e., depth and semantic labels. The main contribution of this paper is to show that this hard constraint can be circumvented, i.e., that we can train CNNs for depth estimation by leveraging the depth and semantic information coming from heterogeneous datasets. In order to illustrate the benefits of our approach, we combine KITTI depth and Cityscapes semantic segmentation datasets, outperforming state-of-the-art results on Monocular Depth Estimation.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst8_08">16:00-18:00, Paper FrPS-SST8.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0535.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('535'); return false" title="Click to show or hide the keywords and abstract">Vision and Dead Reckoning-Based End-To-End Parking for Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37425" title="Click to go to the Author Index">Rathour, Swarn Singh</a></td><td class="r">Tti Nagoya</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28538" title="Click to go to the Author Index">John, Vijay</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36018" title="Click to go to the Author Index">Meenakshi Karunakaran, Nithilan</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15903" title="Click to go to the Author Index">Mita, Seiichi</a></td><td class="r">Toyota Tech. Inst</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab535" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> In this paper a combined vision and dead reckoning-based parking system for end-to-end driving is proposed. Standard autonomous parking frameworks contain multiple modules with each module having its own limitation. On the other hand, the proposed parking framework consists of a single end-to-end module, which reduces these inherent limitations. In the proposed deep learning-based parking system, a novel iterative two-stage learning framework is utilized to predict the steering angles and gear status using a front and back mounted monocular camera. In the first stage of the proposed framework, the encoder-decoder architecture is used to predict an initial estimate of the steering angle trajectory from multiple frames of the front or the back monocular camera. The camera used for steering estimated is selected using the gear status estimate. The gear status is predefined during initialization and estimated subsequently in the second stage of the proposed framework. In the second stage of the proposed framework, the initial estimate of the steering angle trajectory along with the vehicles heading angle, and absolute position is given as an input to the long short-term memory network to estimate the optimal steering angle and gear status. The proposed framework is validated on an acquired dataset. A comparative analysis with baseline algorithms and detailed parametric analysis are performed. The experimental results show that the proposed framework is better than the baseline end-to-end algorithms.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst14"><b>FrPS-SST14</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst14" title="Click to go to the Program at a Glance"><b>Decision Making</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst14_01">16:00-18:00, Paper FrPS-SST14.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0060.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('60'); return false" title="Click to show or hide the keywords and abstract">&#61472; Constraint Decision Optimization Model for Safe Autonomous Vehicle Operation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24269" title="Click to go to the Author Index">Celenk, Mehmet</a></td><td class="r">Ohio Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33840" title="Click to go to the Author Index">Riley, Bryan</a></td><td class="r">Ohio Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab60" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Accuracy in the determination of positioning of the self-driving vehicle relative to lane markers and road boundaries during travel in inclement weather conditions continues to be of paramount importance. This paper presents an investigation and associated results where road land boundary markers are detected in conjunction with the ability decipher the horizon when the front view of the vehicle’s path is degraded. Degradation of driving scenes can be attributed to such weather conditions as heavy rain, fog, snow or dust storms. The detection of lane markers and road boundaries is especially important for roads that exhibit severe curves, aggressive uphill slopes and downhill valleys, respectively. Here, we present a model to predict deviations from reference distances associated with roads with such design constraints. To address self-driving objectives a method is proposed based on the Least Mean Square (LMS) optimization and the orthogonality principle. The paper also presents a design methodology of the concepts to address autonomous operation of passenger vehicles with some promising experimental results.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst14_02">16:00-18:00, Paper FrPS-SST14.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0293.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('293'); return false" title="Click to show or hide the keywords and abstract">Multi-Criteria Decision Making for Autonomous Vehicles Using Fuzzy Dempster-Shafer Reasoning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35308" title="Click to go to the Author Index">Claussmann, Laurene</a></td><td class="r">VEDECOM</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36544" title="Click to go to the Author Index">O'Brien, Marie</a></td><td class="r">The Univ. of British Columbia Okanagan</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10743" title="Click to go to the Author Index">Glaser, Sébastien</a></td><td class="r">Queensland Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36192" title="Click to go to the Author Index">Najjaran, Homayoun</a></td><td class="r">The Univ. of British Columbia</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11746" title="Click to go to the Author Index">Gruyer, Dominique</a></td><td class="r">IFSTTAR</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab293" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> This article considers the problem of high-level decision process for autonomous vehicles on highways. The goal is to select a predictive reference trajectory among a set of candidate ones, issued from a trajectory generator. This selection aims at optimizing multi-criteria functions, such as safety, legal rules, preferences and comfort of passengers, or energy consumption. This work introduces a new framework for Multi-Criteria Decision Making (MCDM). The proposed approach adopts fuzzy logic theory to deal with heterogeneous criteria and arbitrary functions. Moreover, the consideration of uncertain vehicle's sensors data is done using the Dempster-Shafer Theory with fuzzy sets in order to provide a risk assessment. Simulation results using datasets collected under the NGSIM program are presented on car following cases, and extended to lane changing situations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst14_03">16:00-18:00, Paper FrPS-SST14.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0615.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('615'); return false" title="Click to show or hide the keywords and abstract">Exploiting Hierarchy for Scalable Decision Making in Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37583" title="Click to go to the Author Index">Sonu, Ekhlas</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37585" title="Click to go to the Author Index">Sunberg, Zachary</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18816" title="Click to go to the Author Index">Kochenderfer, Mykel</a></td><td class="r">Stanford Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab615" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> A major challenge in autonomous driving has been the intractability of planning algorithms. Research has largely focused on simple, short-term scenarios with few interacting traffic participants. We propose a hierarchical approach for long-horizon tactical planning in large-scale autonomous driving settings. Our approach exploits the locality of interactions with other agents by sequentially setting and accomplishing shortterm goals involving fewer agents and hence is able to scale to more traffic participants. We demonstrate the effectiveness of our approach on an example highway driving problem where the ego vehicle must safely transit to the farthest lane in order to exit the highway at a designated exit.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst14_04">16:00-18:00, Paper FrPS-SST14.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0351.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('351'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Toward Human-Like Behavior Generation in Urban Environment Based on Markov Decision Process with Hybrid Potential Maps</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18355" title="Click to go to the Author Index">Guo, Chunzhao</a></td><td class="r">Toyota Central R&D Labs., Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14043" title="Click to go to the Author Index">Kidono, Kiyosumi</a></td><td class="r">Toyota Central R&D Labs., Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16374" title="Click to go to the Author Index">Terashima, Ryuta</a></td><td class="r">Toyota Central R&D Labs., INC</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13897" title="Click to go to the Author Index">Kojima, Yoshiko</a></td><td class="r">TOYOTA Central R&D Labs., Inc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab351" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0351.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> It is crucial for automated vehicles to interact with the surrounding vehicles properly in urban environment that can make both passengers and surrounding drivers feel safe and at ease. In this paper, we propose a human-like behavior generation approach, which can make safe as well as efficient decisions and generate an appropriate path with the corresponding speed profile to interact with the surrounding vehicles. Firstly, a cumulative cost map is constructed by integrating the hybrid potential maps of the surrounding vehicles according to their categories. Subsequently, three candidate paths, i.e., a route-following path, an in-lane circumventing path and a collision-avoidance path, are extracted to form a Markov decision process (MDP) model. The optimal decision to cope with the current situation, e.g., when and how to perform a passing maneuver with parked vehicles in the host lane, are finally made in the MDP model such that the actual path and target speed profile will be generated. Particularly, three driving modes, i.e., Leisure Mode, Normal Mode and Efficiency Mode, are provided to the driver/passengers to adjust the decision making strategy such that the generated human-like automated driving commands will make them not only feel safe by the collision-free trajectories but also feel at ease by meeting their preferences and needs. Experimental results in various typical but challenging urban traffic scenes have substantiated the effectiveness of the proposed system.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst14_05">16:00-18:00, Paper FrPS-SST14.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0530.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('530'); return false" title="Click to show or hide the keywords and abstract">Model-Based Decision Making with Imagination for Autonomous Parking</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37420" title="Click to go to the Author Index">Feng, Ziyue</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36464" title="Click to go to the Author Index">Chen, Shitao</a></td><td class="r">Xi'an Jiaotong Univ. Xi'an, China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37024" title="Click to go to the Author Index">Chen, Yu</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab530" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Autonomous parking is a key technology in autonomous driving research. In this paper, we propose an imaginative autonomous parking algorithm to solve issues concerned with parking. The proposed algorithm consists of 3 parts: an imaginative model for anticipating possible results before parking, an improved rapid-exploring random tree (RRT) for planning a feasible trajectory from a given start point to the parking lot, and a path smoothing module for optimizing the efficiency of parking tasks. Our algorithm is based on a real kinematic vehicle model, which makes it more suitable for algorithm application on real autonomous cars. What's more, due to the introduction of the imagination mechanism, the processing speed of our algorithm is 10 times faster than that of traditional methods, permitting the realization of a real-time planning at the same time. Then in order to evaluate the algorithm's effectiveness, we have compared our algorithm with traditional RRT in three different parking scenarios. Results show that our algorithm is more stable than traditional RRT and performs better in terms of efficiency and quality.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst15"><b>FrPS-SST15</b></a></td>
               <td class="r">ZhaoWen Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst15" title="Click to go to the Program at a Glance"><b>Simulation and Navigation for Intelligent Vehicles</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst16"><b>FrPS-SST16</b></a></td>
               <td class="r">LongLiQi Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst16" title="Click to go to the Program at a Glance"><b>Robotic and AI Solutions for Smart Mobility</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="frps-sst16_01">16:00-18:00, Paper FrPS-SST16.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0512.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('512'); return false" title="Click to show or hide the keywords and abstract">Dynamic Diffusion Maps-Based Path Planning for Real-Time Collision Avoidance of Mobile Robots (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24996" title="Click to go to the Author Index">Hong, Sanghyun</a></td><td class="r">Ford Motor Company</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20319" title="Click to go to the Author Index">Lu, Jianbo</a></td><td class="r">Ford Motor Company</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19130" title="Click to go to the Author Index">Filev, Dimitar</a></td><td class="r">Ford Res. & Advanced Engineering</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab512" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Given a route to a destination, a mobile robot still needs to locally plan a path to avoid collisions in continuously changing environment, e.g., a hall with pedestrians and moving obstacles. In this paper, diffusion maps are applied to find a local path for reaching a goal and avoiding collisions simultaneously. The proposed path planning algorithm plans a local path by utilizing a receding horizon approach, and therefore the algorithm repeats planning at every sample time. With this approach, mobile robots do not have to carry a prior map all the time because updated environment information is used for planning at every sample time. Extensive simulation is performed in different scenarios and demonstrates a good performance in collision avoidance.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="frps-sst17"><b>FrPS-SST17</b></a></td>
               <td class="r">BoSiDeng Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#frps-sst17" title="Click to go to the Program at a Glance"><b>Computational Intelligence in Vehicle and Transportation Systems</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="fred"><b>FrED</b></a></td>
               <td class="r"></td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#fred" title="Click to go to the Program at a Glance"><b>Exhibition & Demonstration-29June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table>
</div>

<p>&nbsp;<br>&nbsp;</p><p>&nbsp;<br>&nbsp;</p>


</div>

</body>

</html>
