<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
 <head>
  <title>IV 2018</title>
  
  <link href="style.css" rel="stylesheet" type="text/css" media="screen" />

<script language="JavaScript">

function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
</script>


</head>

<body leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">
			   
<div class="c" id="TheTop"><br></div>
<table border="0" cellspacing="0" cellpadding="1" width="85%" nowrap style="margin: auto">
<tr><td>
<br>
<h2>Technical Program for Wednesday June 27, 2018</h2>
<hr>
<br>
</td></tr>
</table>

<div class="c">

                  <span style="color:gray ">To show or hide the keywords and abstract of a paper (if available), click on the paper title</span><br>
                  <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">Open all abstracts</a>&nbsp;&nbsp;
                  <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">Close all abstracts</a>
               
</div>

<div class="c">
<table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="were"><b>WeRE</b></a></td>
               <td class="r">Room T19</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#were" title="Click to go to the Program at a Glance"><b>Registration-27June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weoc"><b>WeOC</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weoc" title="Click to go to the Program at a Glance"><b>Openning</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#17493" title="Click to go to the Author Index">Li, Lingxi</a></td><td class="r">Indiana Univ. Univ. Indianapolis</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="wekn"><b>WeKN</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#wekn" title="Click to go to the Program at a Glance"><b>Keynote-27June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#17493" title="Click to go to the Author Index">Li, Lingxi</a></td><td class="r">Indiana Univ. Univ. Indianapolis</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="web1b"><b>WeB1B</b></a></td>
               <td class="r">Lobby</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#web1b" title="Click to go to the Program at a Glance"><b>Break1-27June</b></a></td>
               <td class="r">Break</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weosas"><b>WeOSAS</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weosas" title="Click to go to the Program at a Glance"><b>Vision Sensing and Perception</b></a></td>
               <td class="r">Plenary Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosas_01">10:20-10:40, Paper WeOSAS.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0274.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('274'); return false" title="Click to show or hide the keywords and abstract">Towards End-To-End Lane Detection: An Instance Segmentation Approach</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35739" title="Click to go to the Author Index">Neven, Davy</a></td><td class="r">KULeuven</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36876" title="Click to go to the Author Index">De Brabandere, Bert</a></td><td class="r">KU Leuven</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36869" title="Click to go to the Author Index">Georgoulis, Stamatios</a></td><td class="r">KU Leuven</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36881" title="Click to go to the Author Index">Proesmans, Marc</a></td><td class="r">KU Leuven</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36878" title="Click to go to the Author Index">Van Gool, Luc S. J.</a></td><td class="r">ETH Zurich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab274" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Modern cars are incorporating an increasing number of driver assist features, among which automatic lane keeping. The latter allows the car to properly position itself within the road lanes, which is also crucial for any subsequent lane departure or trajectory planning decision in fully autonomous cars. Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques, that are computationally expensive and prone to scalability due to road scene variations. More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Despite their advantages, these methods are limited to detecting a pre-defined, fixed number of lanes, eg ego-lanes, and can not cope with lane changes. In this paper, we go beyond the aforementioned limitations and propose to cast the lane detection problem as an instance segmentation problem -- in which each lane forms its own instance -- that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, we further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed &quot;bird's-eye view&quot; transformation. By doing so, we ensure a lane fitting which is robust against road plane changes, unlike existing approaches that rely on a fixed, pre-defined transformation. In summary, we propose a fast lane detection algorithm, running at 50 fps, which can handle a variable number of lanes and cope with lane changes. We verify our method on the tuSimple dataset and achieve competitive results.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosas_02">10:40-11:00, Paper WeOSAS.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0235.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('235'); return false" title="Click to show or hide the keywords and abstract">Box2Pix: Single-Shot Instance Segmentation by Assigning Pixels to Object Boxes</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36605" title="Click to go to the Author Index">Uhrig, Jonas</a></td><td class="r">Daimler AG and Univ. of Freiburg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25412" title="Click to go to the Author Index">Rehder, Eike</a></td><td class="r">Daimler R&D</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27548" title="Click to go to the Author Index">Froehlich, Bjoern</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10226" title="Click to go to the Author Index">Franke, Uwe</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36606" title="Click to go to the Author Index">Brox, Thomas</a></td><td class="r">Univ. of Freiburg</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> The task of semantic instance segmentation has gained a large interest within academia as well as industry, especially in the context of autonomous driving. While several published approaches achieve very strong results, only few of them achieve frame rates that are sufficient for the automotive domain. We present an approach that achieves competitive results on the Cityscapes and KITTI datasets, while being twice as fast as any other existing approach. Our method relies on a single fully-convolutional network (FCN) predicting object bounding boxes, as well as pixel-wise semantic object classes and an offset vector pointing to corresponding object centers. Using those outputs, we present an efficient and simple post-processing that assigns each object pixel to its best matching object detection, resulting in an instance segmentation obtained at real-time speeds.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosas_03">11:00-11:20, Paper WeOSAS.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0607.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('607'); return false" title="Click to show or hide the keywords and abstract">Real-Time Semantic Segmentation-Based Depth Upsampling Using Deep Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31103" title="Click to go to the Author Index">Miclea, Vlad</a></td><td class="r">Tech. Univ. of Cluj-Napoca</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10728" title="Click to go to the Author Index">Nedevschi, Sergiu</a></td><td class="r">Tech. Univ. of Cluj-Napoca</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab607" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> We propose a new real-time depth upsampling method based on convolutional neural networks (CNNs) that uses the local context provided by semantic information. Two solutions based on convolutional networks are introduced, modeled according to the level of sparsity given by the depth sensor. While first CNN upsamples data from a partial-dense input, the second one uses dilated convolutions as means to cope with sparse inputs from cost-effective depth sensors. Experiments over data extracted from Kitti dataset highlight the performance of our methods while running in real-time (11 ms for the first case and 17 ms for the second) on a regular GPU.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosas_04">11:20-11:40, Paper WeOSAS.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0590.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('590'); return false" title="Click to show or hide the keywords and abstract">Probabilistic Prediction of Vehicle Semantic Intention and Motion</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36016" title="Click to go to the Author Index">Hu, Yeping</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33408" title="Click to go to the Author Index">Zhan, Wei</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab590" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> Accurately predicting the possible behaviors of traffic participants is an essential capability for future autonomous vehicles. The majority of current researches fix the number of driving intentions by considering only a specific scenario. However, distinct driving environments usually contain various possible driving maneuvers. Therefore, a intention prediction method that can adapt to different traffic scenarios is needed. To further improve the overall vehicle prediction performance, motion information is usually incorporated with classified intentions. As suggested in some literature, the methods that directly predict possible goal locations can achieve better performance for long-term motion prediction than other approaches due to their automatic incorporation of environment constraints. Moreover, by obtaining the temporal information of the predicted destinations, the optimal trajectories for predicted vehicles as well as the desirable path for ego autonomous vehicle could be easily generated. In this paper, we propose a Semantic-based Intention and Motion Prediction (SIMP) method, which can be adapted to any driving scenarios by using semantic-defined vehicle behaviors. It utilizes a probabilistic framework based on deep neural network to estimate the intentions, final locations, and the corresponding time information for surrounding vehicles. An exemplar real-world scenario was used to implement and examine the proposed method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosas_05">11:40-12:00, Paper WeOSAS.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0391.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('391'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Accurate Localization in Underground Garages Via Cylinder Feature Based Map Matching</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35463" title="Click to go to the Author Index">Tao, Zhongxing</a></td><td class="r">Xi'an Jiaotong Unversity</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28837" title="Click to go to the Author Index">Xue, Jianru</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31702" title="Click to go to the Author Index">Wang, Di</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36796" title="Click to go to the Author Index">Zhang, Shuyang</a></td><td class="r">The School of Electronic and Information Engineering, Xi'an Jiao</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35121" title="Click to go to the Author Index">Cui, Dixiao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36453" title="Click to go to the Author Index">Du, Shaoyi</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab391" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0391.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Autonomous driving in underground garages usually utilizes a 2D/3D occupancy map for localization. However, the real scene is changing, and may not be consistent with the map. Vehicles and other objects not contained in the map are considered as obstacles, which increase the difficulty of localization and affect the accuracy of result. In this paper, we propose a cylinder rotational projection statistics(Cy-RoPS) feature descriptor, which is a local surface feature descriptor to improve the accuracy of localization. The local surface feature motivated by RoPS feature is invariant to rotation of point set enclosed in a cylinder. We also propose to employ the local surface feature for localization in a real underground garage. The experimental results show that the proposed method is robust to dynamic obstacles in the underground garage, and has a higher accuracy in localization, compared with the state-of-the-art methods.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="welb"><b>WeLB</b></a></td>
               <td class="r">Dining Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#welb" title="Click to go to the Program at a Glance"><b>Lunch-27June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weosbs"><b>WeOSBS</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weosbs" title="Click to go to the Program at a Glance"><b>Vehicle Motion Planning and Testing</b></a></td>
               <td class="r">Plenary Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosbs_01">13:30-13:50, Paper WeOSBS.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0643.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('643'); return false" title="Click to show or hide the keywords and abstract">A Graded Offline Evaluation Framework for Intelligent Vehicle's Cognitive Ability</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25523" title="Click to go to the Author Index">Zhang, Chi</a></td><td class="r">Inst. of Artificial Intelligence and Robotics, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25095" title="Click to go to the Author Index">Liu, Yuehu</a></td><td class="r">Inst. of Artificial Intelligence and Robotics, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37355" title="Click to go to the Author Index">Zhang, Qilin</a></td><td class="r">HERE Tech. Chicago, Illinois</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37651" title="Click to go to the Author Index">Wang, Le</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab643" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Cognitive ability evaluation in intelligent vehicles is conventionally evaluated by classical autonomous driving dataset, which lacks comprehensive annotations of driving difficulty. Realistically, different driving conditions require vast different level of cognitive ability, e.g., driving in highly congested traffic is much more challenging than driving on limited access highway; driving in a blizzard/hurricane requires much more robust environmental cognition abilities than driving under ordinary conditions. Different datasets contain different proportions of various driving conditions, rendering intelligent vehicle evaluation susceptible to dataset variations. To overcome such limitations, we propose to first benchmark the driving difficulty with the proposed “Cascaded Tanks Model” and obtain a fine-grained per-segment difficulty rating based on our proposed Semantic Descriptor. With the proposed Graded Offline Evaluation (GOE) framework, it is demonstrated that offline validation of the cognitive abilities in Intelligent Vehicles (IV) is more consistent regardless of dataset choice.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosbs_02">13:50-14:10, Paper WeOSBS.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0059.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('59'); return false" title="Click to show or hide the keywords and abstract">From G2 to G3 Continuity: Continuous Curvature Rate Steering Functions for Sampling-Based Nonholonomic Motion Planning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32414" title="Click to go to the Author Index">Banzhaf, Holger</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16020" title="Click to go to the Author Index">Nienhüser, Dennis</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14985" title="Click to go to the Author Index">Zöllner, J. Marius</a></td><td class="r">FZI Res. Center for Information Tech. KIT Karlsruhe In</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38655" title="Click to go to the Author Index">Berinpanathan, Nijanthan</a></td><td class="r">ETH Zurich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab59" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Motion planning for car-like robots is one of the major challenges in automated driving. It requires to solve a two-point boundary value problem (BVP) in real time while taking into account the nonholonomic constraints of the vehicle and the obstacles in the non-convex environment. This paper introduces Hybrid Curvature Rate (HCR) and Continuous Curvature Rate (CCR) Steer: Two novel steering functions for car-like robots that compute a curvature rate continuous solution of the two-point BVP. Hard constraints on the maximum curvature, maximum curvature rate, and maximum curvature acceleration are satisfied resulting in directly driveable G3 continuous paths. The presented steering functions are benchmarked in terms of computation time and path length against its G1 and G2 continuous counterparts, namely Dubins, Reeds-Shepp, Hybrid Curvature, and Continuous Curvature Steer. It is shown that curvature rate continuity can be achieved with only small computational overhead. The generic motion planner Bidirectional RRT* is finally used to present the effectiveness of HCR and CCR Steer in three challenging automated driving scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosbs_03">14:10-14:30, Paper WeOSBS.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0050.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('50'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Cooperative Lane Change Motion Planning of Connected and Automated Vehicles: A Stepwise Computational Framework</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26452" title="Click to go to the Author Index">Li, Bai</a></td><td class="r">Zhejiang Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36901" title="Click to go to the Author Index">Zhang, Yue</a></td><td class="r">Center for Information and Systems Engineering, Boston Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36902" title="Click to go to the Author Index">Zhang, Youmin</a></td><td class="r">Xi'an Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35956" title="Click to go to the Author Index">Jia, Ning</a></td><td class="r">Tianjin Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab50" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0050.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper focuses on the scheme of cooperative lane change motion planning of multiple connected and automated vehicles, so as to minimize the time for lane change while penalizing large steering angles subject to hard collision avoidance constraints. Nominally this scheme should be formulated in a centralized way with the constraints of all the vehicles considered simultaneously. In order to facilitate the numerical solving process of this centralized optimization problem, we propose a stepwise computation framework. Starting with a sub-problem with all of the collision avoidance constraints removed, a sequence of sub-problems are defined by adding back the removed collision avoidance constraints gradually until the original problem takes shape in the end. The optimum of one sub-problem is always used as the initial guess when solving the next sub-problem. This iterative process continues until the optimum of the original problem is obtained. In this way, the difficulties in the original centralized problem are divided into multiple parts, and every progress made to address the partial difficulties is “solidified” by the initial guess.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosbs_04">14:30-14:50, Paper WeOSBS.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0336.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('336'); return false" title="Click to show or hide the keywords and abstract">A Human-Like Trajectory Planning Method by Learning from Naturalistic Driving Data</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36852" title="Click to go to the Author Index">He, Xu</a></td><td class="r">Peking Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28561" title="Click to go to the Author Index">Xu, Donghao</a></td><td class="r">Peking Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12556" title="Click to go to the Author Index">Zhao, Huijing</a></td><td class="r">Peking Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36871" title="Click to go to the Author Index">Moze, Mathieu</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31507" title="Click to go to the Author Index">Aioun, Francois</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31505" title="Click to go to the Author Index">Guillemard, Franck</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab336" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Trajectory planning has generally been framed as finding the lowest cost one from a set of trajectory candidates, where the cost function has been hand-crafted with carefully tuned parameters by experts. Such methods have technological feasibility of achieving vehicle autonomy, while the resultant behaviors could be much different with those of human drivers. This research proposes a human-like trajectory planning method by learning from naturalistic driving data. A cost function is formulated by incorporating not only the components on comfort, efficiency and safety, but also lane incentive by referring to a human driver's lane change decisions. Coefficients of the cost components are learnt by correlating the probability of a trajectory being selected with its distance (i.e. similarity) to the human driven one at the same driving situation. A data set is developed by using the naturalistic data of human drivers on the motorways in Beijing, containing samples of lane changes to the left and right lanes, and car followings. Experiments are conducted on three aspects: 1) lane change trajectory planning to a given target lane; 2) lane change trajectory planning with simultaneous decision of a target lane; and 3) trajectory planning with simultaneous decision of maneuver. Promising results are presented.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosbs_05">14:50-15:10, Paper WeOSBS.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0441.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('441'); return false" title="Click to show or hide the keywords and abstract">Combining Homotopy Methods and Numerical Optimal Control to Solve Motion Planning Problems</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37139" title="Click to go to the Author Index">Kristoffer, Bergman</a></td><td class="r">Linkoping Univ. Div. of Automatic Control</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30539" title="Click to go to the Author Index">Axehill, Daniel</a></td><td class="r">Linköping Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab441" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper presents a systematic approach for computing local solutions to motion planning problems in non-convex environments using numerical optimal control techniques. It extends the range of use of state-of-the-art numerical optimal control tools to problem classes where these tools have previously not been applicable. Today these problems are typically solved using motion planners based on randomized or graph search. The general principle is to define a homotopy that transforms, or preferably relaxes, the original problem to an easily solved problem. In this work, it is shown that by combining a Sequential Quadratic Programming (SQP) method with a homotopy approach that gradually transforms the problem from a relaxed one to the original one, practically relevant locally optimal solutions to the motion planning problem can be computed. The approach is demonstrated in motion planning problems in challenging 2D and 3D environments, where the presented method significantly outperforms both a state-of-the-art numerical optimal control method and a state-of-the-art open-source optimizing sampling-based planner commonly used as benchmark.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weosbs_06">15:10-15:30, Paper WeOSBS.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0499.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('499'); return false" title="Click to show or hide the keywords and abstract">Online Adaptive Covariance Estimation Approach for Multiple Odometry Sensors Fusion</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37158" title="Click to go to the Author Index">Osman, Mostafa Elsaid Abdelaziz</a></td><td class="r">Autotronics Res. Lab (ARL), Ain Shams Univ. Cairo, Egy</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32226" title="Click to go to the Author Index">Hussein, Ahmed</a></td><td class="r">Intelligent Systems Lab (LSI) - Univ. Carlos III De Madrid</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24517" title="Click to go to the Author Index">Al-Kaff, Abdulla</a></td><td class="r">Univ. Carlos III De Madrid</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18506" title="Click to go to the Author Index">Garcia, Fernando</a></td><td class="r">Univ. Carlos III De Madrid</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15510" title="Click to go to the Author Index">Armingol Moreno, José María</a></td><td class="r">Univ. Carlos III De Madrid</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab499" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Ground__Air_and_Space_Vehicles" title="Click to go to the Keyword Index">Intelligent Ground, Air and Space Vehicles</a></span><br>
                              <strong>Abstract:</strong> Odometry is a crucial task in the design of intelligent vehicles and there are many novel approaches with different sensors in Intelligent Transportation Systems (ITS) field. Accordingly, this leads to the use of multiple methods and sensors for identifying the vehicle pose in the environment, hence the necessity of multiple odometry sensors fusion methods. Quantifying the uncertainties of the sensors used in the vehicle is essential for utilizing effective fusion systems. Since the identification of the true values of the uncertainties is an exhaustive task, this paper introduces an online adaptive covariance estimation approach for drift suffering proprioceptive sensors, using an exteroceptive sensor with known uncertainty. To validate the proposed approach, three scenarios were selected and various experiments were carried out under different conditions. Though the use of multiple odometry sensors fusion algorithm, a comparative study was conducted between the adaptive covariance and several constant covariances based on the true variances. The obtained results show high performance of the proposed approach, in terms of four evaluation metrics for both translation and orientation mean errors.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="web2b"><b>WeB2B</b></a></td>
               <td class="r">Lobby</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#web2b" title="Click to go to the Program at a Glance"><b>Break2-27June</b></a></td>
               <td class="r">Break</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst1"><b>WePS-SST1</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst1" title="Click to go to the Program at a Glance"><b>Eco-Driving and Energy-Efficient Vehicles</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst1_01">16:00-18:00, Paper WePS-SST1.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0395.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('395'); return false" title="Click to show or hide the keywords and abstract">Optimization of Speed Profile and Energy Interaction at Staitons for a Train Vehicle with On-Board Energy Storage Device</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36977" title="Click to go to the Author Index">Wu, Chaoxian</a></td><td class="r">Xi'an Jiaotong-Liverpool Univ. China and Univ. of Liv</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28466" title="Click to go to the Author Index">Lu, Shaofeng</a></td><td class="r">Xi'an Jiaotong-Liverpool Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34066" title="Click to go to the Author Index">Xue, Fei</a></td><td class="r">Xi'an Jiaotong-Liverpool Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35642" title="Click to go to the Author Index">Jiang, Lin</a></td><td class="r">Liverpool Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34067" title="Click to go to the Author Index">Yang, Jie</a></td><td class="r">Beijing Jiaotong Univ. and Jiangxi Univ. of Science An</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab395" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> With the increasing application of railway transportation, energy consumption of railway transportation rises dramatically, which in turn undermines its sustainability. Optimization on train speed profile and use of regenerative energy is becoming a feasible and applicable approach to achieve an energy-efficient operation without changing existing infrastructures. Considering both dwelling at stations and running in the inter-station section, the paper proposes an integrated optimization model for reducing net energy consumption from the viewpoint of energy interaction among train, substation and on-board energy storage device (ESD), based on which the optimal train speed profile is also found. The discharge/charge strategy of on-board ESD is explored and comparative case studies are given, under the assumption of higher efficiency from the on-board ESD, less net energy consumption can be achieved via energy interaction with substation. Through charging from substation the net energy consumption can be reduced, i.e. 0.2%, in the comparison.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst1_02">16:00-18:00, Paper WePS-SST1.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0641.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('641'); return false" title="Click to show or hide the keywords and abstract">GS1 Connected Car: An Integrated Vehicle Information Platform and Its Ecosystem for Connected Car Services Based on GS1 Standards</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37225" title="Click to go to the Author Index">Han, Jiyong</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37282" title="Click to go to the Author Index">Kim, Hyunseob</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37272" title="Click to go to the Author Index">Heo, Sehyeon</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37255" title="Click to go to the Author Index">Lee, Nakyung</a></td><td class="r">KAIST(Korea Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37424" title="Click to go to the Author Index">Kang, Daeyoun</a></td><td class="r">Naver Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40559" title="Click to go to the Author Index">Oh, Byungsoo</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36582" title="Click to go to the Author Index">Kim, KyungTaek</a></td><td class="r">Hyundai Autron</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37256" title="Click to go to the Author Index">Yoon, Wondeuk</a></td><td class="r">Korea Advanced Inst. of Science & Tech. (KAIST)</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37285" title="Click to go to the Author Index">Byun, Jaewook</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37259" title="Click to go to the Author Index">Kim, Daeyoung</a></td><td class="r">KAIST</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab641" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> In recent years, the connected automotive industry has grown explosively. Various connected car services are emerging, such as remote vehicle diagnostics, driver's health monitoring, infotainment, and vehicle safety management. As a result, the number and type of vehicle data are increasing tremendously day by day. However, existing connected automotive solutions have a limitation in that each company manages its own closed data silos. This restricts connected car services from using data sources in various domains. Hence, we propose the GS1 Connected Car, an integrated vehicle information platform, and its ecosystem. We suggest GS1-based automotive data standards for not only in-vehicle data but also all the automotive-related data generated during the lifecycle of vehicles. We provide standardized data collection to EPCIS, the discovery of global automotive services using ONS, IoT Mash-up service between an in-car dashboard platform and IoT devices, video infotainment called GS1 video, and automotive lifecycle management application. We have implemented our platform in a real car by developing an Android-based vehicle dashboard which includes service discovery, Mash-up services, and GS1 Video. Also, a mobile application for lifecycle management and Amazon skills for collecting driver's information are developed. Our demonstration and case study show the feasibility of the proposed platform, widening the scope of future connected car services.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst1_03">16:00-18:00, Paper WePS-SST1.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0653.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('653'); return false" title="Click to show or hide the keywords and abstract">Adaptive Anti-Slip Regulation Method for Electric Vehicle with In-Wheel Motors Considering the Road Slope</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37167" title="Click to go to the Author Index">Li, Bin</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36322" title="Click to go to the Author Index">Lu, Xiong</a></td><td class="r">Tongji Unviersity</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40587" title="Click to go to the Author Index">Leng, Bo</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab653" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> Anti-slip regulation (ASR) is one of the research focus in the field of active safety of electric vehicle. An ASR algorithm adaptive to road condition is proposed in this paper based on 4WD electric vehicle with in-wheel motors. The controller based on anti-windup sliding mode control is robust to wheel parameter uncertainty. The longitudinal velocity estimator based on the fusion of dynamics method and kinematics method is adopted to reduce the velocity estimation error. The road slope is estimated using recursive least square with forgetting factor and the longitudinal acceleration sensor information is calibrated by the road slope estimation for slope adaptive velocity estimation. At the same time, a road coefficient estimator is adopted to estimate road condition using improved Burckhardt model, so the optimal reference slip ratio is selected according to the estimated road adhesion coefficient for the maximum driving efficiency and the realization of adaptive anti-slip regulation. Multi-condition simulations show that the controller is adaptive to road changes, and it can suppress wheel slip ratio and ensure the vehicle stability.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst1_04">16:00-18:00, Paper WePS-SST1.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0330.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('330'); return false" title="Click to show or hide the keywords and abstract">Pollutant Emissions Estimation Framework for Real-Driving Emissions at Microscopic Scale and Environmental Footprint Calculation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36024" title="Click to go to the Author Index">Sabiron, Guillaume</a></td><td class="r">IFP Energies Nouvelles</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31582" title="Click to go to the Author Index">Thibault, Laurent</a></td><td class="r">IFPEN</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33530" title="Click to go to the Author Index">Degeilh, Philippe</a></td><td class="r">IFPEN</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33533" title="Click to go to the Author Index">Corde, Gilles</a></td><td class="r">IFPEN</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab330" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Nowadays, health related issues of local pollutant emissions due to transport is becoming a major concern in our modern society. Exhaust emissions level for various gases are strongly related to the driving behavior of the user. It is of public knowledge that vehicle approval regulation techniques do not reflect actual emissions on everyday trips due, for instance, to the very smooth velocity profile considered. Available means of measuring the actual environmental footprint of a vehicle exist and are called Portable Emissions Measurement Systems (PEMS). However these sensors are usually bulky and too expensive for large scale campaigns. We propose a novel solution to monitor users environmental footprint using only a smartphone device with no additional sensors. The proposed solution is able to provide individualized feedback depending on vehicle characteristics, driving style and trips topology. The comprehensive pollutant gas emission estimation algorithm is presented including vehicle, engine, aftertreatment models and environmental footprint calculation.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst2"><b>WePS-SST2</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst2" title="Click to go to the Program at a Glance"><b>Object Detection and Recognition-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst2_01">16:00-18:00, Paper WePS-SST2.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0439.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('439'); return false" title="Click to show or hide the keywords and abstract">Offline Object Extraction from Dynamic Occupancy Grid Map Sequences</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35075" title="Click to go to the Author Index">Stumper, Daniel</a></td><td class="r">Ulm Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37140" title="Click to go to the Author Index">Gies, Fabian</a></td><td class="r">Ulm Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31451" title="Click to go to the Author Index">Hoermann, Stefan</a></td><td class="r">Univ. of Ulm</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10481" title="Click to go to the Author Index">Dietmayer, Klaus</a></td><td class="r">Univ. of Ulm</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab439" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> A dynamic occupancy grid map (DOGMa) allows a fast, robust, and complete environment representation for automated vehicles. Dynamic objects in a DOGMa, however, are commonly represented as independent cells while modeled objects with shape and pose are favorable. The evaluation of algorithms for object extraction or the training and validation of learning algorithms rely on labeled ground truth data. Manually annotating objects in a DOGMa to obtain ground truth data is a time consuming and expensive process. Additionally the quality of labeled data depend strongly on the variation of filtered input data. The presented work introduces an automatic labeling process, where a full sequence is used to extract the best possible object pose and shape in terms of temporal consistency. A two direction temporal search is executed to trace single objects over a sequence, where the best estimate of its extent and pose is refined in every time step. Furthermore, the presented algorithm only uses statistical constraints of the cell clusters for the object extraction instead of fixed heuristic parameters. Experimental results show a well-performing automatic labeling algorithm with real sensor data even at challenging scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst2_02">16:00-18:00, Paper WePS-SST2.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0047.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('47'); return false" title="Click to show or hide the keywords and abstract">Leveraging Object Proposals for Object-Level Change Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36528" title="Click to go to the Author Index">Sugimoto, Takuma</a></td><td class="r">Univ. of Fukui</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35940" title="Click to go to the Author Index">Tanaka, Kanji</a></td><td class="r">Univ. of Fukui</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36530" title="Click to go to the Author Index">Yamaguchi, Kousuke</a></td><td class="r">Univ. of Fukui</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab47" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Feature-based image differencing is an efficient approach to image change detection, which performs fast enough for self-driving car and robotic applications. Extant approaches typically take local keypoint features as input to the differencing stage. In this study, we aim to extend the differencing stage to consider object-level features. Our object-level approach is inspired by recent advances in two independent object-region proposal techniques: supervised object proposal (e.g., YOLO) and unsupervised object proposal (e.g., BING). A difficulty arises from the fact that even state-of-the-art object proposal techniques suffer from misdetections and false alarms. Our key concept is combining the supervised and unsupervised techniques into a common framework that evaluates the likelihood of change at the semantic object level. We address a challenging urban scenario using the publicly available Malaga dataset and experimentally verify that improved change detection performance can be obtained with our approach.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst2_03">16:00-18:00, Paper WePS-SST2.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0521.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('521'); return false" title="Click to show or hide the keywords and abstract">Learn to Detect Objects Incrementally</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34513" title="Click to go to the Author Index">Guan, Linting</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34510" title="Click to go to the Author Index">Wu, Yan</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34519" title="Click to go to the Author Index">Zhao, Junqiao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36321" title="Click to go to the Author Index">Chen, Ye</a></td><td class="r">Tongji Unviersity</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab521" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Intelligent vehicles need to detect new classes of traffic objects while keeping the performance of old ones. Deep convolution neural network (DCNN) based detector has shown superior performance, however, DCNN is ill-equipped for incremental learning, i.e., a DCNN based vehicle detector trained on traffic sign dataset will catastrophic forget how to detect vehicles. In this paper, we propose a novel method to alleviate this problem, our key insight is that the original class of objects also appears in new task data, by utilizing these objects, our method effectively keeps the detection accuracy of original models while incremental learning to detect new classes of objects. Detailed experiments on PASCAL VOC dataset and TSD-max database verified the effectiveness of our method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst2_04">16:00-18:00, Paper WePS-SST2.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0223.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('223'); return false" title="Click to show or hide the keywords and abstract">Object Modeling from 3D Point Cloud Data for Self-Driving Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36565" title="Click to go to the Author Index">Azam, Shoaib</a></td><td class="r">GIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36566" title="Click to go to the Author Index">Munir, Farzeen</a></td><td class="r">GIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38931" title="Click to go to the Author Index">Rafique, Aasim</a></td><td class="r">Quaid-E-Azam Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38932" title="Click to go to the Author Index">Ko, YeongMin</a></td><td class="r">GIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38923" title="Click to go to the Author Index">Sheri, Ahmad Muqeem</a></td><td class="r">GIST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35749" title="Click to go to the Author Index">Jeon, Moongu</a></td><td class="r">GIST</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab223" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> For autonomous vehicles to be deployed and used practically, many problems are still needed to be solved. One of them we are interested in is to make use of a cheap LIDAR for robust object modeling with 3D point cloud data. Self-driving vehicles require accurate information about the surrounding environments to decide the next course of actions. 3D point cloud data obtained from LIDAR give more accurate distance than the counterpart stereo images. As LIDAR generates low-resolution data, the object detection and modeling is prone to produce errors. In this work, we propose the use of multiple frames of LIDAR data in an urban environment to construct a comprehensive model of the object. We assume the use of LIDAR on a moving platform and the results are almost equal to the 3D CAD model representation of the object.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst2_05">16:00-18:00, Paper WePS-SST2.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0231.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('231'); return false" title="Click to show or hide the keywords and abstract">An Orientation Corrected Bounding Box Fit Based on the Convex Hull under Real Time Constraints</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32064" title="Click to go to the Author Index">Naujoks, Benjamin</a></td><td class="r">Univ. of the Bundeswehr München</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15680" title="Click to go to the Author Index">Wuensche, Hans Joachim Joe</a></td><td class="r">Univ. BW Munich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab231" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> An important requirement for safe autonomous driving is the perception of dynamic and static objects. In urban scenarios, there exist hundreds of potential obstacles. Therefore, it is crucial to have a fast and accurate fitting method which is a key step for many tracking algorithms. In this paper, we demonstrate an orientation corrected bounding box fit based on the convex hull and a line creation heuristic. Our method is capable of fitting hundreds of objects in less than 10 ms and involves only few tuning parameters. Furthermore, orientation estimated through the dynamics of the object can be used to improve the fitting result. Real-world experiments have proven the robustness and effectiveness of our method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst2_06">16:00-18:00, Paper WePS-SST2.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0364.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('364'); return false" title="Click to show or hide the keywords and abstract">A Survey of Anomaly Detection for Connected Vehicle Cybersecurity and Safety</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36886" title="Click to go to the Author Index">Rajbahadur, Gopi Krishnan</a></td><td class="r">Queen's Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36893" title="Click to go to the Author Index">Malton, Andrew James</a></td><td class="r">BlackBerry</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36915" title="Click to go to the Author Index">Walenstein, Andrew</a></td><td class="r">BlackBerry</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36916" title="Click to go to the Author Index">Hassan, Ahmed E.</a></td><td class="r">Queen's Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab364" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> Anomaly detection techniques have been applied to the challenging problem of ensuring both cybersecurity and safety of connected vehicles. We propose a taxonomy of prior research in this domain. Our proposed taxonomy has 3 overarching dimensions subsuming 9 categories and 38 subcategories. Key observations emerging from the survey are: Real-world datasets are seldom used, but instead, most results are derived from simulations; V2V/V2I communications and in-vehicle communication are not considered together; proposed techniques are seldom evaluated against a baseline; safety of the vehicles does not attract as much attention as cybersecurity.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst3"><b>WePS-SST3</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst3" title="Click to go to the Program at a Glance"><b>Object Tracking</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_01">16:00-18:00, Paper WePS-SST3.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0331.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('331'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Lightweight Online Multiple Object Vehicle Tracking Method</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35207" title="Click to go to the Author Index">Gunduz, Gultekin</a></td><td class="r">Galatasaray Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14304" title="Click to go to the Author Index">Acarman, Tankut</a></td><td class="r">Galatasaray Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab331" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0331.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> In this paper, multiple-object vehicle tracking sys- tem by affinity matching using min-cost linear cost assignment is proposed. This tracking system is targeted to scene recordings acquired from cameras mounted on a moving ego vehicle. Vehicle tracking on the road scene and images acquired from moving ego vehicle’s camera amplifies the problem of greater bounding box geometry change in comparison with other low speed tracking applications such as traditional pedestrian tracking. This perturbation occurs in many tracking scenarios such as when a high speed object is approaching from an opposing lane. Since autonomous driving algorithms need to use the processing resources in an efficient manner even while satisfying the requirements of computationally complex tasks like localization, object detection, occupancy grid update, sensor fusion and trajectory planning, our study is particularly focused on the development and benchmarking of an computationally lightweight online multiple object tracking model. To test and evaluate our model, we use KITTI Object Tracking-Car Benchmark dataset and our model statistical metric values are comparably higher; our model outperforms the state-of-the-art methods on ML and MT and places second on MOTA and MOTP metric evaluations, and processing time is 6 to 20 times faster compared to other methods.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_02">16:00-18:00, Paper WePS-SST3.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0580.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('580'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34589" title="Click to go to the Author Index">Scheidegger, Samuel</a></td><td class="r">Zenuity AB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37145" title="Click to go to the Author Index">Benjaminsson, Joachim</a></td><td class="r">Zenuity</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37141" title="Click to go to the Author Index">Rosenberg, Emil</a></td><td class="r">Chalmers Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37189" title="Click to go to the Author Index">Krishnan, Amrit</a></td><td class="r">Zenuity AB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31405" title="Click to go to the Author Index">Granstrom, Karl</a></td><td class="r">Chalmers Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab580" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0580.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> Monocular cameras are one of the most commonly used sensors in the automotive industry for autonomous vehicles. One major drawback using a monocular camera is that it only makes observations in the two dimensional image plane and can not directly measure the distance to objects. In this paper, we aim at filling this gap by developing a multi-object tracking algorithm that takes an image as input and produces trajectories of detected objects in a world coordinate system. We solve this by using a deep neural network trained to detect and estimate the distance to objects from a single input image. The detections from a sequence of images are fed in to a state-of-the art Poisson multi-Bernoulli mixture tracking filter. The combination of the learned detector and the PMBM filter results in an algorithm that achieves 3D tracking using only mono-camera images as input. The performance of the algorithm is evaluated both in 3D world coordinates, and 2D image coordinates, using the publicly available KITTI object tracking dataset. The algorithm shows the ability to accurately track objects, correctly handle data associations, even when there is a big overlap of the objects in the image, and is one of the top performing algorithms on the KITTI object tracking benchmark. Furthermore, the algorithm is efficient, running on average close to 20 frames per second.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_03">16:00-18:00, Paper WePS-SST3.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0078.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('78'); return false" title="Click to show or hide the keywords and abstract">A State Machine-Based Multi-Vehicle Tracking Framework with Dual-Range Radars</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34528" title="Click to go to the Author Index">Huang, Jiawei</a></td><td class="r">Honda Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35189" title="Click to go to the Author Index">Ma, Lichao</a></td><td class="r">Honda Res. Inst. (HRI)</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab78" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Radar_Sensing_and_Perception" title="Click to go to the Keyword Index">Radar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Vehicle tracking is an essential topic in autonomous driving. Currently most systems rely on radars and lidars to perform vehicle tracking. In this paper, we present a novel cross traffic vehicle tracking system with several unique contributions. First of all, it employs a state machine to manage the life cycles of particle filters, resulting in higher tracking robustness. Secondly, the entire software framework is designed to be extensible to support multiple sensors and tracking algorithms. Lastly, we implemented a sensor-vehicle co-simulator to evaluate the tracking performance. We show through experiments that our vehicle tracking system can track multiple vehicles up to 170m away with less than 1m average positional error. We also show that our proposed state machine improves tracking rate under frequent occlusion.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_04">16:00-18:00, Paper WePS-SST3.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0295.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('295'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Real-Time Detection, Tracking, and Classification of Moving and Stationary Objects Using Multiple Fisheye Images</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32023" title="Click to go to the Author Index">Baek, Iljoo</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36401" title="Click to go to the Author Index">Davies, Albert</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36417" title="Click to go to the Author Index">Yan, Geng</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15496" title="Click to go to the Author Index">Rajkumar, R.</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab295" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0295.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> The ability to detect pedestrians and other moving objects is crucial for an autonomous vehicle. This must be done in real-time with minimum system overhead. This paper discusses the implementation of a surround view system to identify moving as well as static objects that are close to the ego vehicle. The algorithm works on 4 views captured by fisheye cameras which are merged into a single frame. The moving object detection and tracking solution uses minimal system overhead to isolate regions of interest (ROIs) containing moving objects. These ROIs are then analyzed using a deep neural network(DNN) to categorize the moving object. With deployment and testing on a real car in urban environments, we have demonstrated the practical feasibility of the solution.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_05">16:00-18:00, Paper WePS-SST3.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0431.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('431'); return false" title="Click to show or hide the keywords and abstract">Millimeter Wave Radar Target Tracking Based on Adaptive Kalman Filter</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36798" title="Click to go to the Author Index">Zhai, Guangyao</a></td><td class="r">Soochow Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37126" title="Click to go to the Author Index">Wu, Cheng</a></td><td class="r">Soochow Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37127" title="Click to go to the Author Index">Wang, Yiming</a></td><td class="r">Soochow Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab431" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Radar_Sensing_and_Perception" title="Click to go to the Keyword Index">Radar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> With the continuous development of the intelligent transportation industry, target tracking has become an important research direction. Under normal circumstances, due to the complex road environment and changing backgrounds, millimeter wave radar has more interference when detecting targets. In addition to the variety of targets in the road and the different scattering intensity of multiple parts, the interference of the flicker noise on the radar must be considered. The combination of these noises can affect the accuracy of radar measurement and even make the radar to lose the target for a short time. The paper constructs a target tracking model based on adaptive Sage-Husa Kalman filter algorithm to track radar signals. The algorithm can not only estimate the real-time state of the system, but also estimate and modify the parameters of the system and the statistical parameters of the noise, so that the system model is closer to the current real state of the system, thus improving the accuracy of the target tracking. Even if radar loses its target in a short time, the target tracking model can estimate the approximate value of the true value of the target. The experimental results show that this method can track the radar target accurately and estimate the position information of the lost target.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_06">16:00-18:00, Paper WePS-SST3.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0462.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('462'); return false" title="Click to show or hide the keywords and abstract">Mono-Camera Based 3D Object Tracking Strategy for Autonomous Vehilces</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37191" title="Click to go to the Author Index">Kuramoto, Akisue</a></td><td class="r">Tokyo Metropolitan Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34775" title="Click to go to the Author Index">Aldibaja, Mohammad</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34776" title="Click to go to the Author Index">Yanase, Ryo</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37201" title="Click to go to the Author Index">Kameyama, Junya</a></td><td class="r">Sony Semiconductor Solutions Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37622" title="Click to go to the Author Index">Kim, TaeHyon</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25257" title="Click to go to the Author Index">Yoneda, Keisuke</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13867" title="Click to go to the Author Index">Suganuma, Naoki</a></td><td class="r">Kanazawa Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab462" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper proposes an approach to calculate 3D positions of far detected vehicles. Mainly, the distance from the vehicles during autonomous driving must be estimated precisely to strategize a safe path planning. A 3D camera model is created to map the pixel positions to the distance values with respect to the vehicle plane and the distortion parameters. In order to refine the distance accuracy, the Extended Kalman Filter (EKF) framework is designed to track the detected vehicles based on the derivative relationship between the camera and world coordinate systems. The experimental results indicate that the proposed method is capable to successfully track 3D positions with sufficient accuracy compared to LIDAR and Radar based tracking systems in terms of cost and stability.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_07">16:00-18:00, Paper WePS-SST3.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0100.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('100'); return false" title="Click to show or hide the keywords and abstract">Multi-Target Track-To-Track Fusion Based on Permutation Matrix Track Association</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28745" title="Click to go to the Author Index">Lee, Kuan-Hui</a></td><td class="r">Toyota Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32182" title="Click to go to the Author Index">Kanzawa, Yusuke</a></td><td class="r">Toyota Motor Engineering & Manufacturing North America, Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36154" title="Click to go to the Author Index">Derry, Matthew</a></td><td class="r">Toyota Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19532" title="Click to go to the Author Index">James, Michael</a></td><td class="r">Toyota Res. Inst</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab100" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> This paper proposes the Permutation Matrix Track Association (PMTA) algorithm to support track-to-track, multi-sensor data fusion for multiple targets in an autonomous driving system. In this system, measurement data from different sensor modalities (LIDAR, radar, and vision) is processed by object trackers operating on each sensor modality independently to create the tracks of the objects. The proposed approach fuses the object track lists from each tracker, first by associating the tracks within each track list, followed by a state estimation (filtering) step. The eventual output is the unified tracks of the objects provided for further autonomous driving processing, such as path and motion planning. The permutation matrix track association (PMTA) algorithm considers both spatial and temporal information to associate object tracks from different sensor modalities. Experimental results show that the proposed approach improves not only the performance of the multiple-target track-to-track fusion, but also stability and robustness in the resulting speed control and decision making in the autonomous driving system.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst3_08">16:00-18:00, Paper WePS-SST3.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0461.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('461'); return false" title="Click to show or hide the keywords and abstract">A Method of Track Starting Point Identification for Tram Based on Integral Algorithm</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37214" title="Click to go to the Author Index">Sun, Yimin</a></td><td class="r">Soochow Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37126" title="Click to go to the Author Index">Wu, Cheng</a></td><td class="r">Soochow Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37127" title="Click to go to the Author Index">Wang, Yiming</a></td><td class="r">Soochow Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab461" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> With the development of intelligent driving technology, the unmanned technology of rail vehicles has also made great progress. How to determine the front limit region in the driving vehicle is a very important topic in the unmanned technology of the rail vehicle.In this paper, a method of Track-Starting-Point(TSP) identification for tramway based on integral algorithm is proposed in this paper. The method is based on the special morphological characteristics of the tramway, and takes the gray mean of column pixels as the processing basis, and uses integral operation to detect the TSP. In order to achieve accurate detection of TSP under various weather conditions, illumination conditions and terrain features, the accurate boundary extraction is finally achieved, providing a guarantee for subsequent tramway obstacle detection.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst4"><b>WePS-SST4</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst4" title="Click to go to the Program at a Glance"><b>Trajectory Planning, Prediction and Optimization-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_01">16:00-18:00, Paper WePS-SST4.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0484.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('484'); return false" title="Click to show or hide the keywords and abstract">Real-Time Trajectory Optimization for Autonomous Vehicle Racing Using Sequential Linearization</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37308" title="Click to go to the Author Index">Alrifaee, Bassam</a></td><td class="r">RWTH Aachen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37310" title="Click to go to the Author Index">Maczijewski, Janis</a></td><td class="r">RWTH Aachen Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab484" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> This paper presents a real-time Model Predictive Controller (MPC) for racing trajectory optimization. The vehicle must respect its dynamic limitations and the track boundaries while simultaneously minimizing lap times. Due to the track boundary constraints, the feasible set of the optimization problem is non-convex. This paper presents the method of sequential linearization to solve this non-convex optimization problem.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_02">16:00-18:00, Paper WePS-SST4.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0072.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('72'); return false" title="Click to show or hide the keywords and abstract">Trajectory Planning with Shadow Trolleys for an Autonomous Vehicle on Bending Roads and Switchbacks</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34644" title="Click to go to the Author Index">Lee, Seungho</a></td><td class="r">Ford Motor Company</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26199" title="Click to go to the Author Index">Tseng, Eric</a></td><td class="r">Ford</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab72" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> In automated driving, the road geometry information such as waypoints is usually available from previously stored maps. In this paper, we present a scenario based Model Predictive Control (MPC) trajectory planning algorithm that consists of spatial planning with embedded temporal optimization that leverages waypoints information of the road. Our trajectory planning algorithm is structured such that the spatial and temporal planning is integrated so that both the longitudinal and lateral aspects, reflected in the shape and length of the planned trajectory, are dynamically changing to best negotiate the constraints from road curvature and surrounding vehicles.<p>The concept of a vehicle connected to shadow trolleys traveling along the rails on the road is introduced. Given waypoints, a reference cubic spline can be constructed to define the rail for trolleys and form a curvilinear coordinate. This concept facilitates the description of vehicle motion, trajectories, and surroundings with respect to the trolley, which is especially convenient for a vehicle traveling on high curvature road and switchbacks. A temporal optimization of the trolley instances is first conducted, which in turn allows us to make a proper approximation and reduce an originally complex nonlinear spatial-temporal optimization problem into one that requires only Quadratic Programming (QP).<p>We present simulation results of various challenging scenarios on complex road geometry with multiple surrounding vehicles of varying behavior to demonstrate the effectiveness and efficiency of the proposed algorithm. Simulation results show reasonable, flexible, and safe maneuvers.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_03">16:00-18:00, Paper WePS-SST4.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0130.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('130'); return false" title="Click to show or hide the keywords and abstract">Towards Risk Minimizing Trajectory Planning in On-Road Scenarios</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31317" title="Click to go to the Author Index">Ward, Erik</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31323" title="Click to go to the Author Index">Folkesson, John</a></td><td class="r">KTH -Royal Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab130" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Trajectory planning for autonomous vehicles should attempt to minimize expected risk given noisy sensor data and uncertain predictions of the near future. In this paper, we present a trajectory planning approach for on-road scenarios where we use a graph search approximation. Uncertain predictions of other vehicles are accounted for by a novel inference technique that allows efficient calculation of the probability of dangerous outcomes for set of modeled situation types.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_04">16:00-18:00, Paper WePS-SST4.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0236.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('236'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Environment Modeling for the Application in Optimization-Based Trajectory Planning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32520" title="Click to go to the Author Index">Lienke, Christian</a></td><td class="r">TU Dortmund Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32521" title="Click to go to the Author Index">Keller, Martin</a></td><td class="r">Tech. Univ. of Dortmund</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31847" title="Click to go to the Author Index">Glander, Karl-Heinz</a></td><td class="r">Zf Trw</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22157" title="Click to go to the Author Index">Bertram, Torsten</a></td><td class="r">Tech. Univ. Dortmund</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab236" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0236.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> The paper at hand proposes an environment model for trajectory planning in structured environments. It is composed of a static and a dynamic environment model. The generated static potential field takes restrictions imposed by the static environment into account. The dynamic environment model is based on the physical interpretation of the required safety distance. By the use of an advanced obstacle trajectory prediction method, the safety distance is calculated in accordance to the predicted situation. As the safety distance affects the dynamic potential field, information provided by the obstacle trajectory prediction is directly considered in the ego vehicle trajectory planning process. On account of the predictive character of the developed environment potential field, simulation experiments demonstrate the feasibility and effectiveness of the proposed method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_05">16:00-18:00, Paper WePS-SST4.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0051.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('51'); return false" title="Click to show or hide the keywords and abstract">Trajectory Optimization for Car-Like Vehicles in Structured and Semi-Structured Environments</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35960" title="Click to go to the Author Index">Nietzschmann, Clemens</a></td><td class="r">RWTH Aachen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34908" title="Click to go to the Author Index">Klaudt, Sebastian</a></td><td class="r">Inst. Für Kraftfahrzeuge, RWTH Aachen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25538" title="Click to go to the Author Index">Klas, Christoph</a></td><td class="r">Inst. Fuer Kraftfahrzeuge RWTH Aachen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36193" title="Click to go to the Author Index">Will, Devid</a></td><td class="r">Inst. Für Kraftfahrzeuge, RWTH Aachen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21389" title="Click to go to the Author Index">Eckstein, Lutz</a></td><td class="r">RWTH Aachen Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab51" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> In this paper we propose a local trajectory planner for front steered car-like vehicles based on a combined direct optimization of the lateral and longitudinal vehicle guidance. The planner is designed for continuously optimizing a local trajectory based on a provided reference path in a structured or semi-structured driving environment. The planner respects constraints of the driving dynamics as well as actuator limitations and avoids static and dynamic obstacles. It is not restricted to a limited set of maneuvers. The implementation of the planner allows an online adaptation of the resulting driving behavior to satisfy different comfort or driving style demands. After Software-in-the-Loop simulations the algorithm was tested in two different real-world driving scenarios in ika's automated vehicle which provides interfaces for full lateral and longitudinal control.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_06">16:00-18:00, Paper WePS-SST4.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0173.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('173'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Trajectory Prediction of Turning Vehicles Based on Intersection Geometry and Observed Velocities</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36409" title="Click to go to the Author Index">Kawasaki, Atsushi</a></td><td class="r">Toshiba Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36275" title="Click to go to the Author Index">Tasaki, Tsuyoshi</a></td><td class="r">Toshiba</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab173" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0173.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> Urban intersections are known to be a hotspot for traffic accidents, and understanding of the dynamic traffic situations at intersections can help to prevent injuries. When an ego-vehicle crosses an intersection, predicting the trajectory of oncoming other vehicles is a requirement for Advanced Driver Assistant Systems. In this paper, we propose a method of trajectory prediction of turning vehicles at urban intersections. Trajectory prediction of turn maneuver vehicles is more difficult than straight-maneuver vehicle because a turning vehicle slows down as it approaches the intersections and speeds up as it leaves the intersections. Furthermore, the variation in velocities depends on factors such as an intersection angle, a corner radius. Our method generates a novel desired velocity model for trajectory prediction that takes into account intersection geometry and observed velocities of other vehicles. Specifically, we assume that the velocity becomes minimum at around the crosswalk, and calculate the velocity model by fitting past sequential velocities and estimated minimum velocity to the cubic function. Our method has the advantage of being able to predict the trajectory at any intersection and from any position. The prediction performance of our method in the real traffic scenarios is better than one of other methods.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_07">16:00-18:00, Paper WePS-SST4.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0350.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('350'); return false" title="Click to show or hide the keywords and abstract">Safe Stop Trajectory Planning for Highly Automated Vehicles: An Optimal Control Problem Formulation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33320" title="Click to go to the Author Index">Svensson, Lars</a></td><td class="r">KTH, Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36259" title="Click to go to the Author Index">Masson, Lola</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36278" title="Click to go to the Author Index">Mohan, Naveen</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31317" title="Click to go to the Author Index">Ward, Erik</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25165" title="Click to go to the Author Index">Feng, Lei</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36346" title="Click to go to the Author Index">Pernestål Brenden, Anna</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35384" title="Click to go to the Author Index">Törngren, Martin</a></td><td class="r">KTH - Dept. of Machine Design</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab350" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Highly automated road vehicles need the capability of stopping safely in a situation that disrupts continued normal operation, e.g. due to internal system faults. Motion planning for safe stop differs from nominal motion planning, since there is not a specific goal location. Rather, the desired behavior is that the vehicle should reach a stopped state, preferably outside of active lanes. Also, the functionality to stop safely needs to be of high integrity. The first contribution of this paper is to formulate the safe stop problem as a benchmark optimal control problem, which can be solved by dynamic programming. However, this solution method cannot be used in real-time. The second contribution is to develop a real-time safe stop trajectory planning algorithm, based on selection from a precomputed set of trajectories. By exploiting the particular properties of the safe stop problem, the cardinality of the set is decreased, making the algorithm computationally efficient. Furthermore, a monitoring based architecture concept is proposed, that ensures dependability of the safe stop function. Finally, a proof of concept simulation using the proposed architecture and the safe stop trajectory planner is presented.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst4_08">16:00-18:00, Paper WePS-SST4.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0561.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('561'); return false" title="Click to show or hide the keywords and abstract"> Decoupled Sampling-Based Velocity Tuning and Motion Planning Method for Multiple Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34291" title="Click to go to the Author Index">Mohseni, Fatemeh</a></td><td class="r">Linkoping Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18217" title="Click to go to the Author Index">Nielsen, Lars</a></td><td class="r">Linköping Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab561" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Assistive_Mobility_Systems" title="Click to go to the Keyword Index">Assistive Mobility Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> This paper describes a decoupled sampling-based motion-planning method, based on the rapidly-exploring random tree (RRT) approach, that is applicable to autonomous vehicles, in order to perform different traffic maneuvers. This is a two-step motion-planning method including path-planning and motion timing steps, where both steps are sampling-based. In the path-planning part, an improved RRT method is defined that increases the smoothness of the path and decreases the computational time of the RRT method; it is called smooth RRT, SRRT. While some other RRT-based methods such as RRT^* can perform better in winding roads, in the problem of interest in this paper (which is performing some regular traffic maneuvers in usual urban roads and highways where the passage is not too winding), SRRT is more efficient since the computational time is less than for the other considered methods. In the motion timing or velocity-tuning step (VTS), a sampling-based method is introduced that guarantees collision avoidance between different vehicles. The proposed motion-timing algorithm can be very useful for collision avoidance and can be used with any other path-planning method. Simulation results show that because of the probabilistic property of the SRRT and VTS algorithms, together with the decoupling feature of the method, the algorithm works well for different traffic maneuvers.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst5"><b>WePS-SST5</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst5" title="Click to go to the Program at a Glance"><b>Trajectory Planning, Prediction and Optimization-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_01">16:00-18:00, Paper WePS-SST5.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0399.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('399'); return false" title="Click to show or hide the keywords and abstract">Decentralized Cooperative Planning for Automated Vehicles with Hierarchical Monte Carlo Tree Search</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35406" title="Click to go to the Author Index">Kurzer, Karl</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35837" title="Click to go to the Author Index">Zhou, Chenyang</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14985" title="Click to go to the Author Index">Zöllner, J. Marius</a></td><td class="r">FZI Res. Center for Information Tech. KIT Karlsruhe In</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab399" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> Today's automated vehicles lack the ability to cooperate implicitly with others. This work presents a Monte Carlo Tree Search (MCTS) based approach for decentralized cooperative planning using macro-actions for automated vehicles in heterogeneous environments. Based on cooperative modeling of other agents and Decoupled-UCT (a variant of MCTS), the algorithm evaluates the state-action-values of each agent in a cooperative and decentralized manner, explicitly modeling the interdependence of actions between traffic participants. Macro-actions allow for temporal extension over multiple time steps and increase the effective search depth requiring fewer iterations to plan over longer horizons. Without predefined policies for macro-actions, the algorithm simultaneously learns policies over and within macro-actions. The proposed method is evaluated under several conflict scenarios, showing that the algorithm can achieve effective cooperative planning with learned macro-actions in heterogeneous environments.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_02">16:00-18:00, Paper WePS-SST5.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0067.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('67'); return false" title="Click to show or hide the keywords and abstract">Road Infrastructure Indicators for Trajectory Prediction</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36042" title="Click to go to the Author Index">Raipuria, Geetank</a></td><td class="r">Delft Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35708" title="Click to go to the Author Index">Gaisser, Floris</a></td><td class="r">Delft Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16184" title="Click to go to the Author Index">Jonker, Pieter</a></td><td class="r">Delft Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab67" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Safe and comfortable path planning in a dynamic urban environment is essential to an autonomous vehicle. This requires the future trajectories of all other road users in the environment of the vehicle. These trajectories are predicted through modeling the motion and behaviour of these road users. In this work we state that for efficient trajectory prediction only motion indicators are not sufficient. Therefore, we propose using a curvilinear coordinate system with curvature as road infrastructure indicators to improve motion modeling and trajectory prediction. With experiments, we show that the curvilinear coordinate system with curvature sufficiently incorporates the road structure. Furthermore, we show that a sequence-to-sequence RNN model is suitable to incorporate road curvature indicators directly into the modeling and prediction.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_03">16:00-18:00, Paper WePS-SST5.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0450.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('450'); return false" title="Click to show or hide the keywords and abstract">An Approach to Vehicle Trajectory Prediction Using Automatically Generated Traffic Maps</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36163" title="Click to go to the Author Index">Quehl, Jannik</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36166" title="Click to go to the Author Index">Hu, Haohao</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32782" title="Click to go to the Author Index">Wirges, Sascha</a></td><td class="r">FZI Forschungszentrum Informatik</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17952" title="Click to go to the Author Index">Lauer, Martin</a></td><td class="r">Karlsruher Inst. Für Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab450" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> Trajectory and intention prediction of traffic participants is an important task in automated driving and crucial for safe interaction with the environment. In this paper, we present a new approach to vehicle trajectory prediction based on automatically generated maps containing statistical information about the behavior of traffic participants in a given area. These maps are generated based on trajectory observations using image processing and map matching techniques. The generated maps contain all typical vehicle movements and probabilities in the considered area. Our prediction approach matches an observed trajectory to a behavior contained in the map and uses this information to generate a prediction. We evaluated our approach on a dataset containing over 14000 trajectories and found that it produces significantly more precise mid-term predictions compared to motion model-based prediction approaches.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_04">16:00-18:00, Paper WePS-SST5.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0366.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('366'); return false" title="Click to show or hide the keywords and abstract">Vehicle Trajectory Prediction with Gaussian Process Regression in Connected Vehicle Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36809" title="Click to go to the Author Index">Afkhami Goli, Sepideh</a></td><td class="r">Univ. of Calgary</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27499" title="Click to go to the Author Index">Far, Behrouz</a></td><td class="r">Univ. of Calgary</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36810" title="Click to go to the Author Index">Fapojuwo, Abraham</a></td><td class="r">Univ. of Calgary</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab366" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> This paper addresses the problem of long term location prediction for collision avoidance in Connected Vehicle (CV) environment where more information about the road and traffic data is available through vehicle-to-vehicle and vehicle-to-infrastructure communications. We use Gaussian Process Regression (GPR) to learn motion patterns from historical trajectory data collected using static sensors on the road. Trained models are then shared among the vehicles through connected vehicle cloud. A vehicle receives information, such as Global Positioning System (GPS) coordinates, about nearby vehicles on the road inter-vehicular communication. The collected data from vehicles together with GPR models received from infrastructure are then used to predict the future trajectories of vehicles in the scene. The contributions of this work are twofold. First, we propose the use of GPR in CV environment as a framework for long term location prediction. Second, we evaluate the effect of pre-analysis of training data via clustering in improving the trajectory pattern learning performance. Experiments using real-world traffic data collected in Los Angeles, California, US show that our proposed method improves prediction accuracy compared to the baseline kinematic models.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_05">16:00-18:00, Paper WePS-SST5.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0485.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('485'); return false" title="Click to show or hide the keywords and abstract">Trajectory Optimization for Autonomous Vehicles on Crossroads with Mobile Obstacles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37318" title="Click to go to the Author Index">Receveur, Jean-Baptiste</a></td><td class="r">Univ. De Bordeaux, IMS - UMR 5218 CNRS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37320" title="Click to go to the Author Index">Victor, Stéphane</a></td><td class="r">Univ. De Bordeaux, IMS - UMR 5218 CNRS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37323" title="Click to go to the Author Index">Melchior, Pierre</a></td><td class="r">Univ. De Bordeaux, IMS - UMR 5218 CNRS</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab485" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> Autonomous vehicles issues have emerged in the past years. Among the classic trajectory planning methods, the potential field method, which is mainly local, is purely reactive to the environment and creates rough trajectories. In this article the creation of a dynamic optimal minimum of the potential is proposed, using multi-criteria trajectory optimization, for unmanned terrestrial vehicles. Obstacles are mobile and with the anticipation of their movement, motion and path of this optimal dynamic minimum are jointly generated. The effects of adding this anticipation in the planning are illustrated, and the optimization is done using a Genetic Algorithm (GA), to improve the Potential Fields (PF) method. In the first two parts of this article, the problem is formulated and the different criteria are described. Then, a GA-PF method is proposed. Finally simulations of the potential field method and its combination with a genetic algorithm are presented.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_06">16:00-18:00, Paper WePS-SST5.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0503.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('503'); return false" title="Click to show or hide the keywords and abstract">Optimal Kinematic-Based Trajectory Planning and Tracking Control of Autonomous Ground Vehicle Using the Variational Approach</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36491" title="Click to go to the Author Index">Majd, Keyvan</a></td><td class="r">Student</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36851" title="Click to go to the Author Index">Razeghi Jahromi, Mohammad</a></td><td class="r">ABB Corp. Res. United States (USCRC)</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27773" title="Click to go to the Author Index">Homaifar, Abdollah</a></td><td class="r">North Carolina a & T State Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab503" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong>  In this paper, a novel kinematic-based optimal trajectory planning formulation for an autonomous vehicle is presented. In this new formulation, the quadratic errors of position, velocity, and acceleration are minimized subject to the rear wheel car-like vehicle nonlinear kinematic model. Minimizing the error of velocity and acceleration in addition to the error of position, allows us to obtain both optimal vehicle trajectory and control law. The Variational approach is used to minimize the cost function. Then, optimal trajectory and control inputs are numerically calculated by solving a set of two-point boundary value (TPBV) nonlinear differential equations. Finally, the proposed method is evaluated in two scenarios of lane changing and multi-curvature road which verify the success of the proposed method in generating an optimal trajectory and control inputs.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst5_07">16:00-18:00, Paper WePS-SST5.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0569.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('569'); return false" title="Click to show or hide the keywords and abstract">Trajectory Planning for Autonomous Vehicles in Time Varying Environments Using Support Vector Machines</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34283" title="Click to go to the Author Index">Morsali, Mahdi</a></td><td class="r">Linköping Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33893" title="Click to go to the Author Index">Åslund, Jan</a></td><td class="r">Linköping Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32085" title="Click to go to the Author Index">Frisk, Erik</a></td><td class="r">Linköping Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab569" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> A novel trajectory planning method is proposed in time varying environments for highway driving scenarios. The main objective is to ensure computational efficiency in the approach, while still ensuring collision avoidance with moving obstacles and respecting vehicle constraints such as comfort criteria and roll-over limits. The trajectory planning problem is separated into finding a collision free corridor in space-time domain using a support vector machine (SVM), which means solving a convex optimization problem. After that a time-monotonic path is found in the collision free corridor by solving a simple search problem that can be solved efficiently. The resulting path in space-time domain corresponds to the resulting planned trajectory of the vehicle. The planner is a deterministic search method associated with a cost function that keeps the trajectory kinematically feasible and close to the maximum separating surface, given by the SVM. A kinematic motion model is used to construct motion primitives in the space-time domain representing the non-holonomic behavior of the vehicle and is used to ensure physical constraints on the states of the vehicle such as acceleration, speed, jerk, steer and steer rate. The speed limits include limitations by law and also rollover speed limits. Two highway maneuvers have been used as test scenarios to illustrate the performance of the proposed algorithm.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst6"><b>WePS-SST6</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst6" title="Click to go to the Program at a Glance"><b>Sensor Fusion and Applications</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_01">16:00-18:00, Paper WePS-SST6.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0007.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('7'); return false" title="Click to show or hide the keywords and abstract">Self-Validation for Automotive Visual Odometry</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31237" title="Click to go to the Author Index">Buczko, Martin</a></td><td class="r">TU Darmstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21270" title="Click to go to the Author Index">Willert, Volker</a></td><td class="r">TU Darmstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25539" title="Click to go to the Author Index">Schwehr, Julian</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25039" title="Click to go to the Author Index">Adamy, Jürgen</a></td><td class="r">TU Darmstadt</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab7" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Automotive visual odometry has become a highly researched topic. The published works have turned cameras into a very precise source of ego motion estimation. However, for automotive application, visual odometry has to be placed in a sensor cluster to simultaneously obtain global localization, maximum availability as well as highest precision. Since any visual odometry remains sensitive to its environment, good motion estimation cannot be guaranteed. Consequently, a selfvalidation scheme is one of the barriers towards application in a sensor fusion system. In order to solve this problem, we first formulate an Ackermann vehicle’s motion as a function of its forward speed and yaw rate. Secondly, we present a data-driven model, which achieves to reconstruct the sideward speed as a function of the yaw rate only. As we show, both models reach the quality of the estimated sideward motion from visual odometry. The therewith achieved redundancy can be used for different tasks. Of course, the estimation of the sideward motion can be excluded from the visual odometry scheme to save computation time. This is of special interest for monocular systems, where yet no absolute scale of a translation motion could be directly calculated. Instead, we propose to maintain the estimation of the sideward motion in the visual odometry and to compare the result to the modeled motion. As we show, the emerging deviation is a very good metric for self-validation of the overall visual odometry estimate. Integrating the resulting method into our visual odometry system, we currently achieve the best frame-to-frame result in the KITTI benchmark.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_02">16:00-18:00, Paper WePS-SST6.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0570.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('570'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Fusion of LiDAR and Camera by Scanning in LiDAR Imagery and Image-Guided Diffusion for Urban Road Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36362" title="Click to go to the Author Index">Zhang, Yigong</a></td><td class="r">Nanjing Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37483" title="Click to go to the Author Index">Gu, Shuo</a></td><td class="r">Nanjing Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37482" title="Click to go to the Author Index">Yang, Jian</a></td><td class="r">Nanjing Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16008" title="Click to go to the Author Index">Alvarez, José M.</a></td><td class="r">NICTA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36361" title="Click to go to the Author Index">Kong, Hui</a></td><td class="r">Nanjing Univ. of Science and Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab570" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0570.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> This paper proposes a new method for road detection based on a 3D LiDAR and a camera. First, the original LiDAR point cloud is re-organized in an ordered way to generate a LiDAR imagery. Then the flat region is extracted from the LiDAR imagery as the candidate road region. Next, a strategy of row- and column- scanning is given in the LiDAR imagery to detect a finer road region from the candidate road area. To fuse the point cloud with image information, we transform the point cloud that corresponds to the above detected road region to the image space according to the calibration parameters between the LiDAR and camera. Then we give two image-guided diffusion schemes to conduct image segmentation of road area, respectively. The proposed method has been tested on the KITTI_Road dataset. Our experiments demonstrate that this training free approach detects the road region fast, accurately and robustly, and compares favorably with the-state of-the-art on the URBAN_ROAD datasets from the KITTI benchmark.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_03">16:00-18:00, Paper WePS-SST6.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0061.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('61'); return false" title="Click to show or hide the keywords and abstract">Sensor Fusion of Intensity and Depth Cues Using the ChiNet for Semantic Segmentation of Road Scenes</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28538" title="Click to go to the Author Index">John, Vijay</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36018" title="Click to go to the Author Index">Meenakshi Karunakaran, Nithilan</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15903" title="Click to go to the Author Index">Mita, Seiichi</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18429" title="Click to go to the Author Index">Tehrani Nik Nejad, Hossein</a></td><td class="r">DENSO Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36019" title="Click to go to the Author Index">Konishi, Masataka</a></td><td class="r">Denso</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29556" title="Click to go to the Author Index">Ishimaru, Kazuhisa</a></td><td class="r">Nippon Soken Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31367" title="Click to go to the Author Index">Xu, Yuquan</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38913" title="Click to go to the Author Index">Oishi, Tomoyuki</a></td><td class="r">DENSO Corp</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab61" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Vision-based environment perception is an important research topic for autonomous driving and advanced driver assistance systems. Vision sensors, such as the monocular camera and stereo camera, are widely used for environment perception. The monocular camera provides the appearance information like intensity, and the stereo camera provides the depth information. The appearance and depth information are complementary, and their effective fusion would result in robust environment perception. Consequently, in this paper, we propose a novel deep learning framework, termed as the ChiNet, for the effective sensor fusion of the appearance and depth information for free space and road object estimation. The ChiNet has two input branches for sensor fusion and two output branches for multilabel segmentation. The ChiNet was identified as the most effective network after experimental validation with semantic segmentation frameworks with varying number of input and output branches. For the input branch variations, we consider semantic segmentation models with single and multiple input branches. The single input branch model combines the intensity and depth information, whereas the multiple input branches model considers separate branches for the intensity and depth information. In case of the output branch variations, we consider models which perform multiclass and multilabel segmentation. The multiclass semantic segmentation model has a single output branch, whereas the multilabel semantic segmentation model contains multiple output branches. A comparative and parametric analysis of the different semantic segmentation models are performed using multiple acquired datasets. Our experimental results show that the ChiNet is better than baseline algorithms.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_04">16:00-18:00, Paper WePS-SST6.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0472.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('472'); return false" title="Click to show or hide the keywords and abstract">Leveraging Spatio-Temporal Evidence and Independent Vision Channel to Improve Multi-Sensor Fusion for Vehicle Environmental Perception</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36967" title="Click to go to the Author Index">Shi, Juwang</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36968" title="Click to go to the Author Index">Wang, Wenxiu</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30202" title="Click to go to the Author Index">Wang, Xiao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30196" title="Click to go to the Author Index">Sun, Hongbin</a></td><td class="r">Xi’an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37289" title="Click to go to the Author Index">Lan, Xuguang</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30143" title="Click to go to the Author Index">Xin, Jingmin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab472" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> For intelligent vehicles, multi-sensor fusion is of great importance to perceive traffic environment with high accuracy and robustness. In this paper, we propose two effective methods, i.e. spatio-temporal evidence generating and independent vision channel, to improve multi-sensor track-level fusion for vehicle environmental perception. The spatio-temporal evidence includes instantaneous evidence, tracking evidence and tracks matching evidence to improve existence fusion. Independent vision channel leverages the specific advantage of vision processing on object recognition to improve classification fusion. The proposed methods are evaluated by using the multi-sensor dataset collected from real traffic environment. Experimental results demonstrate that the proposed methods can significantly improve the multi-sensor track-level fusion in terms of both detection accuracy and classification accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_05">16:00-18:00, Paper WePS-SST6.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0084.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('84'); return false" title="Click to show or hide the keywords and abstract">A General Reliability-Aware Fusion Concept Using DST and Supervised Learning with Its Applications in Multi-Source Road Estimation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32406" title="Click to go to the Author Index">Nguyen, Tran Tuan</a></td><td class="r">Volkswagen Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20943" title="Click to go to the Author Index">Spehr, Jens</a></td><td class="r">Volkswagen Group Res</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38611" title="Click to go to the Author Index">Vock, Dominik</a></td><td class="r">Volkswagen AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31408" title="Click to go to the Author Index">Baum, Marcus</a></td><td class="r">Univ. of Göttingen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36098" title="Click to go to the Author Index">Zug, Sebastian</a></td><td class="r">Otto-Von-Guericke Univ. Magdeburg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36102" title="Click to go to the Author Index">Kruse, Rudolf</a></td><td class="r">Otto-Von-Guericke Univ. Magdeburg</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab84" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Concerning the variety of challenging situations of automated driving, a simple average fusion of all available information sources is not appropriate to obtain satisfying results. Hence, this paper presents a novel framework for the road estimation task by incorporating reliability into the multi-source fusion. First, we specify the common JDL fusion model for this task and extend it at multiple levels. Secondly, we integrate an offline-trained knowledge base for the reliability assessment represented by Bayesian Network or Random Forests. Thirdly, we propose a reliability-aware fusion of various sources at the decision level by applying Dempster-Shafer theory. As a result, our system can solve conflict situations among the sources more satisfyingly. Compared to the average fusion, experiments on real world data verify that our concept can increase the overall performance of automated driving up to 8 percentage points.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_06">16:00-18:00, Paper WePS-SST6.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0193.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('193'); return false" title="Click to show or hide the keywords and abstract">Infrastructure Based Calibration of a Multi-Camera and Multi-LiDAR System Using Apriltags</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36462" title="Click to go to the Author Index">Xie, Yuanfan</a></td><td class="r">Baidu</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36556" title="Click to go to the Author Index">Shao, Rui</a></td><td class="r">Baidu Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39247" title="Click to go to the Author Index">Gui, Popo</a></td><td class="r">Baidu.com Times Tech. (Beijing) Co., Ltd</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25247" title="Click to go to the Author Index">Li, Bo</a></td><td class="r">TrunkTech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39264" title="Click to go to the Author Index">Wang, Liang</a></td><td class="r">Baidu USA</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab193" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper proposes a framework for the calibration of a multi-camera and multi-LiDAR system. It utilizes the Apriltags to build a calibration environment to solve the poses and extrinsics of cameras and then deploy ICP-like algorithm to solve the extrinsics of LiDARs. Compared to previous extrinsic calibration methods, this proposed framework naturally applies to systems with different numbers and configurations of cameras and LiDARs. The calibration procedure produces extrinsics not only accurately and robustly, but efficiently without inconvenient human manipulation. The framework provides general, efficient and standardizable calibration solution for autonomous driving platforms with cameras and LiDARs.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst6_07">16:00-18:00, Paper WePS-SST6.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0603.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('603'); return false" title="Click to show or hide the keywords and abstract">Analysis of Real World Sensor Behavior for Rising Fidelity of Physically Based Lidar Sensor Models</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36642" title="Click to go to the Author Index">Rosenberger, Philipp</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34587" title="Click to go to the Author Index">Holder, Martin Friedrich</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13528" title="Click to go to the Author Index">Winner, Hermann</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37545" title="Click to go to the Author Index">Zirulnik, Marina</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab603" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> Safety validation tests of automated driving (AD) use simulated environments and perception sensor models. To achieve the level of fidelity needed for safety approval, such sensor simulations can be physically based. For formulating requirements for sensor models to be used in such test frameworks, the extent to which they must include physical effects should be determined. One approach is to clarify their relevance for following processing steps like object detection or mapping. But at first, an analysis is needed to determine, which effects are relevant and if they can be implemented at all. In this work, we focus on one lidar sensor and analyze its observable real world sensor behavior to derive the possible effects, physically based lidar sensor models can include. Consequently, we describe environmental parameters that could be considered to influence physically based lidar sensor models. By investigating the specifications given by the manufacturer with own measurements, we show that some of them should be implemented in a dynamic manner. In conclusion, we enable to formulate detailed requirements for sensor models, as their actual possible fidelity is presented.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst7"><b>WePS-SST7</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst7" title="Click to go to the Program at a Glance"><b>Mapping and Localization-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_01">16:00-18:00, Paper WePS-SST7.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0312.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('312'); return false" title="Click to show or hide the keywords and abstract">End-To-End Steering Controller with CNN-Based Closed-Loop Feedback for Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35968" title="Click to go to the Author Index">Jhung, Junekyo</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24878" title="Click to go to the Author Index">Bae, Il</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26393" title="Click to go to the Author Index">Moon, Jaeyoung</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36803" title="Click to go to the Author Index">Kim, Taewoo</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36808" title="Click to go to the Author Index">Kim, Jincheol</a></td><td class="r">SK Telecom</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24881" title="Click to go to the Author Index">Kim, Shiho</a></td><td class="r">Yonsei Unversity</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab312" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Many significant research achievements in the past few decades have demonstrated that convolutional neural networks (CNNs) could be capable of steering wheel control which is the basic and essential maneuver of autonomous vehicles. We propose an end-to-end steering controller with CNN-based closed-loop feedback for autonomous vehicles that improves driving performance compared to traditional CNN-based approaches. This paper demonstrates that the proposed neural network, DAVE-2SKY, is able to learn to inference steering wheel angles for the lateral control of self-driving vehicles through initial supervised pre-training and subsequent reinforced closed-loop post-training with images from a camera mounted on the vehicle. We used the PreScan simulator and Caffe deep learning framework for training under diversified circumstances in a software in the loop (SIL) simulation environment. We used DRIVE™ PX2 computer to implement a self-driving car for an experimental validation of the proposed end-to-end controller. The performance of the proposed system has been investigated under simulations and on-road tests as well. This work shows that the CNN-based end-to-end controller performs robust steering control even under partially observable road conditions, which indicates the possibility of fully self-driving vehicles controlled by CNN-based end-to-end steering controllers.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_02">16:00-18:00, Paper WePS-SST7.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0189.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('189'); return false" title="Click to show or hide the keywords and abstract">Improved Localization Using Visual Features and Maps for Autonomous Cars</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36439" title="Click to go to the Author Index">Kasturi Rangan, Sathya Narayanan</a></td><td class="r">NIO</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30727" title="Click to go to the Author Index">Yalla, Veeraganesh</a></td><td class="r">NIO</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36437" title="Click to go to the Author Index">Bacchet, Davide</a></td><td class="r">NIO</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39163" title="Click to go to the Author Index">Domi, Isabella</a></td><td class="r">NIO</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab189" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> In this paper we present our research work on self-localization of the vehicle (ego-state) aided by visual features and maps. This approach requires only last received GPS location, Odometry estimates as the vehicle moves, a database of visual features around the last received GPS estimate and Segment Definition (SD) Map. Towards this goal, we extract Oriented and Rotated Brief (ORB) Descriptors from the Images collected during a drive and use Bag of Words (BoW) approach for creating a vocabulary of visual words. We also use Inverted File Index method for fast querying of the image descriptor that is seen currently by the ego vehicle against a database of all the descriptors collected for finding the possible locations of the robot. The locations for the corresponding matches are then used to update the measurement model. This approach has helped us to localize within 3 seconds with average position and orientation error of 0.8m and 0.38 degree for all the sequences.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_03">16:00-18:00, Paper WePS-SST7.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0611.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('611'); return false" title="Click to show or hide the keywords and abstract">An Experimental Study on Relative and Absolute Pose Graph Fusion for Vehicle Localization</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37552" title="Click to go to the Author Index">Das, Anweshan</a></td><td class="r">Eindhoven Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16104" title="Click to go to the Author Index">Dubbelman, Gijs</a></td><td class="r">Eindhoven Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab611" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> In this work, we research and evaluate multiple pose-graph fusion strategies for vehicle localization. We focus on fusing a single absolute localization system, i.e. automotive-grade Global Navigation Satellite System (GNSS) at 1 Hertz, with a single relative localization system, i.e. vehicle odometry at 25 Hertz. Our evaluation is based on 180 Km long vehicle trajectories that are recorded in highway, urban and rural areas, and that are accompanied with post-processed Real Time Kinematic GNSS as ground truth. The results exhibit a significant reduction in the error's standard deviation by 18% but the bias in the error is unchanged, when compared to non-fused GNSS. We show that the underlying principle is the fact that errors in GNSS readings are highly correlated in time. This causes a bias that cannot be compensated for by using the relative localization information from the odometry, but it can reduce the standard deviation of the error.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_04">16:00-18:00, Paper WePS-SST7.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0287.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('287'); return false" title="Click to show or hide the keywords and abstract">Vision-Based Semantic Mapping and Localization for Autonomous Indoor Parking</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36337" title="Click to go to the Author Index">Huang, Yewei</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34519" title="Click to go to the Author Index">Zhao, Junqiao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36219" title="Click to go to the Author Index">He, Xudong</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36326" title="Click to go to the Author Index">Zhang, Shaoming</a></td><td class="r">Tongji</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36320" title="Click to go to the Author Index">Feng, Tiantian</a></td><td class="r">Tongji</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab287" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> In this paper, we proposed a novel and practical solution for the real-time indoor localization of autonomous driving in parking lots. High-level landmarks, the parking slots, are extracted and enriched with labels to avoid the aliasing of low-level visual features. We then proposed a robust method for detecting incorrect data associations between parking slots and further extended the optimization framework by dynamically eliminating suboptimal data associations. Visual fiducial markers are introduced to improve the overall precision. As a result, a semantic map of the parking lot can be established fully automatically and robustly. We experimented the performance of real-time localization based on the map using our autonomous driving platform TiEV, and the average accuracy of 0.3m track tracing can be achieved at a speed of 10kph.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_05">16:00-18:00, Paper WePS-SST7.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0089.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('89'); return false" title="Click to show or hide the keywords and abstract">Evaluating Location Compliance Approaches for Automated Road Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36129" title="Click to go to the Author Index">Zhu, Alexander</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13878" title="Click to go to the Author Index">Althoff, Matthias</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34578" title="Click to go to the Author Index">Manzinger, Stefanie</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab89" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> This work presents techniques for efficiently checking location compliance of automated road vehicles. We refer to location compliance as an allowed translational and rotational positioning of a vehicle on a road network, i.e., the vehicle does not enter forbidden lanes or regions reserved for other traffic participants, such as bike lanes or dedicated bus routes. Previous work has mostly focused on efficient collision detection between traffic participants and static obstacles represented as bounded sets. We formulate location compliance as a set enclosure problem, which cannot be solved directly with collision detection; thus, different algorithms from computational geometry have to be applied. We present polygon enclosure and boundary mesh generation approaches and evaluate them using existing road geometries from the CommonRoad database. For a fair comparison, we generate thousands of random instances which are evaluated statistically.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_06">16:00-18:00, Paper WePS-SST7.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0106.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('106'); return false" title="Click to show or hide the keywords and abstract">CPFG-SLAM: A Robust Simultaneous Localization and Mapping Based on LIDAR in Off-Road Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36086" title="Click to go to the Author Index">Ji, Kaijin</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15898" title="Click to go to the Author Index">Chen, Huiyan</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30236" title="Click to go to the Author Index">Di, Huijun</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15879" title="Click to go to the Author Index">Gong, Jianwei</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13755" title="Click to go to the Author Index">Xiong, Guangming</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16050" title="Click to go to the Author Index">Qi, Jianyong</a></td><td class="r">Intelligent Vehicle Res. Center, School of Mechanical and Ve</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38702" title="Click to go to the Author Index">Tao, Yi</a></td><td class="r">Beijing Special Vehicle Acad</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab106" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Simultaneous localization and mapping (SLAM), as an important tool for vehicle positioning and mapping, plays an important role in the unmanned vehicle technology. This paper mainly presents a new solution to the lidar-based SLAM for unmanned vehicles in the off-road environment. Many methods have been proposed to solve the slam problems well. However, in complex environment, especially off-road environment, it is difficult to obtain stable positioning results due to the rough road and scene diversity. We propose a SLAM algorithm based on grid which combining probability and feature by Expectation-maximization (EM). The algorithm is mainly divided into three steps: data preprocessing, pose estimation, updating feature grid map. Our algorithm has strong robustness and real-time performance. We have tested our algorithm with our datasets of the multiple off-road scenes which obtained by LIDAR. Our algorithm performs pose estimation and feature map updating in parallel, which guarantees the real-time performance of the algorithm. The average processing time of each frame is about 55ms, and the average relative translation error is around 0.94%. Compared with several state-of-the-art algorithms, our algorithm has better performance in robustness and location accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_07">16:00-18:00, Paper WePS-SST7.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0213.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('213'); return false" title="Click to show or hide the keywords and abstract">Global Vehicle Localization by Sequence Analysis Using LiDAR Features Derived by an Autoencoder</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27492" title="Click to go to the Author Index">Schlichting, Alexander</a></td><td class="r">Leibniz Univ. Hannover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36543" title="Click to go to the Author Index">Feuerhake, Udo</a></td><td class="r">Leibniz Univ. Hannover</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab213" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Global vehicle localization is normally done by GNSS sensors. In case of GNSS outages, such as in urban canyons or tunnels, highly automated cars cannot localize without an initial known position. In this paper we propose a method for absolute localization in a city based on only one 2D laser scanner.<p>In out method, localization is done by matching vertical scan lines captured by a 2D laser scanner mounted on the vehicle with scan lines derived from a reference point cloud of the environment. We use a neural network to derive significant features describing the shape of the scan lines. Every scan line of a reference data set is labeled with a specific cluster-id using a k-means algorithm and stored in a reference graph. The same k-means algorithm is used to label the single scan lines of a test drive. The localization is done via a sequence mining approach, where a sequence with a specific length is matched to the position with the highest correlation in the reference sequence.<p>In our experiments we analyze the effect of several parameters, including the number of features and sequence length. The results show that the algorithm performs with an accuracy of about 1.4 m and a completeness of up to 99%. Even if the input scan is represented by only ten features, the results are betten than those obtained by using the whole range scan in the localization step.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst7_08">16:00-18:00, Paper WePS-SST7.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0288.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('288'); return false" title="Click to show or hide the keywords and abstract">Real-Time 6D Lidar SLAM in Large Scale Natural Terrains for UGV</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36737" title="Click to go to the Author Index">Liu, Zhongze</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15898" title="Click to go to the Author Index">Chen, Huiyan</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30236" title="Click to go to the Author Index">Di, Huijun</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38702" title="Click to go to the Author Index">Tao, Yi</a></td><td class="r">Beijing Special Vehicle Acad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15879" title="Click to go to the Author Index">Gong, Jianwei</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13755" title="Click to go to the Author Index">Xiong, Guangming</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16050" title="Click to go to the Author Index">Qi, Jianyong</a></td><td class="r">Intelligent Vehicle Res. Center, School of Mechanical and Ve</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab288" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Simultaneous Localization And Mapping (SLAM) plays a more and more important role in the environment perception system of Unmanned Ground Vehicle (UGV), most SLAM technologies used to be applied indoor or in urban scenarios, we present a real-time 6D SLAM approach suitable for large scale natural terrain with the help of an Inertial Measurement Unit(IMU) and two 3D Lidars. Besides dividing the entire map into many submaps which consists of large numbers of tree structure based voxels, we use probabilistic methods to represent the possibility of one voxel being occupied/null. A Sparse Pose Adjustment (SPA) method has been used to solve 6D global pose optimization with some relative poses as pose constraints and relative motions computed from IMU data as kinetics constraints. A place recognition method integrated a method named Rotation Histogram Matching (RHM) and a Branch and Bound Search (BBS) based Iterative Closest Points (ICP) algorithm is applied to realize a real-time loop closure detection. We complete global pose optimization with the help of Ceres. Experimental results obtained from a real large scale natural environment shows an effective reduction for Lidar odometry pose accumulative error and a good performance for 3D mapping.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst8"><b>WePS-SST8</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst8" title="Click to go to the Program at a Glance"><b>Mapping and Localization-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_01">16:00-18:00, Paper WePS-SST8.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0174.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('174'); return false" title="Click to show or hide the keywords and abstract">Evidential Occupancy Grid Map Augmentation Using Deep Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32782" title="Click to go to the Author Index">Wirges, Sascha</a></td><td class="r">FZI Forschungszentrum Informatik</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36258" title="Click to go to the Author Index">Hartenbach, Felix</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10216" title="Click to go to the Author Index">Stiller, Christoph</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab174" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> A detailed environment representation is a crucial component of automated vehicles. Using single range sensor scans, data is often too sparse and subject to occlusions. Therefore, we present a method to augment occupancy grid maps from single views to be similar to evidential occupancy maps acquired from different views using Deep Learning. To accomplish this, we estimate motion between subsequent range sensor measurements and create an evidential 3D voxel map in an extensive post-processing step. Within this voxel map, we explicitly model uncertainty using evidence theory and create a 2D projection using combination rules. As input for our neural networks, we use a multi-layer grid map consisting of the three features detections, transmissions and intensity, each for ground and non-ground measurements. Finally, we perform a quantitative and qualitative evaluation which shows that different network architectures accurately infer evidential measures in real-time.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_02">16:00-18:00, Paper WePS-SST8.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0310.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('310'); return false" title="Click to show or hide the keywords and abstract">A Compact Map Representation for Large-Scale Environments and Localization Method Based on Similarity Measure</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36500" title="Click to go to the Author Index">Matsuzaki, Kohei</a></td><td class="r">KDDI Res. Inc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36517" title="Click to go to the Author Index">Yanagihara, Hiromasa</a></td><td class="r">KDDI Res. Inc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab310" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Targeted at autonomous driving, compact map representation for localization is needed to deal with limited disk space or communication bandwidth. State-of-the-art compact map representation distinguishes partial-objects such as lane markings from whole-object data. However, this requires that objects are correctly detected in both map generation and localization. Therefore, the localization may fail if either of these conditions is violated due to occlusion, poor road texture, or other complications. In this paper, we propose a novel map generation and localization method that uses whole-object data without object detection. The novel map so produced has compactness comparable to state-of-the-art methods involving object detection because it is described as blocks of quantized representation in a voxel grid. In the localization step, we formulate localization as a similarity search between the map data and differently sampled sensor data. The proposed localization method alleviates voxel discretization error. This enables accurate localization even with low-resolution voxels. Experiments show the proposed method achieves sufficient localization accuracy and real-time processing with a map having a data size of 32 kB per km. The storage efficiency is at least 161 times greater than state-of-the-art maps covering the same area.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_03">16:00-18:00, Paper WePS-SST8.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0063.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('63'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Map Management for Efficient Long-Term Visual Localization in Outdoor Environments</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31442" title="Click to go to the Author Index">Bürki, Mathias</a></td><td class="r">ETH Zuerivch</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36003" title="Click to go to the Author Index">Dymczyk, Marcin Tomasz</a></td><td class="r">ETH Zurich, Autonomous Systems Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32930" title="Click to go to the Author Index">Gilitschenski, Igor</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35978" title="Click to go to the Author Index">Cadena, Cesar</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11672" title="Click to go to the Author Index">Siegwart, Roland</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19516" title="Click to go to the Author Index">Nieto, Juan Ignacio</a></td><td class="r">ETH Zurich</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab63" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0063.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> We present a complete map management process for a visual localization system designed for multi-vehicle long-term operations in resource constrained outdoor environments. Outdoor visual localization generates large amounts of data that need to be incorporated into a lifelong visual map in order to allow localization at all times and under all appearance conditions. Processing these large quantities of data is non-trivial, as it is subject to limited computational and storage capabilities both on the vehicle and on the mapping backend. We address this problem with a two-fold map update paradigm capable of, either, adding new visual cues to the map, or updating co-observation statistics. The former, in combination with offline map summarization techniques, allows enhancing the appearance coverage of the lifelong map while keeping the map size limited. On the other hand, the latter is able to significantly boost the appearance-based landmark selection for efficient online localization without incurring any additional computational or storage burden. Our evaluation in challenging outdoor conditions shows that our proposed map management process allows building and maintaining maps for precise visual localization over long time spans in a tractable and scalable fashion.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_04">16:00-18:00, Paper WePS-SST8.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0125.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('125'); return false" title="Click to show or hide the keywords and abstract">Traffic Mapping for Autonomous Cars</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36133" title="Click to go to the Author Index">Steinke, Nicolai</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31307" title="Click to go to the Author Index">Ulbrich, Fritz</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31305" title="Click to go to the Author Index">Goehring, Daniel</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16677" title="Click to go to the Author Index">Rojas, Raúl</a></td><td class="r">Berlin Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab125" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> Today's car traffic is dominated by human drivers. Autonomous cars must comprehend the behavior of human drivers in order to fit in current daily traffic scenarios. To achieve this goal, analysis of the behavior of other traffic participants is necessary. In this paper we present a system to record, store, and analyze the movements of other traffic participants with an autonomous car and evaluate traffic maps, which are obtained from real world experiments. The evaluation shows that the maps cover more than 80% of the driveable area with a precision of 80 to 90%. Additionally, we present the results of a traffic behavior change detection heuristic, which can detect anomalous traffic conditions.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_05">16:00-18:00, Paper WePS-SST8.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0327.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('327'); return false" title="Click to show or hide the keywords and abstract">Real-Time Omnidirectional Visual SLAM with Semi-Dense Mapping</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36397" title="Click to go to the Author Index">Wang, Senbo</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30603" title="Click to go to the Author Index">Yue, Jiguang</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17467" title="Click to go to the Author Index">Dong, Yanchao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36399" title="Click to go to the Author Index">Shen, Runjie</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36398" title="Click to go to the Author Index">Zhang, Xinyu</a></td><td class="r">Datang Guoxin Binhai Offshore Wind Power Co., Ltd</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab327" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> The state of art Visual SLAM is going from sparse feature to semi-dense feature to provide more information for environment perception, whereas the semi-dense methods often suffer from inaccurate depth map estimation and are easy to become instable for some real-world scenarios. The paper proposes to extend the ORB-SLAM2 framework, which is a robust sparse feature SLAM system tracking camera motion with map maintenance and loop closure, by introducing the unified spherical camera model and the semi-dense depth map. The unified spherical camera model fits the omnidirectional camera well, therefore the proposed Visual SLAM system could handle fisheye cameras which are commonly installed on modern vehicles to provide larger perceiving region. In addition to the sparse corners features the proposed system also utilizes high gradient regions as semi-dense features, thereby providing rich environment information. The paper presents in detail how the unified spherical camera model and the semi-dense feature matching are fused with the original SLAM system. Both accuracies of camera tracking and estimated depth map of the proposed SLAM system are evaluated using real-world data and CG rendered data where the ground truth of the depth map is available.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_06">16:00-18:00, Paper WePS-SST8.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0096.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('96'); return false" title="Click to show or hide the keywords and abstract">Extending Occupancy Grid Mapping for Dynamic Environments</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36169" title="Click to go to the Author Index">Wessner, Joseph</a></td><td class="r">Zukunft Mobility GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34244" title="Click to go to the Author Index">Utschick, Wolfgang</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab96" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> In this paper, the commonly used filtering technique <i>occupancy grid mapping</i> for static environments is extended for dynamic environments. The proposed method is able to estimate velocities indirectly. We apply a distribution model of the respective state variable to estimate the cell dynamics by means of prediction and update cycle, as known by standard tracking filters. Therefore, we present a straight forward derivation of the prediction and update rule. Furthermore, we validate our approach by simple one dimensional simulations, and show how it can be extended into a two dimensional world, including the resulting consequences, e.g. in terms of memory requirements.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst8_07">16:00-18:00, Paper WePS-SST8.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0655.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('655'); return false" title="Click to show or hide the keywords and abstract">Evaluation of Digital Map Ability for Vehicle Self-Localization</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33455" title="Click to go to the Author Index">Javanmardi, Ehsan</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33447" title="Click to go to the Author Index">Javanmardi, Mahdi</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18592" title="Click to go to the Author Index">Gu, Yanlei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10598" title="Click to go to the Author Index">Kamijo, Shunsuke</a></td><td class="r">The Univ. of Tokyo</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab655" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Vehicle self-localization based on the matching of Light detection and ranging (LiDAR) scans to the normal distribution (ND) map become more popular in recent years due to the price down and miniaturization of the LiDARs. In such methods, source of self-localization error can be divided into input scan quality, matching algorithm and map. In this work, we focus on the map, as one of the high potential sources of error. By investigating the erroneous scenarios in the map and comparing their characteristics, we come up with some criteria and requirements for the map to be able to perform self-localization with a needed error. In this work, we propose four factors for quantified evaluation of the map requirements. These factors are feature count factor, layout factor, normal entropy factor, and local similarity factor of the map. We evaluated these four factors in a different part of the map with different scenarios by comparing them with the self-localization error. Experimental results show that the local similarity factor with 0.59 of correlation with the maximum error has the highest contribution to the localization error. For normal entropy factor, feature count factor, layout factor, correlations are 0.42, 0.36, and 0.34 respectively. By applying these four factors, maximum localization error can be modeled with RMSE and R-squared (R^2) of 0.44 and 0.598 respectively. Result of this study can be applied to the dynamic determination of the abstraction ratio of the map and sensor fusion as well.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst9"><b>WePS-SST9</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst9" title="Click to go to the Program at a Glance"><b>Mapping and Localization-3</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_01">16:00-18:00, Paper WePS-SST9.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0387.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('387'); return false" title="Click to show or hide the keywords and abstract">Siamese-ResNet: Implementing Loop Closure Detection Based on Siamese Network</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36735" title="Click to go to the Author Index">Qiu, Kai</a></td><td class="r">UCAS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13802" title="Click to go to the Author Index">Ai, Yunfeng</a></td><td class="r">Univ. of Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22302" title="Click to go to the Author Index">Tian, Bin</a></td><td class="r">Chinese Acad. of Sciences Inst. of Automation</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36964" title="Click to go to the Author Index">Wang, Bin</a></td><td class="r">Univ. of Science and Tech. of China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24691" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Lancaster Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab387" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Deep learning has made significant breakthroughs in the tasks of image classification, detection, segmentation, etc. However, the application of deep learning in robotics is still scarce. SLAM is a fundamental problem in robotics and loop closure detection is an important part of SLAM. This paper attempts to use supervised learning methods to solve the loop closure detection problem in vision SLAM. We proposed Siamese-ResNet network, which combines Siamese network with ResNet to detect loop closure. To show the effectiveness of Siamese-ResNet, we evaluate Siamese-ResNet and FabMap2.0 on several open published datasets, like TUM SLAM dataset and FabMap SLAM dataset. Compared with FabMap2.0, Siamese-ResNet shows higher accuracy, better robustness and shorter time-consuming.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_02">16:00-18:00, Paper WePS-SST9.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0158.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('158'); return false" title="Click to show or hide the keywords and abstract">A Cooperative Vehicle Ego-Localization Application Using V2V Communications with CBL Clustering</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34675" title="Click to go to the Author Index">Rivoirard, Lucas</a></td><td class="r">Ifsttar, Cosys, Leost</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18172" title="Click to go to the Author Index">Wahl, Martine</a></td><td class="r">Univ. Lille Nord De France, IFSTTAR, LEOST</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34699" title="Click to go to the Author Index">Sondi Obwang, Patrick</a></td><td class="r">Univ. DU LITTORAL COTE D'OPALE</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11746" title="Click to go to the Author Index">Gruyer, Dominique</a></td><td class="r">IFSTTAR</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29409" title="Click to go to the Author Index">Berbineau, Berbineau</a></td><td class="r">IFSTTAR</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab158" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> The cooperative vehicle paradigm offers new opportunities for enhancing various vehicle functions through distributed applications. The localization function is one of the most important since its services are needed by numerous applications ranging from driver navigation to autonomous vehicle guiding. Though the GPS (Global Positioning System) provides honorable service for driver navigation, its precision is not accurate for allowing an autonomous vehicle to localize itself correctly on the road lanes. Recently an ego-localization technique where a group of vehicles exchange their position and the related correction through V2V communications has been proposed in order to enhance the precision of the location of each node in the group. In this paper, we evaluate the performance that this application can expect from V2V communication services supplied by the CBL (Chain-branch-leaf) clustering scheme. The simulation results show that CBL achieves delays and packet delivery ratio adapted to various rates of the ego-localization application traffic.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_03">16:00-18:00, Paper WePS-SST9.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0392.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('392'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>LocNet: Global Localization in 3D Point Clouds for Mobile Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36945" title="Click to go to the Author Index">Yin, Huan</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36969" title="Click to go to the Author Index">Tang, Li</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36979" title="Click to go to the Author Index">Ding, Xiaqing</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36975" title="Click to go to the Author Index">Wang, Yue</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36972" title="Click to go to the Author Index">Xiong, Rong</a></td><td class="r">Zhejiang Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab392" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0392.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> Global localization in 3D point clouds is a challenging problem of estimating the pose of vehicles without any prior knowledge. In this paper, a solution to this problem is presented by achieving place recognition and metric pose estimation in the global prior map. Specifically, we present a semi-handcrafted representation learning method for LiDAR point clouds using siamese LocNets, which states the place recognition problem to a similarity modeling problem. With the final learned representations by LocNet, a global localization framework with range-only observations is proposed. To demonstrate the performance and effectiveness of our global localization system, KITTI dataset is employed for comparison with other algorithms, and also on our long-time multi-session datasets for evaluation. The result shows that our system can achieve high accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_04">16:00-18:00, Paper WePS-SST9.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0421.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('421'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Precise Point Set Registration Using Point-To-Plane Distance and Correntropy for Lidar Based Localization</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36779" title="Click to go to the Author Index">Xu, Guanglin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36453" title="Click to go to the Author Index">Du, Shaoyi</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35121" title="Click to go to the Author Index">Cui, Dixiao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36781" title="Click to go to the Author Index">Zhang, Sirui</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37368" title="Click to go to the Author Index">Chen, Badong</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17872" title="Click to go to the Author Index">Zhang, Xuetao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28837" title="Click to go to the Author Index">Xue, Jianru</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36783" title="Click to go to the Author Index">Gao, Yue</a></td><td class="r">Tsinghua Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab421" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0421.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> In this paper, we propose a robust point set registration algorithm which combines correntropy and point-to-plane distance, which can register rigid point sets with noises and outliers. Firstly, as correntropy performs well in handling data with non-gaussian noises, we introduce it to model rigid point set registration problem based on point-to-plane distance; Secondly, we propose an iterative algorithm to solve this problem, which repeats to compute correspondence and transformation parameters respectively in closed form solutions. Simulated experimental results demonstrate the high precision and robustness of the proposed algorithm. In addition, Lidar based localization experiments on automated vehicle performs satisfactory for localization accuracy and time consumption.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_05">16:00-18:00, Paper WePS-SST9.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0355.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('355'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Reliability Estimation of Vehicle Localization Result</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34496" title="Click to go to the Author Index">Akai, Naoki</a></td><td class="r">Nagoya Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33104" title="Click to go to the Author Index">Morales Saiki, Luis Yoichi</a></td><td class="r">Nagoya Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13895" title="Click to go to the Author Index">Murase, Hiroshi</a></td><td class="r">Nagoya Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab355" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0355.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper proposes a method for estimation of the reliability of vehicle localization results. We previously proposed a fault detection method for indoor mobile robots using a convolutional neural network (CNN). Because image data is generally fed to a CNN, we feed image data obtained from the robot pose, occupancy grid map, and laser scan data to the CNN, which decides of whether localization has failed. The previous method also employed a Rao-Blackwellized particle filter to estimate the robot pose and reliability of this estimation simultaneously. However, it was difficult for vehicle robots to use the previous method as creating and processing image data is not a light computation process. In this study, we extend the previous method by improving the data fed to the CNN, thus making it possible for vehicle robots to perform simultaneous localization and estimation. This paper describes in detail the simultaneous estimation and shows that the reliability can be used as an exact criterion for detecting localization failures.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_06">16:00-18:00, Paper WePS-SST9.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0317.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('317'); return false" title="Click to show or hide the keywords and abstract">Evaluation of Methods to Estimate Vehicle Location in Electronic Toll Collection Service with C-ITS</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36745" title="Click to go to the Author Index">Randriamasy, Malalatiana</a></td><td class="r">Normandie Univ. UNIROUEN, ESIGELEC, IRSEEM & Sanef, France</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30716" title="Click to go to the Author Index">Cabani, Adnane</a></td><td class="r">ESIGELEC-IRSEEM</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36832" title="Click to go to the Author Index">Chafouk, Houcine</a></td><td class="r">ESIGELEC</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21217" title="Click to go to the Author Index">Fremont, Guy</a></td><td class="r">Sanef</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab317" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Smart_Infrastructure" title="Click to go to the Keyword Index">Smart Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> This paper aims to present our method to evaluate a vehicle location to perform Electronic Toll Collection transactions (ETC System) with Cooperative Intelligent Transport System (C-ITS) and it compares different model systems to evaluate the most reliable filter to have the vehicle location. We consider ITS components using the technology ITS-G5 with features specified by the standardization Institute ETSI: RoadSide Unit (RSU) and On-Board Unit (OBU). The idea of the proposed algorithm takes advantage of the communications data. The method is inspired by the principle of Differential - GPS combined with Kalman Filtering or extended Kalman filtering applied to the GPS and CAN bus measurements performed on the RSU. The evaluation of the filter will be done according to the motion models chosen.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst9_07">16:00-18:00, Paper WePS-SST9.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0014.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('14'); return false" title="Click to show or hide the keywords and abstract">A Statistical GPS Error Model for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35889" title="Click to go to the Author Index">Karlsson, Erik</a></td><td class="r">ÅF Tech. AB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34534" title="Click to go to the Author Index">Mohammadiha, Nasser</a></td><td class="r">Zenuity</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab14" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Autonomous driving (AD) is envisioned to have a significant impact on people’s life regarding safety and comfort. Positioning is one of the key challenges in realizing AD, where global navigation systems (GNSS) is traditionally used as an important source of information. The area of GNSS are well explored and the different sources of error are deeply investigated. However the existing modeling methods often have very comprehensive requirements for the training data where all affecting conditions, such as ephemeris data and satellite clock should be well known. The main goal of this paper is to develop a solution to model GPS error that only requires information which is available in the vehicle without having access to detailed information about the conditions. For this purpose, we propose a state-based semi-stochastic model and an efficient learning algorithm, where stochastic parts are modelled using autoregression and Gaussian mixture models. The resulting model successfully mimics the distributions of the absolute error and the first difference, for data with ideal GNSS conditions.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst10"><b>WePS-SST10</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst10" title="Click to go to the Program at a Glance"><b>LiDAR Sensing and Navigation</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_01">16:00-18:00, Paper WePS-SST10.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0322.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('322'); return false" title="Click to show or hide the keywords and abstract">A Benchmark for Lidar Sensors in Fog: Is Detection Breaking Down?</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36070" title="Click to go to the Author Index">Bijelic, Mario</a></td><td class="r">RD Daimler Ulm and Univ. Ulm</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36069" title="Click to go to the Author Index">Gruber, Tobias</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14030" title="Click to go to the Author Index">Ritter, Werner</a></td><td class="r">Daimler AG</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab322" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Autonomous driving at level five means not only self-driving at sunshine. Especially adverse weather is criticallybecause fog, rain and snow hinder the perception of the environment. In this work, current state of the art light detection and ranging (lidar) sensors are tested in controlled conditions in a fog chamber. We present current problems and disturbance patterns of four different state of the art lidar systems. Moreover, we investigate how tuning internal parameters can improve their performance in bad weather situations. This is of great importance as most state of the art detection algorithms are based on undisturbed lidar data.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_02">16:00-18:00, Paper WePS-SST10.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0636.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('636'); return false" title="Click to show or hide the keywords and abstract">LIDAR Based Altitude Estimation for Autonomous Vehicles Using Elevation Maps</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37595" title="Click to go to the Author Index">Yanase, Ryo</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34775" title="Click to go to the Author Index">Aldibaja, Mohammad</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37191" title="Click to go to the Author Index">Kuramoto, Akisue</a></td><td class="r">Tokyo Metropolitan Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37622" title="Click to go to the Author Index">Kim, TaeHyon</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25257" title="Click to go to the Author Index">Yoneda, Keisuke</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13867" title="Click to go to the Author Index">Suganuma, Naoki</a></td><td class="r">Kanazawa Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab636" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper presents a strategy to estimate the vehicle altitude using predefined elevation maps. The map images in multilayers roads must be distinguished and separated with respect to the driving segments during the autonomous driving. This can be achieved by continuously estimating the altitude value and retiring the map images accordingly. The main influence is to increase the lateral and longitudinal localization based on matching the LIDAR and retrieving map images. Accordingly, a framework is designed and evaluated in complicated environments of common driving areas between bridges, tunnels and ground roads. The experimental results have verified that the proposed strategy is very reliable and provides precise altitude estimation with averaged accuracy of 10cm.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_03">16:00-18:00, Paper WePS-SST10.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0576.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('576'); return false" title="Click to show or hide the keywords and abstract">DLO: Direct LiDAR Odometry for 2.5D Outdoor Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36214" title="Click to go to the Author Index">Sun, Lu</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34519" title="Click to go to the Author Index">Zhao, Junqiao</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36219" title="Click to go to the Author Index">He, Xudong</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36321" title="Click to go to the Author Index">Ye, Chen</a></td><td class="r">Tongji Unviersity</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab576" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> For autonomous vehicles, high-precision real-time localization is the guarantee of stable driving. Compared with the visual odometry (VO), the LiDAR odometry (LO) has the advantages of higher accuracy and better stability. However, 2D LO is only suitable for the indoor environment, and 3D LO has less efficiency in general. Both are not suitable for the online localization of an autonomous vehicle in an outdoor driving environment. In this paper, a direct LO method based on the 2.5D grid map is proposed. The fast semi-dense direct method proposed for VO is employed to register two 2.5D maps. Experiments show that this method is superior to both the 3D-NDT and LOAM in the outdoor environment.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_04">16:00-18:00, Paper WePS-SST10.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0344.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('344'); return false" title="Click to show or hide the keywords and abstract">Radar-Based Feature Design and Multiclass Classification for Road User Recognition</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36762" title="Click to go to the Author Index">Scheiner, Nicolas</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15976" title="Click to go to the Author Index">Appenrodt, Nils</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16271" title="Click to go to the Author Index">Dickmann, Jürgen</a></td><td class="r">Mercedes-Benz AG</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28429" title="Click to go to the Author Index">Sick, Bernhard</a></td><td class="r">Univ. of Kassel</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab344" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Radar_Sensing_and_Perception" title="Click to go to the Keyword Index">Radar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> The classification of individual traffic participants is a complex task, especially for challenging scenarios with multiple road users or under bad weather conditions. Radar sensors provide an - with respect to well established camera systems - orthogonal way of measuring such scenes. In order to gain accurate classification results, 50 different features are extracted from the measurement data and tested on their performance. From these features a suitable subset is chosen and passed to random forest and long short-term memory (LSTM) classifiers to obtain class predictions for the radar input. Moreover, it is shown why data imbalance is an inherent problem in automotive radar classification when the dataset is not sufficiently large. To overcome this issue, classifier binarization is used among other techniques in order to better account for underrepresented classes. A new method to couple the resulting probabilities is proposed and compared to others with great success. Final results show substantial improvements when compared to ordinary multiclass classification.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_05">16:00-18:00, Paper WePS-SST10.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0405.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('405'); return false" title="Click to show or hide the keywords and abstract">Radar-Based Analysis of Pedestrian Micro-Doppler Signatures Using Motion Capture Sensors</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36063" title="Click to go to the Author Index">Held, Patrick</a></td><td class="r">Tech. Hochschule Ingolstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36120" title="Click to go to the Author Index">Steinhauser, Dagmar</a></td><td class="r">Tech. Hochschule Ingolstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32825" title="Click to go to the Author Index">Kamann, Alexander</a></td><td class="r">Tech. Hochschule Ingolstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36096" title="Click to go to the Author Index">Holdgrün, Thomas</a></td><td class="r">MESSRING Active Safety GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31826" title="Click to go to the Author Index">Doric, Igor</a></td><td class="r">MESSRING Active Safety GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37013" title="Click to go to the Author Index">Koch, Andreas</a></td><td class="r">Continental, Business Unit ADAS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20558" title="Click to go to the Author Index">Brandmeier, Thomas</a></td><td class="r">Ingolstadt Univ. of Applied Sciences</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab405" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Radar_Sensing_and_Perception" title="Click to go to the Keyword Index">Radar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> For the realization of autonomous driving, early detection and classification of vulnerable road users such as pedestrians is indispensable. High-resolution radars in the short range enable the detection of pedestrian-identifying micro-Doppler distributions. Especially the limb motion can provide fundamental features, which can support the design of automotive safety systems. This paper presents a detailed analysis of high-resolution human micro-Doppler signatures in terms of their composition by characeristic limb movements using synchronized measurements of a motion capture system. The use of state-of-the-art radar sensor technology as well as signal processing including clustering and OS-CFAR filtering leads to comparisons of simultaneously captured movements with the associated micro-Doppler signatures and provides new insights in the detailed radar-based analysis of the human gait. The potential feasibility to detect characteristic features for predictive algorithms is discussed and analyzed on the basis of realistic automotive scenarios such as road crossing movements.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_06">16:00-18:00, Paper WePS-SST10.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0442.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('442'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Autonomous Car Navigation Using Vector Fields</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34973" title="Click to go to the Author Index">Boroujeni, Zahra</a></td><td class="r">Freie Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37143" title="Click to go to the Author Index">Mohammadi, Mostafa</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34484" title="Click to go to the Author Index">Neumann, Daniel</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31305" title="Click to go to the Author Index">Goehring, Daniel</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16677" title="Click to go to the Author Index">Rojas, Raúl</a></td><td class="r">Berlin Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab442" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0442.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> In this paper, a method based on vector fields for the navigation of autonomous cars is developed. Vector fields—used to generate the desired heading angle of a vehicle toward a specified road lane—attract the car to the desired path and prevent the car from colliding with obstacles. Also, a control law is developed to define the velocity direction and the desired steering angle based on the angle between the car and the vector field. The efficacy of the proposed approach is investigated through several simulations and lab experimental tests.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_07">16:00-18:00, Paper WePS-SST10.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0069.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('69'); return false" title="Click to show or hide the keywords and abstract">Learning Urban Navigation Via Value Iteration Network</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34707" title="Click to go to the Author Index">Yang, Shu</a></td><td class="r">Beijing Univ. of Posts and Telecommunications</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29277" title="Click to go to the Author Index">Li, Jinglin</a></td><td class="r">Beijing Univ. of Posts and Telecommunications</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36066" title="Click to go to the Author Index">Wang, Jie</a></td><td class="r">Beijing Univ. of Posts and Telecommunications</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29276" title="Click to go to the Author Index">Liu, Zhihan</a></td><td class="r">Beijing Univ. of Post and Telecommunications</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36068" title="Click to go to the Author Index">Yang, Fangchun</a></td><td class="r">Beijing Univ. of Posts and Telecommunications</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab69" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Assistive_Mobility_Systems" title="Click to go to the Keyword Index">Assistive Mobility Systems</a></span><br>
                              <strong>Abstract:</strong>  Choosing an appropriate route is a critical problem in urban navigation. Being familiar with roads topology and other vehicles' routes, experienced drivers could usually find a near optimal route. However, the nowadays navigation applications hardly catch the domain knowledge and drivers' interaction in this scenario. In fact, they only recommend several routes and leave the most difficult decision to driver. Hence the route is often congested by many vehicles whose drivers make a similar choose. To intelligently make right decision on navigation and improve traffic efficiency for each vehicle, we propose a neural network structure which learns to plan coarse-grained route in complex urban areas.<p> Focused on learning to navigate, this paper first formalizes urban map and vehicle route model. The city map is segmented into grids and each vehicle's route is mapped to grids. Based on grid-world model, we solicit both global traffic status and driving actions from large-scale taxicab GPS data. The learn-to-plan problem is therefore to find a policy function from a global status representation to an experienced driver's action under that status. Traditional neural network is difficult to learn to this plan-involved function, so we leverage and modify value iteration network (VIN), which explicitly takes long-term plan into consideration. Finally we evaluate the performance of proposed network on real map and trajectory data in Beijing, China. The results show that VIN can achieve human driver performance in most cases, with high success rate and less commuting time.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_08">16:00-18:00, Paper WePS-SST10.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0343.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('343'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Autonomous Urban Localization and Navigation with Limited Information</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36645" title="Click to go to the Author Index">Chipka, Jordan</a></td><td class="r">Cornell Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21396" title="Click to go to the Author Index">Campbell, Mark</a></td><td class="r">Cornell U</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab343" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0343.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> Urban environments offer a challenging scenario for autonomous driving. Globally localizing information, such as a GPS signal, can be unreliable due to signal shadowing and multipath errors. Detailed a priori maps of the environment with sufficient information for autonomous navigation typically require driving the area multiple times to collect large amounts of data, substantial post-processing on that data to obtain the map, and then maintaining updates on the map as the environment changes. This paper addresses the issue of autonomous driving in an urban environment by investigating algorithms and an architecture to enable fully functional autonomous driving with limited information. An algorithm to autonomously navigate urban roadways with little to no reliance on an a priori map or GPS is developed. Localization is performed with an extended Kalman filter with odometry, compass, and sparse landmark measurement updates. Navigation is accomplished by a compass-based navigation control law. Key results from Monte Carlo studies show success rates of urban navigation under different environmental conditions. Experiments validate the simulated results and demonstrate that, for given test conditions, an expected range can be found for a given success rate.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst10_09">16:00-18:00, Paper WePS-SST10.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0144.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('144'); return false" title="Click to show or hide the keywords and abstract">Navigating Automated Vehicle through Expressway Toll Gate</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36323" title="Click to go to the Author Index">Mao, Xuesong</a></td><td class="r">Wuhan Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31367" title="Click to go to the Author Index">Xu, Yuquan</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15903" title="Click to go to the Author Index">Mita, Seiichi</a></td><td class="r">Toyota Tech. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36284" title="Click to go to the Author Index">Chin, Hakusho</a></td><td class="r">DENSO Corp</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18429" title="Click to go to the Author Index">Tehrani Nik Nejad, Hossein</a></td><td class="r">DENSO Corp</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab144" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Path planning for automated driving vehicle in wide, unstructured toll gate is a difficult task while several surrounding vehicles are driving with almost random behavior. To deal with this problem, we propose a decision tree based method to control host vehicle speed & acceleration along a predefined path. Our proposed method is able to reach the destination without collision and with minimum changes in acceleration or deceleration. To validate the performance, we simulated the toll gate region in Nagoya Expressway, where driving area is wide with no lane markers. Simulation results show that host vehicle is able to avoid potential collisions through acceleration or deceleration, which is determined by the proposed decision tree. Speed profiles show that host vehicle is driving in a comfortable range of acceleration/deceleration.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst11"><b>WePS-SST11</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst11" title="Click to go to the Program at a Glance"><b>Object Detection and Recognition-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_01">16:00-18:00, Paper WePS-SST11.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0216.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('216'); return false" title="Click to show or hide the keywords and abstract">Object Detection Using a Single Extended Feature Map</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15759" title="Click to go to the Author Index">Lim, Young-Chul</a></td><td class="r">Deagu Gyeongbuk Inst. of S&T</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34781" title="Click to go to the Author Index">Kang, MinSung</a></td><td class="r">Daegu Gyeongbuk Inst. of Science & Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab216" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Fully convolutional neural network-based object detectors have achieved considerable detection accuracy in recent years. It is a recent trend to establish complex and deep network architectures for improvement of the detection accuracy. However, object detectors for intelligent vehicle applications require fast inference speed, lightweight network architecture, and less memory usage as well as high detection accuracy to implement the algorithm in an embedded hardware. In this paper, we propose a fast object detection method based on a single stage and a single extended feature map. A lightweight network based on an extended paththrough layer is proposed to improve both the accuracy and speed. The extended paththrough layer enlarges the resolution of the last feature map by concatenating later feature maps with lower resolution to earlier feature map maps with higher resolution. The layer helps to search and detect smaller objects more densely on the extended last feature map. Our experimental results show that the proposed detection model outperforms the previous state-of-the-art methods in both detection accuracy and inference speed.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_02">16:00-18:00, Paper WePS-SST11.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0234.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('234'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Object Detection on Dynamic Occupancy Grid Maps Using Deep Learning and Automatic Label Generation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31451" title="Click to go to the Author Index">Hoermann, Stefan</a></td><td class="r">Univ. of Ulm</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36600" title="Click to go to the Author Index">Henzler, Philipp</a></td><td class="r">Ulm Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30905" title="Click to go to the Author Index">Bach, Martin</a></td><td class="r">Ulm Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10481" title="Click to go to the Author Index">Dietmayer, Klaus</a></td><td class="r">Univ. of Ulm</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab234" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0234.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> We tackle the problem of object detection and pose estimation in a shared space downtown environment. For perception multiple laser scanners with 360° coverage were fused in a dynamic occupancy grid map (DOGMa). A single-stage deep convolutional neural network is trained to provide object hypotheses comprising shape, position, orientation and an existence score from a single input DOGMa. Furthermore, an algorithm for offline object extraction was developed to automatically label several hours of training data. The algorithm is based on a two-pass trajectory extraction, forward and backward in time. Typical for engineered algorithms, the automatic label generation suffers from misdetections, which makes hard negative mining impractical. Therefore, we propose a loss function counteracting the high imbalance between mostly static background and extremely rare dynamic grid cells. Experiments indicate, that the trained network has good generalization capabilities since it detects objects occasionally lost by the label algorithm. Evaluation reaches an average precision (AP) of 75.9%.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_03">16:00-18:00, Paper WePS-SST11.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0299.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('299'); return false" title="Click to show or hide the keywords and abstract">Fusing Bird’s Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36778" title="Click to go to the Author Index">Wang, Zining</a></td><td class="r">Univ. of California Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33408" title="Click to go to the Author Index">Zhan, Wei</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab299" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird’s eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird’s eye view object detection dataset, which produces 3D bounding boxes from the bird’s eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_04">16:00-18:00, Paper WePS-SST11.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0465.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('465'); return false" title="Click to show or hide the keywords and abstract">Simultaneous Object Detection and Association in Connected Vehicle Platform</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34400" title="Click to go to the Author Index">Guo, Rui</a></td><td class="r">Toyota InfoTechnology Center USA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37223" title="Click to go to the Author Index">Keshavamurthy, Shalini</a></td><td class="r">Toyota InfoTechnology Center USA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30729" title="Click to go to the Author Index">Oguchi, Kentaro</a></td><td class="r">Toyota ITC</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab465" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> The connectivity in vehicular network extends the sensing capability in both the range and quality of the sensing data. One of the most significant benefits is the availability of sensing data from connected vehicles. Leveraging this data in useful ways is an attractive research topic in the community. In this paper, a novel one-pass deep neural network is proposed to implement object detection and the association simultaneously. Considering the bandwidth limitation in the typical vehicular network communication, the proposed algorithm not only highly compresses the feature representation but also maintains the high quality in the detection and association performance. The learning architecture is delicately designed to enhance the task by incorporating multi-modality features. Each modular unit in the system can be appropriately deployed in the vehicle onboard electrical control unit (ECU) and on remote servers to realize a pratical implementation in many applications.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_05">16:00-18:00, Paper WePS-SST11.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0415.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('415'); return false" title="Click to show or hide the keywords and abstract">Efficient Rectangle Fitting of Sparse Laser Data for Robust On-Road Object Detection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37046" title="Click to go to the Author Index">Yang, Shuai</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37061" title="Click to go to the Author Index">Xiang, Zhaohong</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37068" title="Click to go to the Author Index">Wu, Jin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30202" title="Click to go to the Author Index">Wang, Xiao</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30196" title="Click to go to the Author Index">Sun, Hongbin</a></td><td class="r">Xi’an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30143" title="Click to go to the Author Index">Xin, Jingmin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab415" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> On-road object detection is one of the most important tasks for the autonomous driving of intelligent vehicle. Nevertheless, the previous methods based on 2D LIDAR sensor only focus on the detection of vehicles, and show severe limitations on the detection of other objects. Accordingly, this paper proposes an on-road object detection method, which employs rectangle fitting and concavity determination to improve the robustness of object detection. The proposed approaches are extensively evaluated by using the sparse laser data collected by 2D LIDAR from real traffic environment. Experimental results demonstrate that the proposed rectangle fitting outperforms the previous approaches in terms of both detection accuracy and computational efficiency.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_06">16:00-18:00, Paper WePS-SST11.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0289.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('289'); return false" title="Click to show or hide the keywords and abstract">A Fusion of a Monocular Camera and Vehicle-To-Vehicle Communication for Vehicle Tracking: An Experimental Study</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36747" title="Click to go to the Author Index">Tekeli, Mustafa</a></td><td class="r">Galatasaray Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26493" title="Click to go to the Author Index">Yaman, Cagdas</a></td><td class="r">Galatasaray Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14304" title="Click to go to the Author Index">Acarman, Tankut</a></td><td class="r">Galatasaray Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40813" title="Click to go to the Author Index">Akin, Murat</a></td><td class="r">Galatasaray Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab289" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> In this paper we present the procedure of fusing a monocular camera based vehicle tracking and IEEE 802.11p Vehicle-to-Vehicle communication enabled position, velocity and time sharing. Toward a monocular camera-based detection and tracking, Haar-like features of a vehicle are trained, median flow tracking algorithm is applied, pixel based relative distance and relative speed is estimated. In order to improve reliability and availability of tracking system, IEEE 802.11p radio modem is added. Then, we implement Particle Filter algorithm in order to fuse the information provided by these two sensors subject to different characteristics. We evaluate the tracking system by the real road data collected on highway. Sensor fusion results along different road scenarios are presented. We present the state-of-the-art low cost sensor fusion, our application setup and elaborate some experimental results.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst11_07">16:00-18:00, Paper WePS-SST11.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0233.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('233'); return false" title="Click to show or hide the keywords and abstract">Object Recognition from Very Few Training Examples for Enhancing Bicycle Maps</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36593" title="Click to go to the Author Index">Reinders, Christoph</a></td><td class="r">Leibniz Univ. Hannover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25579" title="Click to go to the Author Index">Ackermann, Hanno</a></td><td class="r">Leibniz Univ. Hannover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36594" title="Click to go to the Author Index">Yang, Michael Ying</a></td><td class="r">Univ. of Twente</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19455" title="Click to go to the Author Index">Rosenhahn, Bodo</a></td><td class="r">Leibniz Univ. Hannover</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab233" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a></span><br>
                              <strong>Abstract:</strong> In recent years, data-driven methods have shown great success for extracting information about the infrastructure in urban areas. These algorithms are usually trained on large datasets consisting of thousands or millions of labeled training examples. While large datasets have been published regarding cars, for cyclists very few labeled data is available although appearance, point of view, and positioning of even relevant objects differ. Unfortunately, labeling data is costly and requires a huge amount of work. In this paper, we thus address the problem of learning with very few labels. The aim is to recognize particular traffic signs in crowdsourced data to collect information which is of interest to cyclists. We propose a system for object recognition that is trained with only 15 examples per class on average. To achieve this, we combine the advantages of convolutional neural networks and random forests to learn a patch-wise classifier. In the next step, we map the random forest to a neural network and transform the classifier to a fully convolutional network. Thereby, the processing of full images is significantly accelerated and bounding boxes can be predicted. Finally, we integrate data of the Global Positioning System (GPS) to localize the predictions on the map. In comparison to Faster R-CNN and other networks for object recognition or algorithms for transfer learning, we considerably reduce the required amount of labeled data. We demonstrate good performance on the recognition of traffic signs for cyclists as well as their localization in maps.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst12"><b>WePS-SST12</b></a></td>
               <td class="r">ZhaoWen Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst12" title="Click to go to the Program at a Glance"><b>Fully Actuated Drive-By-Wire Electric Vehicles</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst12_01">16:00-18:00, Paper WePS-SST12.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0269.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('269'); return false" title="Click to show or hide the keywords and abstract">Path Tracking of Full Drive-By-Wire Electric Vehicle Based on Model Prediction Control (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36647" title="Click to go to the Author Index">Zhang, Bing</a></td><td class="r">Jilin Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36620" title="Click to go to the Author Index">Chen, Guoying</a></td><td class="r">Jilin Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36621" title="Click to go to the Author Index">Zong, Changfu</a></td><td class="r">State Key Lab. of Automotive Simulation and Control, Jilin</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab269" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Developments of wire control and sensing technologies give more possibilities to automatic vehicle. For this, a path tracking controller is proposed to full drive-by-wire electric vehicle (FWEV). Method of model prediction control (MPC) is utilized to design path tracking controller, for ensuing dynamic stability with minimally sacrificing tracking performance, dynamic limit defined by stability phase plane is introduced as optimal constraints. Control method for realizing the desired motion with FWEV is designed subsequently. Finally, simulation results shown that this suggested path tracking controller can ensure the dynamic stability without compromising tracking accuracy.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst13"><b>WePS-SST13</b></a></td>
               <td class="r">LongLiQi Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst13" title="Click to go to the Program at a Glance"><b>The Special Used Unamnned Ground Vehicle in China</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst13_01">16:00-18:00, Paper WePS-SST13.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0482.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('482'); return false" title="Click to show or hide the keywords and abstract">Autonomous Driving System Design for Formula Student Driverless Racecar (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37298" title="Click to go to the Author Index">Tian, Hanqing</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27275" title="Click to go to the Author Index">Ni, Jun</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24956" title="Click to go to the Author Index">Hu, Jibin</a></td><td class="r">Beijing Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab482" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> This paper summarizes the work of building the autonomous system including detection system and path tracking controller for a formula student autonomous racecar. A LIDAR-vision cooperating method of detecting traffic cone which is used as track mark is proposed. Detection algorithm of the racecar also implements a precise and high rate localization method which combines the GPS-INS data and LIDAR odometry. Besides, a track map including the location and color information of the cones is built simultaneously. Finally, the system and vehicle performance on a closed loop track is tested. This paper also briefly introduces the Formula Student Autonomous Competition (FSAC) in 2017.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst13_02">16:00-18:00, Paper WePS-SST13.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0434.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('434'); return false" title="Click to show or hide the keywords and abstract">Research on Independent Driving Electric Vehicle in the Pivot Steering and Experimental Validation (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36652" title="Click to go to the Author Index">Zhao, Yue</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27275" title="Click to go to the Author Index">Ni, Jun</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24956" title="Click to go to the Author Index">Hu, Jibin</a></td><td class="r">Beijing Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab434" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Electric_and_Hybrid_Technologies" title="Click to go to the Keyword Index">Electric and Hybrid Technologies</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong>  In recent years, researchers concern the ability of the independent driving vehicle with the development of the electric vehicle. Hub-motor and wheel-side motor driving structure promotes the performance of vehicles. But most of the researchers only concern the improving in handing-stability and ignore the ability of pivot steering which highly improves the maneuverability. So in this paper, the pivot steering process of a rear driving vehicle equipped with wheel side motor has been researched. Based on a 7DOF math model and experiment of a prototype vehicle, dynamic feature of this process has been explored and the mechanism of the action has been found.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst13_03">16:00-18:00, Paper WePS-SST13.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0171.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('171'); return false" title="Click to show or hide the keywords and abstract">Robust H&#8734; Handling Stability Control for All-Wheel Independent Steering Vehicle with Time Delay (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36400" title="Click to go to the Author Index">Zhang, Naisi</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27275" title="Click to go to the Author Index">Ni, Jun</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24956" title="Click to go to the Author Index">Hu, Jibin</a></td><td class="r">Beijing Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Intelligent_Ground__Air_and_Space_Vehicles" title="Click to go to the Keyword Index">Intelligent Ground, Air and Space Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> A robust control method considering the pole placement for handling stability of the ground vehicles is studied in this paper. For anglicizing the lateral dynamics problem, the bike vehicle model is introduced firstly. Since saturation and non-linear phenomenon may happen on tires during the time when the vehicle is steering, parameters like cornering stiffness would change during that procedure and hence there’re parameter uncertainties in the system. Moreover, the modelling uncertainties and disturbance also exist. Robust control is applied for dealing the problem above. A H&#8734; controller is designed to guarantee performance of the handling stability. Pole placement method is used for the consideration of the transient response. Linear matrix inequality method is used for solving the gain of the feedback matrix. Simulations are done for analyzing the effectiveness of the controller. The controller can improve the safety and helping the vehicle track the trajectory.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weps-sst14"><b>WePS-SST14</b></a></td>
               <td class="r">BoSiDeng Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weps-sst14" title="Click to go to the Program at a Glance"><b> Intelligent Vehicle Motion Control and Safety</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst14_01">16:00-18:00, Paper WePS-SST14.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0276.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('276'); return false" title="Click to show or hide the keywords and abstract">Fuzzy Steering Assistance Control for Path Following of the Steer-By-Wire Vehicle Considering Characteristics of Human Driver (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36713" title="Click to go to the Author Index">Dai, Mengmeng</a></td><td class="r">Southeast Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36707" title="Click to go to the Author Index">Wang, Jinxiang</a></td><td class="r">Southeast Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31617" title="Click to go to the Author Index">Chen, Nan</a></td><td class="r">Southeast Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31552" title="Click to go to the Author Index">Yin, Guodong</a></td><td class="r">Southeast Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab276" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> The fuzzy full-order dynamic output-feedback steering assistance control is proposed in this paper to follow large-curvature path for the steer-by-wire (SBW) vehicle. The driver-vehicle-road (DVR) model to follow large-curvature path is built under the assumption that the near and far vision information of the road for guidance is considered by the human driver. Five parameters describing the driver's steering characteristics and behaviors are considered as uncertainties of the DVR models with different drivers. The Takagi-Sugeno (T-S) fuzzy model is applied to handle these uncertainties in designing the dynamic output-feedback parallel distributed compensator (DPDC). The compensator design is then reduced to solving several linear matrix inequalities (LMIs). Simulation results show that the proposed controller can provide different human drivers with individual steering assistance in following the large-curvature path, and can reduce the driver's physical and mental workloads.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst14_02">16:00-18:00, Paper WePS-SST14.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0228.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('228'); return false" title="Click to show or hide the keywords and abstract">Study on Vehicular Adaptive Cruise Control in Complicated Driving Conditions (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36583" title="Click to go to the Author Index">Yang, Xiujian</a></td><td class="r">Kunming Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36584" title="Click to go to the Author Index">Li, Jinyu</a></td><td class="r">Kunming Univ. of Science and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39215" title="Click to go to the Author Index">Cui, Yifeng</a></td><td class="r">Linyi Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab228" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> This work concentrates on the problem of vehicular adaptive cruise control (ACC) in complicated driving conditions especially in low-adhesion road situations, and an ACC control scheme based on real-time estimation of road adhesion is proposed. The ACC controller is designed based on the model predictive control (MPC) method and the steady-state inter-vehicle spacing, relative velocity and the acceleration limits are incorporated into the MPC controller as the constraints. Thereby, the MPC problem is converted into the problem of optimal control with constraints and a receding-horizon approach is utilized to compute the desired vehicle acceleration. The real-time estimation of road adhesion coefficient is accomplished by the recursive least square (RLS) method. A four-degree-of-freedom (4DOF) nonlinear vehicle model is established based on the Lagrangian approach to act as the actual vehicle in the ACC system. The proposed ACC scheme is evaluated by simulations respectively in high and low adhesion conditions. The results reveal that collisions may occur in low adhesion condition if a fixed acceleration limit constraint is employed in the MPC control law. While the proposed adhesion-estimation based ACC system can accomplish the desired inter-vehicle distance safely.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst14_03">16:00-18:00, Paper WePS-SST14.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0591.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('591'); return false" title="Click to show or hide the keywords and abstract">Multi-Module Range Anxiety Reduction Scheme for Battery-Powered Vehicles (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34811" title="Click to go to the Author Index">Faraj, Mahmoud</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14389" title="Click to go to the Author Index">Fidan, Baris</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37546" title="Click to go to the Author Index">Gaudet, Vincent</a></td><td class="r">Univ. of Waterloo</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab591" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Electric_and_Hybrid_Technologies" title="Click to go to the Keyword Index">Electric and Hybrid Technologies</a>, <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a></span><br>
                              <strong>Abstract:</strong> Limited battery capacity and long charging time resulting in what is known as range anxiety have been major obstacles to the widespread adoption of electric vehicles. In addition to running out of battery power, some drivers are also concerned about the amount of time required to recharge their batteries (i.e., time anxiety). This paper focuses on these problems, proposing a Multi-Module Range Anxiety Reduction Scheme. The proposed scheme takes into account traffic density on roadways to provide an accurate computation of energy consumption to charging stations in order to overcome the driver’s concern of being stranded en-route. Furthermore, it addresses the driver’s concern about completing the recharging process in either minimum energy or time. Simulations are conducted to test and validate the proposed scheme.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="weps-sst14_04">16:00-18:00, Paper WePS-SST14.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0226.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('226'); return false" title="Click to show or hide the keywords and abstract">Task Period Selection for Engagement Control of Automatic Clutches (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35841" title="Click to go to the Author Index">Cheng, Xiaoxuan</a></td><td class="r">Shanghai Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36537" title="Click to go to the Author Index">Cheng, Cheng</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36577" title="Click to go to the Author Index">Chen, Li</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab226" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Task period of the control algorithm in electronically controlled clutches is one of the influencing factors on the engagement quality and computation load. The previous studies provide control algorithms in continuous time-domain, but ignore the performance degradation due to discretization. In order to select an appropriate task period on engagement quality, this paper analyzes the friction-involved clutch engagement dynamics in discrete domain. Models of a closed-loop control system are developed. The models consist of the powertrain with a clutch element, the clutch actuator with time delay, the classical control algorithm combining a feedback and feed-forward link, and a zero-order holder of the task period. After discretization by z-transform, the stability is analyzed, thereafter, the maximum permissible task period, which is 0.3s for the studied system, is determined in terms of stability. Furthermore, the dynamic response is tested in a dSPACE real-time environment. The results show that the maximum vehicle jerk increases rapidly when the task period falls into the unstable region, and the friction loss ceaselessly increases due to the failed synchronization. On the other hand, when the task period falls into the stable area, basically, shorter period gains less vehicle jerk, and the friction loss can be almost the same as that of the continuous system.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="wesa"><b>WeSA</b></a></td>
               <td class="r">Intelligent Vehicle Proving Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#wesa" title="Click to go to the Program at a Glance"><b>Student Activity</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="weed"><b>WeED</b></a></td>
               <td class="r">Room T21</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#weed" title="Click to go to the Program at a Glance"><b>Exhibition & Demonstration-27June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table>
</div>

<p>&nbsp;<br>&nbsp;</p><p>&nbsp;<br>&nbsp;</p>

</div>

</body>

</html>
