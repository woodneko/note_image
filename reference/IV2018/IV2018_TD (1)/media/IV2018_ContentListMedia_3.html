<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
 <head>
  <title>IV 2018</title>
  
  <link href="style.css" rel="stylesheet" type="text/css" media="screen" />

<script language="JavaScript">

function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
</script>


</head>

<body leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">
			   
<div class="c" id="TheTop"><br></div>
<table border="0" cellspacing="0" cellpadding="1" width="85%" nowrap style="margin: auto">
<tr><td>
<br>
<h2>Technical Program for Thursday June 28, 2018</h2>
<hr>
<br>
</td></tr>
</table>

<div class="c">

                  <span style="color:gray ">To show or hide the keywords and abstract of a paper (if available), click on the paper title</span><br>
                  <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">Open all abstracts</a>&nbsp;&nbsp;
                  <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">Close all abstracts</a>
               
</div>

<div class="c">
<table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thre"><b>ThRE</b></a></td>
               <td class="r">Room T21</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thre" title="Click to go to the Program at a Glance"><b>Registration-28June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thkn"><b>ThKN</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thkn" title="Click to go to the Program at a Glance"><b>Keynote-28June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#11351" title="Click to go to the Author Index">Li, Li</a></td><td class="r">Tsinghua Univ</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thb1b"><b>ThB1B</b></a></td>
               <td class="r">Lobby</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thb1b" title="Click to go to the Program at a Glance"><b>Break1-28June</b></a></td>
               <td class="r">Break</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thosas"><b>ThOSAS</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thosas" title="Click to go to the Program at a Glance"><b>Advanced Driver Assistance Systems</b></a></td>
               <td class="r">Plenary Session</td>
             </tr>
            

<tr><td>Co-Chair: <a href="IV2018_AuthorIndexMedia.html#37529" title="Click to go to the Author Index">Kissai, Moad</a></td><td class="r">Ec. Nationale Supérieure Des Tech. Avancées</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosas_01">10:20-10:40, Paper ThOSAS.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0071.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('71'); return false" title="Click to show or hide the keywords and abstract">Learning-Based Multiple-Path Prediction for Early Warning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20511" title="Click to go to the Author Index">Sato, Ikuro</a></td><td class="r">Denso IT Lab. INC</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36017" title="Click to go to the Author Index">Liu, Guoqing</a></td><td class="r">Denso IT Lab. Inc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab71" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> It has been claimed that development of an in-vehicle early warning system is important to maximize collision avoidance capability. This study aims to provide a method that can predict the true ego-vehicle motion long enough so that early warning could be feasible, using a monocular forward-view camera. Based on an observation that several-second future position of a vehicle is probabilistic rather than deterministic, our approach utilizes a machine learning algorithm that can probabilistically model human driving behavior from data to predict multiple possible paths, such as going straight, turning left, and turning right. Experimental results on KITTI dataset suggest that our probabilistic learning approach could suffice for 4-second early warning, while an orthodox, deterministic learning approach and a simple motion extrapolation approach do not satisfy the requirement.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosas_02">10:40-11:00, Paper ThOSAS.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0129.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('129'); return false" title="Click to show or hide the keywords and abstract">Rendering Physically Correct Raindrops on Windshields for Robustness Verification of Camera-Based Object Recognition</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36180" title="Click to go to the Author Index">von Bernuth, Alexander</a></td><td class="r">Eberhard Karls Univ. Tübingen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36224" title="Click to go to the Author Index">Volk, Georg</a></td><td class="r">Eberhard Karls Univ. Tübingen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25104" title="Click to go to the Author Index">Bringmann, Oliver</a></td><td class="r">Eberhard Karls Univ. Tübingen</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab129" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Recent developments in the field of autonomous cars indicate the appearance of those vehicles on the streets of every city in the near future. This urban driving requires zero error tolerance. In order to guarantee safety requirements self-driving cars and the used software have to pass exhaustive tests under as many different conditions as possible. The more versatile the considered influences and the more thorough the tests made under those influences, the safer the car will drive under real conditions. Unfortunately, it is very time and resource intensive to record the same test set of images over and over again, every time producing, or hoping for, specific conditions; especially when using real test vehicles. This is where environment simulation comes into play.<p>This research investigates the simulation of environmental influences which may affect the sensors used in autonomous vehicles, in particular how raindrops resting on a windshield affect cameras as they may occlude large parts of the field of view. We propose a novel method to render these raindrops using Continuous Nearest Neighbour search leveraging the benefits of Rnobreakdash-trees. The 3D scene in front of the camera, which is generated from stereo images, reflects physically correct in these drops. This leads to near photorealistic simulated results. The derived images may be used to extend the training datasets used for machine learning without being forced to capture new real pictures.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosas_03">11:00-11:20, Paper ThOSAS.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0579.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('579'); return false" title="Click to show or hide the keywords and abstract">Optimizing Vehicle Motion Control for Generating Multiple Sensations</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37529" title="Click to go to the Author Index">Kissai, Moad</a></td><td class="r">Ec. Nationale Supérieure Des Tech. Avancées</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37530" title="Click to go to the Author Index">Mouton, Xavier</a></td><td class="r">Group Renault</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37531" title="Click to go to the Author Index">Monsuez, Bruno</a></td><td class="r">Ec. Nationale Supérieure Des Tech. Avancées</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37532" title="Click to go to the Author Index">Martinez, Didier</a></td><td class="r">Group Renault</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37533" title="Click to go to the Author Index">Tapus, Adriana</a></td><td class="r">Ec. Nationale Supérieure Des Tech. Avancées</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab579" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Most of automotive researches focus on autonomous vehicles. Studies regarding trajectory planning and trajectory tracking became preponderant. As in case of commercial ground vehicles there is a driver in the loop, one should raise the important question of how the trajectory should be tracked. In this paper, we investigate the influence of controlling integrated chassis systems on the vehicle’s behavior. A fixed Model Predictive Control is used to track the trajectory. Tunable vehicle motion control is however used to provide different motion feelings. Results show that a specific trajectory could be followed in different manners. Therefore, vehicle dynamics can be and should be controlled in such a way to generate adaptive trust feelings to passengers in case of autonomous driving.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosas_04">11:20-11:40, Paper ThOSAS.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0587.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('587'); return false" title="Click to show or hide the keywords and abstract">Generic Vehicle Tracking Framework Capable of Handling Occlusions Based on Modified Mixture Particle Filter</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36013" title="Click to go to the Author Index">Li, Jiachen</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33408" title="Click to go to the Author Index">Zhan, Wei</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab587" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Accurate and robust tracking of surrounding road participants plays an important role in autonomous driving. However, there is usually no prior knowledge of the number of tracking targets due to object emergence, disappearance or false alarms. To overcome this challenge, we propose a generic vehicle tracking framework based on modified mixture particle filter, which can make the number of tracking targets adaptive to realtime observations and track all the vehicles within sensor range simultaneously in a uniform architecture without explicit data association. Each object corresponds to a mixture component whose distribution is non-parametric and approximated by particle hypotheses. Most tracking approaches employ vehicle kinematic models as the prediction model. However, it is hard for these models to make proper predictions when sensor measurements are lost or with low quality due to partial or complete occlusions. Moreover, these models are incapable of forecasting sudden maneuvers. To address these problems, we propose to incorporate learning-based behavioral models instead of pure vehicle kinematic models to realize prediction in the prior update of recursive Bayesian state estimation. Two typical driving scenarios including lane keeping and lane change are demonstrated to verify the effectiveness and accuracy of the proposed framework as well as the advantages of employing learning-based models.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosas_05">11:40-12:00, Paper ThOSAS.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0083.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('83'); return false" title="Click to show or hide the keywords and abstract">Courtesy Behavior for Highly Automated Vehicles on Highway Interchanges</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34969" title="Click to go to the Author Index">Menendez-Romero, Cristina</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36094" title="Click to go to the Author Index">Sezer, Mustafa</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34971" title="Click to go to the Author Index">Winkler, Franz</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34970" title="Click to go to the Author Index">Dornhege, Christian</a></td><td class="r">Albert-Ludwigs-Univ. Freiburg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34265" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab83" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> On the highway, human drivers continuously make decisions adapting their driving behavior. In some of these, for example near highway ramp-entrances, they adjust intuitively their own behavior in order to facilitate the merging of the incoming vehicles. On highly automated vehicles, this decision should be taken by the system. In such situations, not only the own goal should be optimized but also the comfort of the surrounding traffic. The ability to plan adequate courtesy behaviors improves public acceptance of autonomous systems and the comfort of the surrounding vehicles without considerably decreasing the own comfort. We present a novel method that automatically adapts the driving behavior, integrating the merging intention of other vehicles. In contrast to other systems, robustness is achieved by considering not only the most likely evolution, but also the expected value of the possible outcomes in real time. The flexibility of this method allows us to integrate it within different planning systems. We are therefore able to offer courtesy behaviors to other vehicles, thereby improving the collective comfort and also safety of the situation. We evaluate the method in simulation and in real world experiments with our test vehicle. Results show an improvement of the aggregate traffic comfort and in addition a reduction of critical situations, as a result of applying our courtesy behavior to different planning strategies.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thlb"><b>ThLB</b></a></td>
               <td class="r">Dining Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thlb" title="Click to go to the Program at a Glance"><b>Lunch-28June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thosbs"><b>ThOSBS</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thosbs" title="Click to go to the Program at a Glance"><b>Mapping and Localization</b></a></td>
               <td class="r">Plenary Session</td>
             </tr>
            
<tr><td>Chair: <a href="IV2018_AuthorIndexMedia.html#36033" title="Click to go to the Author Index">Gopalswamy, Swaminathan</a></td><td class="r">Texas A&M Univ</td></tr>

<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosbs_01">13:30-13:50, Paper ThOSBS.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0401.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('401'); return false" title="Click to show or hide the keywords and abstract">Autonomous Vehicle Testing and Validation Platform: Integrated Simulation System with Hardware in the Loop</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37024" title="Click to go to the Author Index">Chen, Yu</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36464" title="Click to go to the Author Index">Chen, Shitao</a></td><td class="r">Xi'an Jiaotong Univ. Xi'an, China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37198" title="Click to go to the Author Index">Zhang, Tangyike</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37203" title="Click to go to the Author Index">Zhang, Songyi</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab401" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> With the development of autonomous driving, efficient offline testing and validation remains an important process allowing low-cost and efficient validation of vehicle performance and vehicle control algorithms in multiple virtual scenarios. This paper aims to propose a novel simulation platform with hardware in the loop (HiL). This platform comprises of four layers: the vehicle simulation layer, the virtual sensors layer, the virtual environment layer and the Electronic Control Unit (ECU) layer for hardware control. Our platform has attained multiple capabilities: (1) it enables the construction and simulation of kinematic car models, various sensors and virtual testing fields; (2) it performs a closed-loop evaluation of scene perception, path planning, decision-making and vehicle control algorithms, whilst also having multi-agent interaction system; (3) it further enables rapid migrations of control and decision-making algorithms from the virtual environment to real self-driving cars. In order to verify the effectiveness of our simulation platform, several experiments have been performed with self-defined car models in virtual scenarios of a public road and an open parking lot and the results are substantial.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosbs_02">13:50-14:10, Paper ThOSBS.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0334.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('334'); return false" title="Click to show or hide the keywords and abstract">CNN-Based Multi-Frame IMO Detection from a Monocular Camera</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31612" title="Click to go to the Author Index">Fanani, Nolang</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31167" title="Click to go to the Author Index">Ochs, Matthias</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34551" title="Click to go to the Author Index">Stuerck, Alina</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16129" title="Click to go to the Author Index">Mester, Rudolf</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab334" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> This paper presents a method for detecting independently moving objects (IMOs) from a monocular camera mounted on a moving car. A CNN-based classifier is employed to generate IMO candidate patches; independent motion is detected by geometric criteria on keypoint trajectories in these patches. Instead of looking only at two consecutive frames, we analyze keypoints inside the IMO candidate patches through multi-frame epipolar consistency checks. The obtained motion labels (IMO/static) are then propagated over time using the combination of motion cues and appearance-based information of the IMO candidate patches. We evaluate the performance of our method on the KITTI dataset, focusing on sub-sequences containing IMOs.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosbs_03">14:10-14:30, Paper ThOSBS.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0105.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('105'); return false" title="Click to show or hide the keywords and abstract">Dense 3D Semantic SLAM of Traffic Environment Based on Stereo Vision</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19467" title="Click to go to the Author Index">Li, Linhui</a></td><td class="r">Dalian Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36145" title="Click to go to the Author Index">ZhiJie, Liu</a></td><td class="r">Dalian Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10356" title="Click to go to the Author Index">Ozguner, Umit</a></td><td class="r">Ohio State Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36160" title="Click to go to the Author Index">Lian, Jing</a></td><td class="r">Dalian Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36161" title="Click to go to the Author Index">Zhou, Yafu</a></td><td class="r">Dalian Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19466" title="Click to go to the Author Index">Yibing, Zhao</a></td><td class="r">Dalian Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab105" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> To solve the intelligent vehicles’ problems of ‘where am I?’ and ‘what is around me?’, a dense 3D sematic Simultaneous Localization and Mapping (SLAM) system is proposed to evaluate the pose of the intelligent vehicles and build the dense 3D semantic map. We address these challenges by combining a state of art Stereo-ORB-SLAM system and Convolutional Neural Networks. Firstly, we build a dense 3D point cloud map by using a four thread Stereo-ORB-SLAM system. Subsequently, a fully convolutional neural network architecture which uses RGB-D image as input is used to obtain pixel-wise segmentation. Finally, we fuse the geometric information and semantic information to get the semantic map. We test our method on the KITTI dataset and our dataset made with the Fpgalena stereo camera. Results indicate the system was effective in the real-time building of a semantic map, the speed of the entire system is about 10Hz, and the loop closing function can eliminate most of the drifting errors.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosbs_04">14:30-14:50, Paper ThOSBS.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0635.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('635'); return false" title="Click to show or hide the keywords and abstract">Vehicle Localization Using 76GHz Omnidirectional Millimeter-Wave Radar for Winter Automated Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25257" title="Click to go to the Author Index">Yoneda, Keisuke</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37616" title="Click to go to the Author Index">Hashimoto, Naoya</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34776" title="Click to go to the Author Index">Yanase, Ryo</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34775" title="Click to go to the Author Index">Aldibaja, Mohammad</a></td><td class="r">Kanazawa Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13867" title="Click to go to the Author Index">Suganuma, Naoki</a></td><td class="r">Kanazawa Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab635" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> This paper presents the 76GHz MWR (Millimeter-Wave Radar)-based self-localization method for automated driving during snowfall. Previously, there were many LIDAR (Light Detection and Ranging)-based localization techniques proposed for high measurement accuracy and robustness to changes of day and night. However, they did not provide effective approaches for snow conditions because of sensing noise (i.e., environmental resistance) created by snowfall.Therefore, this paper developed an MWR-based map generation and a real-time localization method by modeling the uncertainties of error propagation. Quantitative evaluations are performed on driving data with and without snow conditions, using a LIDAR-based method as the baseline. Experimental results show that a lateral RMS (root mean square) error of about 0.25m can be obtained, regardless of the presence or absence of snowfall. Thus, it can be investigated that a potential performance of radar-based localization.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosbs_05">14:50-15:10, Paper ThOSBS.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0566.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('566'); return false" title="Click to show or hide the keywords and abstract">Planecell: Representing Structural Space with Plane Elements</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35839" title="Click to go to the Author Index">Fan, Lei</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18409" title="Click to go to the Author Index">Chen, Long</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34952" title="Click to go to the Author Index">Huang, Kai</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34596" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Cranfield Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab566" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a></span><br>
                              <strong>Abstract:</strong> Reconstruction based on the stereo camera has received considerable attention recently, but two particular challenges still remain. The first concerns the need to present and compress data in an effective way, and the second is to maintain as much of the available information as possible while ensuring sufficient accuracy. To overcome these issues, we propose a new 3D representation method, namely, planecell, that extracts planarity from the depth-assisted image segmentation and then directly projects these depth planes into the 3D world. The proposed method demonstrates its advancement especially dealing with large-scale structural environment, such as autonomous driving scene. The reconstruction result of our method achieves equal accuracy compared to dense point clouds and compresses the output file 200 times. To further obtain global surfaces, an energy function formulated from Conditional Random Field that generalizes the planar relationships is maximized. We evaluate our method with reconstruction baselines on the KITTI outdoor scene dataset, and the results indicate the superiorities compared to other 3D space representation methods in accuracy, memory requirements and the scope of applications.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thosbs_06">15:10-15:30, Paper ThOSBS.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0303.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('303'); return false" title="Click to show or hide the keywords and abstract">Infrastructure Enabled Autonomy: A Distributed Intelligence Architecture for Autonomous Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36033" title="Click to go to the Author Index">Gopalswamy, Swaminathan</a></td><td class="r">Texas A&M Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36790" title="Click to go to the Author Index">Rathinam, Sivakumar</a></td><td class="r">Texas a & M Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab303" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Smart_Infrastructure" title="Click to go to the Keyword Index">Smart Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a></span><br>
                              <strong>Abstract:</strong> Multiple studies have illustrated the potential for dramatic societal, environmental and economic benefits from significant penetration of autonomous driving. However, all the current approaches to autonomous driving require the automotive manufacturers to shoulder the primary responsibility and liability associated with replacing human perception and decision making with automation, potentially slowing the penetration of autonomous vehicles, and consequently slowing the realization of the societal benefits of autonomous vehicles. We propose here a new approach to autonomous driving that will re-balance the responsibility and liabilities associated with autonomous driving between traditional automotive manufacturers, infrastructure players, and third-party players. Our proposed distributed intelligence architecture leverages the significant advancements in connectivity and edge computing in the recent decades to partition the driving functions between the vehicle, edge computers on the road side, and specialized third-party computers that reside in the vehicle. Infrastructure becomes a critical enabler for autonomy. With this Infrastructure Enabled Autonomy (IEA) concept, the traditional automotive manufacturers will only need to shoulder responsibility and liability comparable to what they already do today, and the infrastructure and third-party players will share the added responsibility and liabilities associated with autonomous functionalities. We propose a Bayesian Network Model based framework for assessing the risk benefits of such a distributed intelligence architecture. An additional benefit of the proposed architecture is that it enables &quot;autonomy as a service&quot; while still allowing for private ownership of automobiles.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thb2b"><b>ThB2B</b></a></td>
               <td class="r">Lobby</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thb2b" title="Click to go to the Program at a Glance"><b>Break2-28June</b></a></td>
               <td class="r">Break</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst1"><b>ThPS-SST1</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst1" title="Click to go to the Program at a Glance"><b>Semantic Segmentation & Autonomous Driving</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_01">16:00-18:00, Paper ThPS-SST1.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0373.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('373'); return false" title="Click to show or hide the keywords and abstract">Adaptive Behavior Generation for Autonomous Driving Using Deep Reinforcement Learning with Compact Semantic States</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31970" title="Click to go to the Author Index">Wolf, Peter</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35406" title="Click to go to the Author Index">Kurzer, Karl</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36938" title="Click to go to the Author Index">Wingert, Tobias</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27237" title="Click to go to the Author Index">Kuhnt, Florian</a></td><td class="r">FZI Forschungszentrum Informatik</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14985" title="Click to go to the Author Index">Zöllner, J. Marius</a></td><td class="r">FZI Res. Center for Information Tech. KIT Karlsruhe In</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab373" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_02">16:00-18:00, Paper ThPS-SST1.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0549.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('549'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Road Lane Semantic Segmentation for High Definition Map</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35112" title="Click to go to the Author Index">Jang, Wonje</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30974" title="Click to go to the Author Index">An, Jhonhyun</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37448" title="Click to go to the Author Index">Lee, Sangyun</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30912" title="Click to go to the Author Index">Cho, Minho</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37468" title="Click to go to the Author Index">Sun, MyungKi</a></td><td class="r">Hyundai-Mnsoft</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14782" title="Click to go to the Author Index">Kim, Euntai</a></td><td class="r">Yonsei Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab549" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0549.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> High Definition map (HD Map) is an important part of autonomous driving vehicle. Most conventional method to generate HD map requires expensive system and post- processing of observed data. In this paper, we propose automatic HD map generating algorithm using just monocular camera without further human labors. The proposed algorithm detects road lane from image and classifies the type of road lane at pixel-level with Fully Convolutional Network (FCN) which outperforms the other semantic segmentation methods. The segmentation results are used to extract lane features, and the features are used for loop-closure detection. Final map is generated with graph-based Simultaneous Localization and Mapping (SLAM) algorithm. The experiment is done with monocular camera mounted on mobile vehicle. In this paper, final map generated by proposed method is compared with aerial view data. The results show that the proposed method can generate reliable map that is comparable to real roads even only the low-cost sensor is used.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_03">16:00-18:00, Paper ThPS-SST1.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0586.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('586'); return false" title="Click to show or hide the keywords and abstract">A Simple Weight Recall for Semantic Segmentation: Application to Urban Scenes</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37179" title="Click to go to the Author Index">Li, Xuhong</a></td><td class="r">Univ. De Tech. De Compiègne</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23015" title="Click to go to the Author Index">Davoine, Franck</a></td><td class="r">CNRS, Univ. De Tech. De Compiègne</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37487" title="Click to go to the Author Index">Grandvalet, Yves</a></td><td class="r">CNRS/UTC</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab586" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> The performance of most transfer learning tasks, including semantic image segmentation, can be effectively improved through fine-tuning pre-trained convolutional networks, instead of training from scratch. When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving a segmentation task, but that would be difficult to extract from a smaller amount of training data for segmentation of urban driving scenes. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in classical fine-tuning approaches for keeping the generic features. Even worse, the standard weight decay drives the parameters towards the origin and harms the learned features. In this paper, we propose a simple weight recall approach that uses the pre-trained model as a reference, and we show that the simple regularization that we propose consistently improves the performance when applied to semantic urban driving scene segmentation. Experiments are done on the Cityscapes dataset, with four different convolutional networks.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_04">16:00-18:00, Paper ThPS-SST1.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0646.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('646'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>MultiNet: Real-Time Joint Semantic Reasoning for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37643" title="Click to go to the Author Index">Teichmann, Marvin</a></td><td class="r">Univ. of Cambridge</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31616" title="Click to go to the Author Index">Weber, Michael</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14985" title="Click to go to the Author Index">Zöllner, J. Marius</a></td><td class="r">FZI Res. Center for Information Tech. KIT Karlsruhe In</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37644" title="Click to go to the Author Index">Cipolla, Roberto</a></td><td class="r">Univ. of Cambridge</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32069" title="Click to go to the Author Index">Urtasun, Raquel</a></td><td class="r">Univ. of Toronto</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab646" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0646.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> While most approaches to semantic reasoning have focused on improving performance, in this paper we argue that computational times are very important in order to enable real time applications such as autonomous driving. Towards this goal, we present an approach to joint classification, detection and semantic segmentation via a unified architecture where the encoder is shared amongst the three tasks. Our approach is very simple, can be trained end-to-end and performs extremely well in the challenging KITTI dataset. Our approach is also very efficient, allowing us to perform inference at more then 23 frames per second.<p>Training scripts and trained weights to reproduce our results can be found here: url{https://github.com/MarvinTeichmann/MultiNet}
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_05">16:00-18:00, Paper ThPS-SST1.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0272.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('272'); return false" title="Click to show or hide the keywords and abstract">LiSeg: Lightweight Road-Object Semantic Segmentation in 3D LiDAR Scans for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31208" title="Click to go to the Author Index">Zhang, Wenquan</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36559" title="Click to go to the Author Index">Zhou, Chancheng</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36191" title="Click to go to the Author Index">Yang, Junjie</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34952" title="Click to go to the Author Index">Huang, Kai</a></td><td class="r">Sun Yat-Sen Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab272" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> LiDAR based perception module plays an important role in autonomous driving. Deep learning is applied to cope with the problems on the traditional multi-stage pipeline and feature design for LiDAR point cloud based scene understanding. However, the present CNN models are designed for image processing but not LiDAR point clouds. The performances of such models are limited by the great memory consumption and heavy computation cost. In this work, a lightweight CNN model, LiSeg, is proposed to perform real-time road-object semantic segmentation on LiDAR point cloud scans for autonomous driving. The model size of LiSeg is several times smaller than others, while achieving high accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_06">16:00-18:00, Paper ThPS-SST1.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0424.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('424'); return false" title="Click to show or hide the keywords and abstract">On the Impact of Illumination-Invariant Image Pre-Transformation for Contemporary Automotive Semantic Scene Understanding</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37105" title="Click to go to the Author Index">Alshammari, Naif</a></td><td class="r">Durham Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37107" title="Click to go to the Author Index">Akcay, Samet</a></td><td class="r">Durham Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16017" title="Click to go to the Author Index">Breckon, Toby</a></td><td class="r">Durham Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab424" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Illumination changes in outdoor environments under non-ideal weather conditions have a negative impact on automotive scene understanding and segmentation performance. In this paper, we present an evaluation of illumination-invariant image transforms applied to this application domain. We compare four recent transforms for illumination invariant image representation, individually and with colour hybrid images, to show that despite assumptions to contrary such invariant pre-processing can improve the state of the art in scene understanding performance. In addition, we propose a robust approach based on using an illumination-invariant image representation, combined with the chromatic component of a perceptual colour-space to improve contemporary automotive scene understanding and segmentation. By using an illumination invariant pre-process, to reduce the impact of environmental illumination changes, we show that the performance of deep convolutional neural network based scene understanding and segmentation can yet be further improved. This illuminating result enforces the need for invariant (unbiased) training sets within such deep network training and shows that even a well trained network may still not offer truly optimal performance (if we ignore any prior data transforms attributable to a priori insight). Our approach is demonstrated over a range of example imagery where we show a notable improvement in performance using pre-processed, illumination invariant, automotive scene imagery.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_07">16:00-18:00, Paper ThPS-SST1.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0043.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('43'); return false" title="Click to show or hide the keywords and abstract">Unifying Terrain Awareness through Real-Time Semantic Segmentation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35840" title="Click to go to the Author Index">Yang, Kailun</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12570" title="Click to go to the Author Index">Bergasa, Luis M.</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31401" title="Click to go to the Author Index">Romera, Eduardo</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35933" title="Click to go to the Author Index">Cheng, Ruiqi</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35928" title="Click to go to the Author Index">Chen, Tianxue</a></td><td class="r">Univ. of California, Los Angeles</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35929" title="Click to go to the Author Index">Wang, Kaiwei</a></td><td class="r">Zhejiang Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab43" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Assistive_Mobility_Systems" title="Click to go to the Keyword Index">Assistive Mobility Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> Active research on computer vision accelerates the progress in autonomous driving. Following this trend, we aim to leverage the recently emerged methods for Intelligent Vehicles (IV), and transfer them to develop navigation assistive technologies for the Visually Impaired (VI). This topic grows notoriously challenging as it requires to detect a variety of scenes towards higher level of assistance. Computer vision based techniques with monocular detectors or depth sensors sprung up within years of research. These separate approaches achieved remarkable results with relatively low processing time, and improved the mobility of visually impaired people to a large extent. However, running all detectors jointly increases the latency and burdens the computational resources. In this paper, we put forward to seize pixel-wise semantic segmentation to cover the perception needs of navigational assistance in a unified way. This is critical not only for the terrain awareness regarding traversable areas, sidewalks, stairs and water hazards, but also for the avoidance of short-range obstacles, fast-approaching pedestrians and vehicles. At the heart of our proposal is a combination of efficient residual factorized network (ERFNet), pyramid scene parsing network (PSPNet) and 3D point cloud based segmentation. This approach proves to be with qualified accuracy and speed for real-world applications by a comprehensive set of experiments on a wearable navigation system.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_08">16:00-18:00, Paper ThPS-SST1.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0345.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('345'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>CNN-Based Fisheye Image Real-Time Semantic Segmentation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36751" title="Click to go to the Author Index">Sáez, Álvaro</a></td><td class="r">Univ. De Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12570" title="Click to go to the Author Index">Bergasa, Luis M.</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31401" title="Click to go to the Author Index">Romera, Eduardo</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32189" title="Click to go to the Author Index">López-Guillén, Elena</a></td><td class="r">Univ. of Alcalá</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24492" title="Click to go to the Author Index">Barea, Rafael</a></td><td class="r">Univ. of Alcala</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37133" title="Click to go to the Author Index">Sanz, Rafael</a></td><td class="r">Univ. of Vigo</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab345" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0345.VD.mpg" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Semantic segmentation based on Convolutional Neural Networks (CNNs) has been proven as an efficient way of facing scene understanding for autonomous driving applications. Traditionally, environment information is acquired using narrow-angle pin-hole cameras, but autonomous vehicles need wider field of view to perceive the complex surrounding, especially in urban traffic scenes. Fisheye cameras have begun to play an increasingly role to cover this need. This paper presents a real-time CNN-based semantic segmentation solution for urban traffic images using fisheye cameras. We adapt our Efficient Residual Factorized CNN (ERFNet) architecture to handle distorted fish-eye images. A new fisheye image dataset for semantic segmentation from the existing CityScapes dataset is generated to train and evaluate our CNN. We also test a data augmentation suggestion for fisheye image proposed in [1]. Experiments show outstanding results of our proposal regarding other methods of the state of the art.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_09">16:00-18:00, Paper ThPS-SST1.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0565.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('565'); return false" title="Click to show or hide the keywords and abstract">Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37500" title="Click to go to the Author Index">Meletis, Panagiotis</a></td><td class="r">TU Eindhoven</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16104" title="Click to go to the Author Index">Dubbelman, Gijs</a></td><td class="r">Eindhoven Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab565" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> We propose a convolutional network with hierarchical classifiers for per-pixel semantic segmentation, which is able to be trained on multiple, heterogeneous datasets and exploit their semantic hierarchy. Our network is the first to be simultaneously trained on three different datasets from the intelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and is able to handle different semantic level-of-detail, class imbalances, and different annotation types, i.e. dense per-pixel and sparse bounding-box labels. We assess our hierarchical approach, by comparing against flat, non-hierarchical classifiers and we show improvements in mean pixel accuracy of 13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB classes. Our implementation achieves inference rates of 17 fps at a resolution of 520 x 706 for 108 classes running on a GPU.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst1_10">16:00-18:00, Paper ThPS-SST1.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0124.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('124'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A New Metric for Evaluating Semantic Segmentation: Leveraging Global and Contour Accuracy</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36249" title="Click to go to the Author Index">Fernandez-Moral, Eduardo</a></td><td class="r">INRIA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34117" title="Click to go to the Author Index">Martins, Renato</a></td><td class="r">Inria</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22913" title="Click to go to the Author Index">Wolf, Denis</a></td><td class="r">Univ. of Sao Paulo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15318" title="Click to go to the Author Index">Rives, Patrick</a></td><td class="r">INRIA-Sophia</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab124" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0124.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> Semantic segmentation of images is an important issue for intelligent vehicles and mobile robotics because it offers basic information which can be used for complex reasoning and safe navigation. Different solutions have been proposed for this problem along the last two decades, where recent deep neural networks approaches have shown very promising results in the context of urban navigation. One of the main problems when comparing different semantic segmentation solutions is how to select an appropriate metric to evaluate their accuracy. On the one hand, classic metrics do not measure properly the accuracy on the object contours, which is important in urban driving to differentiate road from sidewalk for instance. On the other hand, contour-based metrics disregard the information far from class contours. This paper explores the problem multi-modal image segmentation, and presents a new metric to leverage global and contour accuracy in a simple formulation. This metric is validated with the evaluation of several semantic segmentation solutions that exploit RGB-D images to rank these solutions taking into account the quality of the segmented contours. We also present a comparative analysis of several commonly used metrics together with a statistical analysis of their correlation.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst2"><b>ThPS-SST2</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst2" title="Click to go to the Program at a Glance"><b>Vehicular Communication and Networks-1</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst2_01">16:00-18:00, Paper ThPS-SST2.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0404.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('404'); return false" title="Click to show or hide the keywords and abstract">CPSS-Based Signal Forwarding Method at Relays for Full-Duplex Cooperative Vehicular Networks</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28116" title="Click to go to the Author Index">Han, Shuangshuang</a></td><td class="r">Inst. of Automation, Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29722" title="Click to go to the Author Index">Wang, Xiao</a></td><td class="r">Chinese Acad. of Science, Inst. of Automation</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34596" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10337" title="Click to go to the Author Index">Wang, Fei-Yue</a></td><td class="r">Chinese Acad. of Sciences</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab404" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> With increasing popularity of Internet of Vehicles (IoV), concerns for reliable and low complexity communication techniques are proposed due to the requirements of signal reliability and transmission delay for vehicles. Meanwhile, the explosive and pervasive use of social network applications further adds drivers' social relationships and behavioural characteristics into it, and makes it a cyber-physical-social system (CPSS). This paper proposes and analyzes an improved forward scheme for full deplex cooperative vehicular networks in terms of its CPSS features. The proposed CPSS-based forwarding (CPSS-F) strategy forwards a soft estimate of the received signal based on social historic data at the relay node (vehicle/infrastructure) to the destination node (vehicle/infrastructure), which achieves improved reliability than the two conventional strategies in cooperative networks, i.e., amplify-and-forward (AF) and decode-and-forward (DF). Furthermore, the proposed CPSS-F relay achieves performance gains and complexity reduction compared to the conventional AF and DF. Experimental results further confirm the advantages of the proposed CPSS-F for cooperative vehicular networks. The proposed CPSS-F approach is easily extended to other cooperative vehicular social networks, for example, multi-way, half-duplex, or large-antenna networks.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst2_02">16:00-18:00, Paper ThPS-SST2.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0616.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('616'); return false" title="Click to show or hide the keywords and abstract">Risk Analysis for the Wireless Communication of the High-Speed Maglev under the Cognitive Uncertainties</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37572" title="Click to go to the Author Index">Wenyi, Zheng</a></td><td class="r">Jiangsu Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24123" title="Click to go to the Author Index">Chien, Stanley</a></td><td class="r">Indiana Univ. Univ. Indianapolis</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab616" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> The wireless communication between a train and the ground in 38GHz band is an important subsystem in operation and control system (OCS) which is the artificial neural network of the whole high-speed maglev transportation system. The risk analysis of wireless communication reliability is a critical issue related to operation safety. However, the fault parameters especially under cognitive uncertainties cannot always be properly estimated, not only because of the large number of redundancy design being applied in wireless communication system for the sake of reliability enhancement, but also the lack of status data collected by distributed sensors of the maintenance and management subsystem(MMS) displayed in wireless communication equipment due to the limited operation life. In this paper, based on the status data acquired by the distributed sensors from MMS, a risk analysis method is proposed using Continuous Time Bayesian Network with its key condition parameters expressed by Triangular Fuzzy Numbers. Moreover, with an expert confidence index, the credibility of the fuzzy probability evaluation for basic risk factors can be ensured, which means the limitation in risk analysis caused by cognitive uncertainties can be eliminated. The method proposed in this paper integrates together a risk analysis process with deductive reasoning, sensitivity analysis and abductive inference. Compared with traditional fault tree analysis, the proposed method is more flexible and adaptive in fault diagnosis and dynamic reliability estimation of wireless communication for high-speed maglev train.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst2_03">16:00-18:00, Paper ThPS-SST2.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0605.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('605'); return false" title="Click to show or hide the keywords and abstract">A Security Aware Fuzzy Enhanced Reliable Ant Colony Optimization Routing in Vehicular Ad Hoc Networks</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37555" title="Click to go to the Author Index">Zhang, Hang</a></td><td class="r">Univ. of Goettingen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37566" title="Click to go to the Author Index">Bochem, Arne</a></td><td class="r">Univ. of Goettingen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37568" title="Click to go to the Author Index">Sun, Xu</a></td><td class="r">Univ. of Goettingen</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37569" title="Click to go to the Author Index">Hogrefe, Dieter</a></td><td class="r">Univ. of Goettingen</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab605" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Telematics" title="Click to go to the Keyword Index">Telematics</a></span><br>
                              <strong>Abstract:</strong> With the growing relevance for Vehicular Ad Hoc Networks (VANETs), the number of applications for such networks also grows. These networks allow vehicles to coordinate, improving both efficiency and safety of road traffic. To make such networks feasible, efficient routing protocols that are robust against malfunctioning or malicious network participants are required. Additionally, the routing algorithm has to be able to cope with the transient nature of connections in VANETs as vehicles pass by each other at high speeds. In this work, we propose the Security Aware Fuzzy Enhanced Reliable Ant Colony Optimization (SAFERACO) routing protocol which makes use of a fuzzy logic module to identify misbehaving nodes and exclude them from the routing process. We implement SAFERACO in the NS-3 simulator and evaluate it under different scenarios. The results show superior performance in all relevant metrics, such as packet delivery rate and delay. Due to its ability to identify misbehaving nodes, SAFERACO also provides a high level of robustness against the black hole and flooding attacks.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst2_04">16:00-18:00, Paper ThPS-SST2.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0589.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('589'); return false" title="Click to show or hide the keywords and abstract">Towards "Smarter" Vehicles through Cloud-Backed Swarm Cognition</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36980" title="Click to go to the Author Index">Vega, Augusto</a></td><td class="r">IBM Res</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37160" title="Click to go to the Author Index">Buyuktosunoglu, Alper</a></td><td class="r">IBM T. J. Watson Res</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37164" title="Click to go to the Author Index">Bose, Pradip</a></td><td class="r">IBM T. J. Watson Res. Center</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab589" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a></span><br>
                              <strong>Abstract:</strong> We consider an exciting new computing paradigm called &quot;cloud-backed swarm cognition&quot;. It refers to an envisioned future where (mobile) edge devices work together collaboratively to provide intelligent services. The cloud provides a resilient back-up cover and supervision for such services. A specific example domain is that of smart, connected autonomous vehicles (e.g. cars, buses, trucks, or drones). In this computational paradigm, the cloud serves as the (relatively) stable repository of reference knowledge that is less frequently accessed than in non-swarm computation. The &quot;vehicle swarm&quot;, on the other hand, serves as a (relatively) dynamic cache of knowledge at the &quot;edge&quot;. Leveraging the collaborative swarm mode reduces real-time deadline pressures at the individual node level, while improving edge resilience through redundancy. Overall, this leads to smarter vehicles through: (a) improved edge inferential accuracy and (b) improved system-level energy efficiency. In this paper, we consider the system architecture represented by the cloud-backed swarm cognition apparatus. We provide a visionary perspective of the fundamental trade-offs that one must model and interpret in tuning the parameters of this new architecture in a scenario where a swarm of connected cars conducts real-time traffic sign recognition.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst2_05">16:00-18:00, Paper ThPS-SST2.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0594.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('594'); return false" title="Click to show or hide the keywords and abstract">A Realistic Analytical Model for Uplink Drive-Thru Internet</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37525" title="Click to go to the Author Index">Cao, Shengbin</a></td><td class="r">City Univ. of Hong Kong</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19293" title="Click to go to the Author Index">Lee, Victor C. S.</a></td><td class="r">City Univ. of Hong Kong</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab594" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> With the advent of various mobile Internet applications and social network services, the demand for Internet access from traveling vehicles has largely increased. In view of the ability to provide cost-effective Internet access, the Drive-thru Internet system, where road-side Access Points (APs) enable vehicular users to obtain temporary Internet connection as the vehicle passes through, is drawing dramatic attention. In this paper, we propose an analytical model to evaluate the performance of an uplink Drive-thru Internet system in the real channel conditions. The proposed analytical model accurately quantifies the performance metrics of an uplink Drive-thru Internet in terms of a number of system parameters.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst2_06">16:00-18:00, Paper ThPS-SST2.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0471.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('471'); return false" title="Click to show or hide the keywords and abstract">A Dual Traffic Network Coupled with Improved Delay Models to Estimate Travel Time in Urban Traffic Systems</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37104" title="Click to go to the Author Index">Fan, Xinqi</a></td><td class="r">Southwest Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37231" title="Click to go to the Author Index">Liang, Yan</a></td><td class="r">Southwest Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37232" title="Click to go to the Author Index">Sun, Yuting</a></td><td class="r">Southwest Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37233" title="Click to go to the Author Index">Fan, Zichuan</a></td><td class="r">Southwest Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab471" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Travel time estimation plays a core role in intelligent traffic systems, as people or smart vehicles can choose a better travel route in advance, while local authorities can make urban traffic planning based on it. The paper proposes a dual traffic network topology coupled with improved delay models to estimate travel time in urban traffic system. The structure of the system is demonstrated by the dual traffic network (DTN) topology. It hires nodes to represent joint points of links and intersections; it also has two kinds of edges – links and turning cases of intersections. The BPR model is improved by concerning the influence of pedestrians and the lane width, and the HCM model is improved by adding travel time at intersections and considering three different turning cases to match with the DTN topology. The proposed methodology is evaluated by a comparison and an experiment in Changzhou, China. The error is greatly reduced, even though the degree of saturation exceeds 0.72. This proposed estimation methodology can improve the estimation accuracy significantly, especially in high degree of saturation cases.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst3"><b>ThPS-SST3</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst3" title="Click to go to the Program at a Glance"><b>Vehicular Communication and Networks-2</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst3_01">16:00-18:00, Paper ThPS-SST3.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0353.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('353'); return false" title="Click to show or hide the keywords and abstract">Collaborative Perception for Automated Vehicles Leveraging Vehicle-To-Vehicle Communications</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36608" title="Click to go to the Author Index">Yee, Ryan Matthew</a></td><td class="r">Exponent</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36609" title="Click to go to the Author Index">Chan, Ellick</a></td><td class="r">Exponent</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36611" title="Click to go to the Author Index">Senatore, Carmine</a></td><td class="r">Exponent</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32055" title="Click to go to the Author Index">Cheng, Bin</a></td><td class="r">USA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34124" title="Click to go to the Author Index">Bansal, Gaurav</a></td><td class="r">Toyota InfoTechnology Center, USA</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab353" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> Currently, many automated vehicle systems primarily perceive the environment from a single perspective and as a result are unable to leverage additional scene information from the viewpoint of other vehicles on the road using vehicle-to-vehicle communication technologies. We study how increased data sharing can improve the perception capabilities of automated vehicles. Our methodology shares sensor measurements and objects detected by state-of-the-art deep learning networks between vehicles to increase the automated driving system’s confidence of detecting objects using a 3D sensor fusion algorithm. This approach can benefit scenarios where an object may be occluded (fully or partially) or located too far away to classify accurately by a single vehicle alone.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst3_02">16:00-18:00, Paper ThPS-SST3.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0458.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('458'); return false" title="Click to show or hide the keywords and abstract">LoRa on the Move: Performance Evaluation of LoRa in V2X Communications</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29535" title="Click to go to the Author Index">Li, Yuke</a></td><td class="r">Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28116" title="Click to go to the Author Index">Han, Shuangshuang</a></td><td class="r">Inst. of Automation, Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37037" title="Click to go to the Author Index">Yang, LinYao</a></td><td class="r">Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10337" title="Click to go to the Author Index">Wang, Fei-Yue</a></td><td class="r">Chinese Acad. of Sciences</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37199" title="Click to go to the Author Index">Zhang, Hui</a></td><td class="r">Computer Science Department, the School of Computer Science, Car</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab458" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> Recent years have witnessed much interest in Low Power Wide Area (LPWA) technologies, which are gaining unprecedented momentum and commercial interest towards the realisation of the Internet of Things (IoT). Long Range (LoRa), as a representative LPWA technology, has the potential to satisfy the growing demand for the longer range and larger amount connectivities in vehicular communication networks. In this paper, LoRa is firstly applied into two typical vehicular networks, namely Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V), and performance of LoRa schemes with different parameter configurations are evaluated and compared. Further, Monte Carlo simulations indicate that the schemes equipped with higher bandwidth or lower spreading factor exhibit significant advantages in combating the fast fading caused by Doppler effect in V2X networks.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst3_03">16:00-18:00, Paper ThPS-SST3.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0215.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('215'); return false" title="Click to show or hide the keywords and abstract">Enhancing the Performance of Vehicle-To-Vehicle Realtime Video Streaming for Platoons</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36545" title="Click to go to the Author Index">Kuk, Seungho</a></td><td class="r">Korea Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31035" title="Click to go to the Author Index">Kim, Hyogon</a></td><td class="r">Korea Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31036" title="Click to go to the Author Index">Park, Yongtae</a></td><td class="r">Korea Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> Platooning is nowadays one of the most promising applications for vehicle-to-vehicle communications. Platooning makes multiple vehicles travel as closely as possible to get the maximum driving efficiency. The problem here is that the view of the rear vehicle driver may be blocked by the front vehicle. In order to improve rear driver’s safety and psychological stability, video streaming among vehicles aims to provide the front view of the platoon leader to rear vehicle drivers. In this paper, we first focus on measurement of realtime video streaming performance using IEEE 802.11p broadcast in the platoon. As a result, packet delivery ratio dropped to 48 % in the worst case that the collision and hidden terminal problem coexist. To mitigate the frame loss, we use pseudo-broadcast that can recover frames by retransmission instead of broadcast. In addition, we employ Request-To-Send/Clear-To-Send whose transmission power increased by 5 dB to solve the hidden terminal problem. Consequently, we can prevent the frame loss and increase the packet delivery ratio up to 96.8 %. Our proposal enables vehicle-to-vehicle communications to exploit a new method integrating characteristics of unicast and broadcast to solve this problem. Besides, our proposal can easily apply to existing devices by updating a device driver without any hardware chip level modification.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst3_04">16:00-18:00, Paper ThPS-SST3.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0257.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('257'); return false" title="Click to show or hide the keywords and abstract">Cooperative Driving Based on Negotiation with Persuasion and Concession</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36404" title="Click to go to the Author Index">Peng, Cheng</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab257" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> Recently, along with emergence of autonomous driving vehicles, it is predicted that in the near future, human drivers need to share road and interact with self-driving cars in all the possible traffic scenarios. In this paper, an algorithm based on negotiation with both persuasion and concession is proposed to tackle the challenging task of cooperative driving involving both human and robot drivers. The decision making process is formulated as an optimization based negotiation problem. The persuasion of autonomous vehicle is achieved by making commitment to tipping towards cooperation in the form of convex constraint. The concession is accomplished by gradually tuning weights in the objective function. We propose an approach suitable for most common driving scenarios including ramp merging, lane keeping/changing and intersection crossing. The effectiveness of the proposed algorithm is demonstrated by simulation for several different driving scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst3_05">16:00-18:00, Paper ThPS-SST3.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0324.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('324'); return false" title="Click to show or hide the keywords and abstract">Pulse Synchronization for Vehicular Networks</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36373" title="Click to go to the Author Index">Han, Cheng-Yu</a></td><td class="r">Univ. Paris Sud</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36375" title="Click to go to the Author Index">Nowak, Thomas</a></td><td class="r">Univ. Paris-Sud</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11745" title="Click to go to the Author Index">Lambert, Alain</a></td><td class="r">Univ. Paris Sud</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab324" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a>, <a href="IV2018_KeywordIndexMedia.html#Telematics" title="Click to go to the Keyword Index">Telematics</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> This paper improves pulse-coupled synchronization for highly dynamic wireless networks, where vehicles may move unpredictably, causing topological changes of the network. For pulse-coupled methods, vehicles broadcast zero-bit pulses to estimate the clock differences to their neighbors. Vehicles then update local clocks according to the average of the difference of pulses from its neighbors. The proposed algorithm further introduces (1) a time wheel to improve the robustness of the synchronization protocol and (2) an additional drift compensation mechanism to reduce clock skew. We compare our new algorithm to previous works via simulation. Both static and dynamic networks are simulated and compared. Different frequencies and clock drifts are analyzed. The proposed algorithm adapts to highly dynamic vehicle networks more quickly and more robustly than previous algorithms.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst3_06">16:00-18:00, Paper ThPS-SST3.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0622.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('622'); return false" title="Click to show or hide the keywords and abstract">Integration Challenges of Facilities-Layer DCC for Heterogeneous V2X Services</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37514" title="Click to go to the Author Index">Khan, Mohammad Irfan</a></td><td class="r">Eurecom</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32285" title="Click to go to the Author Index">Haerri, Jerome</a></td><td class="r">EURECOM</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab622" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> Decentralized Congestion Control (DCC) for 802.11p based inter-vehicular communication (V2X) is a critical mechanism for distributed wireless resource allocation of future connected intelligent vehicles. Studies so far mostly focused on optimizing resources for a single Cooperative Awareness service, whereas future connected intelligent vehicles will be based on multiple heterogeneous new V2X services. In this paper, we present a Facilities-layer DCC, currently being standardized in Europe, capable of handling heterogeneous V2X services and evaluate its integration impact with legacy DCC mechanisms. We first emphasize significant wireless resource under-utilization and application performance degradation stemming from conflicting decisions between legacy and Facilities-layer DCC. We then show the capability of the DCC mechanism purely based at service layer and illustrate its flexibility for agile wireless resource allocation between V2X services for intelligent vehicles.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst4"><b>ThPS-SST4</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst4" title="Click to go to the Program at a Glance"><b>Motion Recognition, Planning and Routing</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_01">16:00-18:00, Paper ThPS-SST4.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0524.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('524'); return false" title="Click to show or hide the keywords and abstract">Learning a Deep Motion Planning Model for Autonomous Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37414" title="Click to go to the Author Index">Song, Sheng</a></td><td class="r">Hubei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37412" title="Click to go to the Author Index">Hu, Xuemin</a></td><td class="r">Hubei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37415" title="Click to go to the Author Index">Yu, Jin</a></td><td class="r">Hubei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37416" title="Click to go to the Author Index">Bai, Liyun</a></td><td class="r">Hubei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18409" title="Click to go to the Author Index">Chen, Long</a></td><td class="r">Sun Yat-Sen Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab524" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> To deal with the issue of computational complexity and robustness of traditional motion planning methods for autonomous driving, an end-to-end motion planning model based on a deep cascaded neural network is proposed in this paper. The model can directly predict the driving parameters from the input sequence images. We combine two classical deep learning models including the convolution neural network (CNN) and the long short-term memory (LSTM) which are used to extract spatial and temporary features of the input images, respectively. The proposed model can fit the nonlinear relationship between the input sequence images and the output motion parameters for making the end-to-end planning. The experiments are conducted using the data collected from a driving simulator. Experimental results show that the proposed method can efficiently learn humans’ driving behaviors, adapt to different roads, and has a better robustness performance than some existing methods.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_02">16:00-18:00, Paper ThPS-SST4.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0592.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('592'); return false" title="Click to show or hide the keywords and abstract">Gaussian Process Based Motion Pattern Recognition with Sequential Local Models</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37524" title="Click to go to the Author Index">Tiger, Mattias</a></td><td class="r">Linköping Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37549" title="Click to go to the Author Index">Heintz, Fredrik</a></td><td class="r">Linköping Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab592" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Conventional trajectory-based vehicular traffic analysis approaches work well in simple environments such as a single crossing but they do not scale to more structurally complex environments such as networks of interconnected crossings (e.g. urban road networks). Local trajectory models are necessary to cope with the multi-modality of such structures, which in turn introduces new challenges. These larger and more complex environments increase the occurrences of non-consistent lack of motion and self-overlaps in observed trajectories which impose further challenges. In this paper we consider the problem of motion pattern recognition in the setting of sequential local motion pattern models. That is, classifying sub-trajectories from observed trajectories in accordance with which motion pattern that best explains it. We introduce a Gaussian process (GP) based modeling approach which outperforms the state-of-the-art GP based motion pattern approaches at this task. We investigate the impact of varying local model overlap and the length of the observed trajectory trace on the classification quality. We further show that introducing a pre-processing step filtering out stops from the training data significantly improves the classification performance. The approach is evaluated using real GPS position data from city buses driving in urban areas for extended periods of time.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_03">16:00-18:00, Paper ThPS-SST4.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0620.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('620'); return false" title="Click to show or hide the keywords and abstract">Probabilistic Prediction from Planning Perspective: Problem Formulation, Representation Simplification and Evaluation Metric</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33408" title="Click to go to the Author Index">Zhan, Wei</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13613" title="Click to go to the Author Index">de La Fortelle, Arnaud</a></td><td class="r">MINES ParisTech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35453" title="Click to go to the Author Index">Chen, Yi-Ting</a></td><td class="r">Honda Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10376" title="Click to go to the Author Index">Chan, Ching-Yao</a></td><td class="r">ITS, Univ. of California at Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab620" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> Accurate probabilistic prediction for intention and motion of road users is a key prerequisite to achieve safe and high-quality decision-making and motion planning for autonomous driving. Typically, the performance of probabilistic predictions was only evaluated by learning metrics for approximation to the motion distribution in the dataset. However, as a module supporting decision and planning, probabilistic prediction should also be evaluated from decision and planning perspective. Moreover, the evaluation of probabilistic prediction highly relies on the problem formulation variation and motion representation simplification, which lacks a formal foundation in a comprehensive framework. To address such concerns, we provide a systematic and unified framework for the analysis of three under-explored aspects of probabilistic prediction: problem formulation, representation simplification and evaluation metric. More importantly, we address the omitted but crucial problems in the three aspects from decision and planning perspective. In addition to a review of learning metrics, metrics to be considered from planning perspective are highlighted, such as planning consequence of inaccurate and erroneous prediction, as well as violations of predicted motions to planning constraints. We address practical formulation variations of prediction problems, such as decision-maker view and blind view for viewpoint, as well as reactive prediction for interaction, so that decision and planning can be facilitated.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_04">16:00-18:00, Paper ThPS-SST4.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0357.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('357'); return false" title="Click to show or hide the keywords and abstract">Value Sensitive Design for Autonomous Vehicle Motion Planning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35845" title="Click to go to the Author Index">Thornton, Sarah</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36897" title="Click to go to the Author Index">Lewis, Francis E.</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36900" title="Click to go to the Author Index">Zhang, Vivian</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18816" title="Click to go to the Author Index">Kochenderfer, Mykel</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12039" title="Click to go to the Author Index">Gerdes, J Christian</a></td><td class="r">Stanford Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab357" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Human drivers navigate the roadways by balancing values such as safety, legality, and mobility. The public will likely judge an autonomous vehicle by similar values. The iterative methodology of value sensitive design formalizes the connection of human values to engineering specifications. We apply a modified value sensitive design methodology to the development of an autonomous vehicle speed control algorithm to safely navigate an occluded pedestrian crosswalk. The first iteration presented here models the problem as a partially observable Markov decision process and uses dynamic programming to compute an optimal policy to control the longitudinal acceleration of the vehicle based on the belief of a pedestrian crossing. The speed control algorithm is then tested in real-time on an experimental vehicle on a closed road course.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_05">16:00-18:00, Paper ThPS-SST4.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0116.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('116'); return false" title="Click to show or hide the keywords and abstract">The Mathematical Modeling of the Two-Echelon Ground Vehicle and Its Mounted Unmanned Aerial Vehicle Cooperated Routing Problem</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35874" title="Click to go to the Author Index">Luo, Zhi Hao</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29370" title="Click to go to the Author Index">Liu, Zhong</a></td><td class="r">Science and Tech. on Information Systems Engineering Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29368" title="Click to go to the Author Index">Shi, Jianmai</a></td><td class="r">Science and Tech. on Information Systems Engineering Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40557" title="Click to go to the Author Index">Wang, Qi</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36223" title="Click to go to the Author Index">Liu, Yao</a></td><td class="r">National Univ. of Defense Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#38618" title="Click to go to the Author Index">Zhou, Tianren</a></td><td class="r">National Univ. of Defence Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab116" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Intelligent_Ground__Air_and_Space_Vehicles" title="Click to go to the Keyword Index">Intelligent Ground, Air and Space Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> In this paper, we presents a novel Two-Echelon Ground Vehicle and Its Mounted Unmanned Aerial Vehicle Cooperated Routing Problem (2E-GUCRP), which consists of optimizing the route of both ground vehicle (GV) and its mounted Unmanned Aerial Vehicle(UAV) in the context of Intelligence, Surveillance and Reconnaissance(ISR) mission. The UAV is launched from the ground vehicle and automatically flies to the designated target to accomplish the ISR mission. Meanwhile, the ground vehicle is synchronized to charge or change the UAV’s battery on the designated landing points based on the UAV’s battery life. The objective is to design efficient ground vehicle and UAV routes to minimize the total mission time while meeting the operational constraints. The experimental results show that the model proposed in this paper is correct, but the existing commercial software cannot solve the large-scale problem with an acceptable time.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_06">16:00-18:00, Paper ThPS-SST4.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0613.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('613'); return false" title="Click to show or hide the keywords and abstract">Limited Visibility and Uncertainty Aware Motion Planning for Automated Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30515" title="Click to go to the Author Index">Tas, Omer Sahin</a></td><td class="r">FZI Res. Center for Information Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10216" title="Click to go to the Author Index">Stiller, Christoph</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab613" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> Adverse weather conditions and occlusions in urban environments result in impaired perception. The uncertainties are handled in different modules of an automated vehicle, ranging from sensor level over situation prediction until motion planning. This paper focuses on motion planning given an uncertain environment model with occlusions. We present a method to remain collision-free for the worst-case evolution of the given scene. We define criteria that measures the available margins to a collision while considering visibility and interactions and consequently integrate conditions that apply these criteria into an optimization-based motion planner. We show the generality our method by validating it in several distinct urban scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_07">16:00-18:00, Paper ThPS-SST4.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0640.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('640'); return false" title="Click to show or hide the keywords and abstract">Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver Based LSTMs</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33590" title="Click to go to the Author Index">Deo, Nachiket</a></td><td class="r">Univ. of California San Diego</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10332" title="Click to go to the Author Index">Trivedi, Mohan M.</a></td><td class="r">Univ. of California at San Diego</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab640" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a></span><br>
                              <strong>Abstract:</strong> To safely and efficiently navigate through complex traffic scenarios, autonomous vehicles need to have the ability to predict the future motion of surrounding vehicles. Multiple interacting agents, the multi-modal nature of driver behavior, and the inherent uncertainty involved in the task make motion prediction of surrounding vehicles a challenging problem. In this paper, we present an LSTM model for interaction aware motion prediction of surrounding vehicles on freeways. Our model assigns confidence values to maneuvers being performed by vehicles and outputs a multi-modal distribution over future motion based on them. We compare our approach with the prior art for vehicle motion prediction on the publicly available NGSIM US-101 and I-80 datasets. Our results show an improvement in terms of RMS values of prediction error. We also present an ablative analysis of the components of our proposed model and analyze the predictions made by the model in complex traffic scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_08">16:00-18:00, Paper ThPS-SST4.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0127.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('127'); return false" title="Click to show or hide the keywords and abstract">Kinodynamic Motion Planning Using Multi-Objective Optimization</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36110" title="Click to go to the Author Index">Hart, Patrick Christopher</a></td><td class="r">Tech. Univ. of Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19222" title="Click to go to the Author Index">Knoll, Alois</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab127" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> As autonomous driving gains importance, universally applicable motion planning approaches that offer safe and comfortable rides have to be developed. Most planning methods up-to-date still struggle when dealing with dynamic environments. They require extensive parameter-fine tuning in order to generate comfortable and safe solutions and it is not known prior to optimization which set of parameters would produce the ``best&quot; solution. Therefore, we introduce a multi-objective optimization that plans a set of trajectories using several weights and targets (e.g. desired velocity or lanes). Thus, reducing the need of extensive parameter fine-tuning and increasing the planner's capabilities to handle dynamic environments. Furthermore, in order to plan multiple trajectories in real-time, a textit{smart-initialization} of the optimization problem is introduced that speeds up the multi-objective optimization further. Due to the proposed architecture that consists of a Planning-, Evaluation- and Selection-module, the planner is capable of providing a high level of comfort and safety -- even in the case of non-convergence of the optimization. The novel motion planning approach is evaluated in terms of its applicability and performance.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst4_09">16:00-18:00, Paper ThPS-SST4.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0348.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('348'); return false" title="Click to show or hide the keywords and abstract">Learning and Generalizing Motion Primitives from Driving Data for Path-Tracking Applications</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35958" title="Click to go to the Author Index">Wang, Boyang</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35867" title="Click to go to the Author Index">Li, Zirui</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15879" title="Click to go to the Author Index">Gong, Jianwei</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36888" title="Click to go to the Author Index">Liu, Yidi</a></td><td class="r">SAIC Motor Corp. Limited</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15898" title="Click to go to the Author Index">Chen, Huiyan</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25641" title="Click to go to the Author Index">Lu, Chao</a></td><td class="r">Beijing Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab348" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Considering the driving habits which are learned from the naturalistic driving data in the path-tracking system can significantly improve the acceptance of intelligent vehicles. Therefore, the goal of this paper is to generate the prediction results of lateral commands with confidence regions according to the reference based on the learned motion primitives. We present a two-level structure for learning and generalizing motion primitives through demonstrations. The lower-level motion primitives are generated under the path segmentation and clustering layer in the upper-level. The Gaussian Mixture Model (GMM) is utilized to represent the primitives and Gaussian Mixture Regression (GMR) is selected to generalize the motion primitives. We show how the upper-level can help to improve the prediction accuracy and evaluate the influence of different time scales and the number of Gaussian components. The model is trained and validated by using the driving data collected from the Beijing Institute of Technology intelligent vehicle platform. Experiment results show that the proposed method can extract the motion primitives from the driving data and predict the future lateral control commands with high accuracy.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst5"><b>ThPS-SST5</b></a></td>
               <td class="r">TianHua Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst5" title="Click to go to the Program at a Glance"><b>Traffic Sign Classification</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst5_01">16:00-18:00, Paper ThPS-SST5.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0632.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('632'); return false" title="Click to show or hide the keywords and abstract">Benchmarking Deep Learning Frameworks with FPGA-Suitable Models on a Traffic Sign Dataset</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37430" title="Click to go to the Author Index">Lin, Zhongyi</a></td><td class="r">Univ. of California Davis</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18416" title="Click to go to the Author Index">Ota, Jeffrey</a></td><td class="r">Intel</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21004" title="Click to go to the Author Index">Owens, John</a></td><td class="r">Univ. of California, Davis</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21003" title="Click to go to the Author Index">Muyan-Ozcelik, Pinar</a></td><td class="r">California State Univ. Sacramento</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab632" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> We benchmark several widely used deep-learning frameworks for performing deep-learning-related automotive tasks (e.g., traffic sign recognition) that need to achieve real-time and high accuracy results with limited resources available on embedded platforms such as FPGAs. In our benchmarks, we use various input image sizes on models that are suitable for FPGA deployment, and investigate the training speed and inference accuracy of selected frameworks for these different sizes on a popular traffic sign recognition dataset. We report results by running the frameworks solely on the CPU as well as by turning on GPU acceleration. We also provide optimizations we apply to fine-tune the performance of the frameworks. We discover that Neon and MXNet deliver the best training speed and inference accuracy in general for all our test cases, while Tensorflow is always among the frameworks with the highest inference accuracies. We also observe that on the particular dataset we tested on (i.e., GTSRB), the image size of the region of interest does not necessarily affect the inference accuracy, and that using deep models, e.g., ResNet-32, which have longer training times, might not provide improvements to inference accuracy.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst5_02">16:00-18:00, Paper ThPS-SST5.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0671.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('671'); return false" title="Click to show or hide the keywords and abstract">Application Technology Investigation of Traffic Signal Controllers Which Output DC Signal</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37979" title="Click to go to the Author Index">Haifeng, Lu</a></td><td class="r">Traffic Management Res. Inst. of the Ministry of Public</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37980" title="Click to go to the Author Index">Junhua, Wang</a></td><td class="r">Traffic Management Res. Inst. of the Ministry of Public</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37981" title="Click to go to the Author Index">Jun, Xu</a></td><td class="r">Traffic Management Res. Inst. of the Ministry of Public</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab671" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a></span><br>
                              <strong>Abstract:</strong> Firstly, the paper gives a brief introduction to worldwide applications of specified traffic signal controllers which output direct-current signal. Secondly, related technologies to develop such traffic signal controllers as well as the problems that need attention are addressed in detail. Thirdly, several real benefits of using DC traffic signal controllers are given. Finally, the prospects of direct-current traffic signal controllers are analyzed and concluded.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst5_03">16:00-18:00, Paper ThPS-SST5.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0505.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('505'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Vision-Based Pole-Like Obstacle Detection and Localization for Urban Mobile Robots</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32104" title="Click to go to the Author Index">Sabatini, Stefano</a></td><td class="r">Pol. Di Milano</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21291" title="Click to go to the Author Index">Corno, Matteo</a></td><td class="r">Pol. Di Milano</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37365" title="Click to go to the Author Index">Fiorenti, Simone</a></td><td class="r">Yape S.r.l</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12463" title="Click to go to the Author Index">Savaresi, Sergio M.</a></td><td class="r">Pol. Di Milano</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab505" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0505.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> Despite the enormous progress of the last years, urban environments still represent a challenge for robot autonomous navigation. This paper focuses on the problem of detecting street pole-like obstacles using a monocular camera. Such obstacles, due to their thin structure, may be difficult to be detected by common active sensors like lasers. This is even more critical for innovative solid state LiDARs like the one employed in this work because, at the actual state, they are characterized by very low angular resolutions. The approach described here is based on identifying poles as long vertical structures in the image and in locating them with respect to the robot using a Kalman filter based depth estimation. This information can then be fused with the information coming from LiDARs realizing a complete obstacle detection module.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst5_04">16:00-18:00, Paper ThPS-SST5.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0211.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('211'); return false" title="Click to show or hide the keywords and abstract">Adaptive Traffic Signal Control with Deep Recurrent Q-Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35957" title="Click to go to the Author Index">Zeng, Jinghong</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11444" title="Click to go to the Author Index">Hu, Jianming</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10742" title="Click to go to the Author Index">Zhang, Yi</a></td><td class="r">Tsinghua Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab211" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a>, <a href="IV2018_KeywordIndexMedia.html#Smart_Infrastructure" title="Click to go to the Keyword Index">Smart Infrastructure</a></span><br>
                              <strong>Abstract:</strong> The application of modern technologies makes it possible for a transportation system to collect real-time data of some specific traffic scenes, helping traffic control center to improve the traffic efficiency. Based on such consideration, we introduce a variant deep reinforcement learning agent that might take advantage of the real-time GPS data and learn how to control the traffic lights in an isolated intersection. We combine the recurrent neural network (RNN) with Deep Q-Network, namely DRQN and compare its performance with standard Deep Q-Network (DQN) in partially observed traffic situations. The agent is trained by using Q-learning with experience replay in traffic simulator SUMO, so as to generate traffic signal control policy. Based on the experiments, both DQN and DRQN method are able to adjust its traffic signal timing policy to specific traffic environment and achieve lower average vehicle delay than fixed time control. In addition, the recurrent Q-learning method gets better simulation result than standard Q-learning method in the environment of different probe vehicle proportion.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst6"><b>ThPS-SST6</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst6" title="Click to go to the Program at a Glance"><b>Reinforcement Learning for Vehicles</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst6_01">16:00-18:00, Paper ThPS-SST6.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0548.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('548'); return false" title="Click to show or hide the keywords and abstract">A Game-Theoretical Approach to Driving Decision Making in Highway Scenarios</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36288" title="Click to go to the Author Index">Yan, Zhihai</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35200" title="Click to go to the Author Index">Wang, Jun</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31245" title="Click to go to the Author Index">Zhang, Yihuan</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab548" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> With the development of self-driving technology, the fundamental behaviors like car-following, lane change have been validated and tested in various kinds of scenarios. Currently one of the most challenging domain for self-driving is decision making under dynamic environments. For self- driving cars, it is essential to understand and estimate other vehicles’ behavior and behave like a human driver to interact with other vehicles in the mean time. In this paper, a game theoretical approach is proposed to model the interaction of vehicles while considering the surrounding traffic situations. One of the novel move is that a neural network is applied to establish the payoff function in the game which is able to describe the interaction more precisely. The calibration method is then applied to estimate the parameters by using the Next Generation SIMulation (NGSIM) dataset. The experiments demonstrate the accuracy of the proposed method and the ability of making a cooperate decision in highway scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst6_02">16:00-18:00, Paper ThPS-SST6.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0562.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('562'); return false" title="Click to show or hide the keywords and abstract">Highway Traffic Modeling and Decision Making for Autonomous Vehicle Using Reinforcement Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37484" title="Click to go to the Author Index">You, Changxi</a></td><td class="r">Gatech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20319" title="Click to go to the Author Index">Lu, Jianbo</a></td><td class="r">Ford Motor Company</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19130" title="Click to go to the Author Index">Filev, Dimitar</a></td><td class="r">Ford Res. & Advanced Engineering</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21316" title="Click to go to the Author Index">Tsiotras, Panagiotis</a></td><td class="r">Georgia Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab562" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> This paper studies the decision making problem of autonomous vehicles in traffic. We model the interaction between an autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an experienced driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. By designing the reward function of the MDP, the desired, driving behavior of the autonomous vehicle is obtained using reinforcement learning. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst6_03">16:00-18:00, Paper ThPS-SST6.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0599.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('599'); return false" title="Click to show or hide the keywords and abstract">Automatically Generated Curriculum Based Reinforcement Learning for Autonomous Vehicles in Urban Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35861" title="Click to go to the Author Index">Qiao, Zhiqian</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37563" title="Click to go to the Author Index">Muelling, Katharina</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15953" title="Click to go to the Author Index">Dolan, John</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37562" title="Click to go to the Author Index">Palanisamy, Praveen</a></td><td class="r">General Motors</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20643" title="Click to go to the Author Index">Mudalige, Priyantha</a></td><td class="r">General Motors</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab599" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> We address the problem of learning autonomous driving behaviors in urban intersections using deep reinforcement learning (DRL). DRL has become a popular choice for creating autonomous agents due to its success in various tasks. However, as the problems tackled become more complex, the number of training iterations necessary increase drastically. Curriculum learning has been shown to reduce the required training time and improve the performance of the agent, but creating an optimal curriculum often requires human handcrafting. In this work, we learn a policy for urban intersection crossing using DRL and introduce a method to automatically generate the curriculum for the training process from a candidate set of tasks. We compare the performance of the automatically generated curriculum (AGC) training to those of randomly generated sequences and show that AGC can significantly reduce the training time while achieving similar or better performance.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst6_04">16:00-18:00, Paper ThPS-SST6.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0301.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('301'); return false" title="Click to show or hide the keywords and abstract">Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35170" title="Click to go to the Author Index">Chen, Jianyu</a></td><td class="r">UC Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36778" title="Click to go to the Author Index">Wang, Zining</a></td><td class="r">Univ. of California Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12030" title="Click to go to the Author Index">Tomizuka, Masayoshi</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab301" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Deep reinforcement learning has achieved great progress recently in domains such as learning to play Atari games from raw pixel input. The model-free characteristics of reinforcement learning free us from hand-encoding complex policies. However, for real world tasks such as autonomous driving, there are some complex sequential decision making processes that contain distinct behaviors. Due to the delayed rewards and the averaged gradient, it is pretty difficult for a flat deep reinforcement learning algorithm to learn a good policy. <p>In this paper, we design a hierarchical neural network policy and propose a hierarchical policy gradient method to train the network with the semi markov decision process (SMDP) temporal abstraction formulation. We apply this method to a traffic light passing scenario in autonomous driving, where the vehicle has two distinct behaviors (e.g., pass and stop) and its primitive actions (e.g., acceleration) should follow the corresponding behavior. We show via simulation that our method is able to select correct decision and acts appropriately when the traffic light turns yellow. On the contrary, the flat reinforcement learning algorithm is not able to achieve a good performance and exhibits a large variance. Furthermore, the trained neural network modules are reusable in the future to cover more scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst6_05">16:00-18:00, Paper ThPS-SST6.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0110.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('110'); return false" title="Click to show or hide the keywords and abstract">Inverse Reinforcement Learning Via Neural Network in Driver Behavior Modeling</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36200" title="Click to go to the Author Index">Zou, Qijie</a></td><td class="r">Dalian Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36197" title="Click to go to the Author Index">Li, HaoYu</a></td><td class="r">Dalian Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40374" title="Click to go to the Author Index">Zhang, RuBo</a></td><td class="r">Dalian Minzu Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab110" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Reinforcement Learning(IRL) is formulated within the framework of Markov decision process(MDP) where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. Then the expert as trying to maximize a reward function that is expressible as a linear combination of known features specifying the reward function. However, in autonomous driving tasks, due to the difference of scene factor, such as obstacle and weather, the state spaces are frequently large and demonstrations can hardly visit all the states. In this paper, we focus on driving behavior modeling with IRL method which introduce the convolutional neural network to extract the associated state feature automatically, and express the policy by neural network to generalize the expert’s behaviors. Experimental results on simulated vehicle show that: compared with the traditional end-to-end method, the accuracy of decision-making greatly improved, and in the new curve scene with a large number of unvisited state, this method completes the task compared to the traditional end-to-end method, the time required is also obviously reduced, which shows a perfect generalization efficiency.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst6_06">16:00-18:00, Paper ThPS-SST6.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0437.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('437'); return false" title="Click to show or hide the keywords and abstract">Human-Like Autonomous Vehicle Speed Control by Deep Reinforcement Learning with Double Q-Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37117" title="Click to go to the Author Index">Zhang, Yi</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36777" title="Click to go to the Author Index">Sun, Ping</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37118" title="Click to go to the Author Index">Yin, Yuhan</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37119" title="Click to go to the Author Index">Lin, Lin</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36774" title="Click to go to the Author Index">Wang, Xuesong</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab437" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Autonomous driving has become a popular research project. How to control vehicle speed is a core problem in autonomous driving. Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to control the vehicle speed. However, the popular Q-learning algorithm is unstable in some games in the Atari 2600 domain. In this paper, a reinforcement learning approach called Double Q-learning is used to control a vehicle’s speed based on the environment constructed by naturalistic driving data. Depending on the concept of the direct perception approach, we propose a new method called integrated perception approach to construct the environment. The input of the model is made up of high dimensional data including road information processed from the video data and the low dimensional data processed from the sensors. During experiment, compared with deep Q-learning algorithm, double deep Q-learning has improvements both in terms of value accuracy and policy quality. Our model’s score is 271.73% times that of deep Q-learning.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst7"><b>ThPS-SST7</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst7" title="Click to go to the Program at a Glance"><b>Pedestrian Motion and Intention Classification</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_01">16:00-18:00, Paper ThPS-SST7.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0262.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('262'); return false" title="Click to show or hide the keywords and abstract">Multi-Feature Fusion Based Region of Interest Generation Method for Far-Infrared Pedestrian Detection System</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27211" title="Click to go to the Author Index">Wang, Zhiling</a></td><td class="r">Hefei Inst. of Physical Science&#65292; Chinese Acad. of Sc</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36662" title="Click to go to the Author Index">Lin, Linglong</a></td><td class="r">Inst. of Applied Tech. Hefei Inst. of Physical Sci</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40568" title="Click to go to the Author Index">Li, Yuxin</a></td><td class="r">Hefei Inst. of Physical Science&#65292; Chinese Acad. of Sc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab262" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> Given the uncontrolled outdoor environments and different physical properties of clothing, the appearance of pedestrians in far-infrared (FIR) images changes dramatically. Finding a robust region of interest (ROI) generation method for pedestrian detection remains challenging. Previous researches can obtain reliable results in some conditions. But they always got inappropriate results in warmer conditions. This study presents a Multi-feature fusion based ROI generation method for FIR pedestrian detection system to solve this problem. We extract two kinds of salient feature regions, namely, highlighting feature areas and vertical feature areas. A reasonable threshold is set to derive the highlighting feature areas and Scharr operator is used to find vertical edges. These areas are not necessarily connected to each other in the image. We think an upright pedestrian is a highly structured target consisting of highlighting feature areas and vertical feature areas. The distribution of feature areas is demonstrated using a skeleton model. So we apply the dilating morphological operation to ensure that these adjacent feature areas within pedestrians’ regions will connect together. The size of the structuring element is set adaptively according to the size of feature areas. Finally, the experimental results show the robust performance of our method in different ambient conditions.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_02">16:00-18:00, Paper ThPS-SST7.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0088.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('88'); return false" title="Click to show or hide the keywords and abstract">Pedestrian Classification for 79 GHz Automotive Radar Systems</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36117" title="Click to go to the Author Index">Prophet, Robert</a></td><td class="r">FAU Erlangen-Nuremberg</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36118" title="Click to go to the Author Index">Hoffmann, Marcel</a></td><td class="r">FAU Erlangen-Nuremberg, Inst. of Microwaves and Photonics (L</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36119" title="Click to go to the Author Index">Ossowska, Alicja</a></td><td class="r">Active Safety Product Line, VALEO Schalter Und Sensoren GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36121" title="Click to go to the Author Index">Malik, Waqas</a></td><td class="r">Active Safety Product Line, VALEO Schalter Und Sensoren GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36122" title="Click to go to the Author Index">Sturm, Christian</a></td><td class="r">Active Safety Product Line, VALEO Schalter Und Sensoren GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36123" title="Click to go to the Author Index">Vossiek, Martin</a></td><td class="r">FAU Erlangen-Nuremberg, Inst. of Microwaves and Photonics (L</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab88" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Radar_Sensing_and_Perception" title="Click to go to the Keyword Index">Radar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> Radar sensors have become an integral part of advanced driver assistance systems. However, to increase their contribution, a mere target detection is not sufficient. Instead, a classification is required to distinguish vulnerable road users from other objects such as vehicles. To achieve this, targets that are determined from the Range-Doppler-Matrix created by a 79 GHz chirp sequence radar are clustered to objects. Different classifiers then use previously calculated characteristic features of moving objects to generate the object classes “Pedestrian” and “Other”. As a result, the success rate of one measurement reaches up to 95.3 % for well-suited classifiers and a bandwidth of 1.6 GHz. Moreover, the robustness of the classification process is increased by tracking the objects. The proposed algorithm for pedestrian classification is not only faster than conventional approaches using micro-Doppler signatures, but also requires less computational effort. Implemented in vehicles, this can be a major contribution to protect vulnerable road users such as pedestrians.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_03">16:00-18:00, Paper ThPS-SST7.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0238.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('238'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Is the Pedestrian Going to Cross? Answering by 2D Pose Estimation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36124" title="Click to go to the Author Index">Fang, Zhijie</a></td><td class="r">Univ. Autònoma De Barcelona</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#11454" title="Click to go to the Author Index">López, Antonio M.</a></td><td class="r">Univ. Autònoma De Barcelona</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab238" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0238.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> Our recent work suggests that, thanks to nowadays powerful CNNs, image-based 2D pose estimation is a promising cue for determining pedestrian intentions such as crossing the road in the path of the ego-vehicle, stopping before entering the road, and starting to walk or bending towards the road. This statement is based on the results obtained on non-naturalistic sequences (Daimler dataset), in sequences choreographed specifically for performing the study. Fortunately, a new publicly available dataset (JAAD) has appeared recently to allow developing methods for detecting pedestrian intentions in naturalistic driving conditions; more specifically, for addressing the relevant question is the pedestrian going to cross? Accordingly, in this paper we use JAAD to assess the usefulness of 2D pose estimation for answering such a question. We combine CNN-based pedestrian detection, tracking and pose estimation to predict the crossing action from monocular images. Overall, the proposed pipeline provides new state-of-the-art results.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_04">16:00-18:00, Paper ThPS-SST7.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0507.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('507'); return false" title="Click to show or hide the keywords and abstract">Learning to Forecast Pedestrian Intention from Pose Dynamics</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36009" title="Click to go to the Author Index">Ghori, Omair</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36021" title="Click to go to the Author Index">Mackowiak, Radek Jakob</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36164" title="Click to go to the Author Index">Bautista, Miguel</a></td><td class="r">Heidelberg Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25444" title="Click to go to the Author Index">Beuter, Niklas</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36011" title="Click to go to the Author Index">Rego Drumond, Lucas</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19512" title="Click to go to the Author Index">Diego, Ferran</a></td><td class="r">Bosch</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36071" title="Click to go to the Author Index">Ommer, Bjorn</a></td><td class="r">Heidelberg Univ. IWR</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab507" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a></span><br>
                              <strong>Abstract:</strong> For an autonomous car, the ability to foresee a humans action is very useful for mitigating the risk of a possible collision. To humans this pedestrian intention foresight comes naturally as they are able to recognize another person's actions just by perceiving subtle changes in posture. Approximating this intention inference ability by directly training a deep neural network is useful but especially challenging. First, sufficiently large datasets for intention recognition with frame--wise human pose and intention annotations are rare and expensive to compile. Second, training on smaller datasets can lead to overfitting and make it difficult to adapt to intra-class variations in action executions. Therefore, in this paper, we propose a real time framework that learns (i) intention recognition using weak-supervision and (ii) locomotion dynamics of intention from pose information using transfer learning. This new formulation is able to tackle the lack of frame-wise annotations and to learn intra-class variation in action executions. We empirically demonstrate that our proposed approach leads to earlier and more stable detection of intention than other state of the art approaches with real time operation and the ability to detect intention one second before the pedestrian reaches the kerb.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_05">16:00-18:00, Paper ThPS-SST7.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0497.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('497'); return false" title="Click to show or hide the keywords and abstract">Probabilistic Map-Based Pedestrian Motion Prediction Taking Traffic Participants into Consideration</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37311" title="Click to go to the Author Index">Wu, Jingyuan</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37314" title="Click to go to the Author Index">Ruenz, Johannes</a></td><td class="r">Robert Bosch GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13878" title="Click to go to the Author Index">Althoff, Matthias</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab497" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> As pedestrians are one of the most vulnerable traffic participants, their motion prediction is of utmost importance for intelligent transportation systems. Predicting motions of pedestrians is especially hard since they move in less structured environments and have less inertia compared to road vehicles. To account for this uncertainty, we present an approach for probabilistic prediction of pedestrian motion using Markov chains. In contrast to previous work, we not only consider motion models, constraints from a semantic map, and various goals, but also explicitly adapt the prediction based on crash probabilities with other traffic participants. Also, our approach works in any situation; this is typically challenging for pure machine learning techniques that learn behaviors for a particular road section and which might consequently struggle with a different road section. The usefulness of combining the aforementioned aspects in a single approach is demonstrated by an evaluation using recordings of real pedestrians.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_06">16:00-18:00, Paper ThPS-SST7.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0634.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('634'); return false" title="Click to show or hide the keywords and abstract">Skeleton Model Based Behavior Recognition for Pedestrians and Cyclists from Vehicle Scene Camera</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37609" title="Click to go to the Author Index">Deng, Qiwen</a></td><td class="r">Indiana Univ. Purdue Univ. Indianapolis</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25161" title="Click to go to the Author Index">Tian, Renran</a></td><td class="r">Indiana Univ. Univ. Indianapolis</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17557" title="Click to go to the Author Index">Chen, Yaobin</a></td><td class="r">Purdue School of Engineering and Tech. IUPUI</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37613" title="Click to go to the Author Index">Li, Kang</a></td><td class="r">Rutgers Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab634" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a></span><br>
                              <strong>Abstract:</strong> With the significant advances in computer vision research, skeleton model based human pose recognition has become more accurate and time-efficient, although most of the applications are limited in laboratory environment or on surveillance videos. This paper proposes a pose tracking and behavior recognition method from in-vehicle scene camera. It will not only detect pedestrians on the road, but also generate their skeleton models describing head, limb, and trunk movements. Based on these more detailed movements of body parts, the proposed method is designed to track poses of pedestrians and cyclists with the potentials to enable automated pedestrian gesture reading and non-verbal interactions between autonomous vehicles and pedestrians. The proposed algorithm has been tested on different databases including TASI 110-car naturalistic driving database and Joint Attention for Autonomous Driving (JAAD) database. Results show that key frames describing different pedestrian and cyclist negotiation gestures are detected from the raw video streams using the proposed method. These results will improve our understanding of pedestrian and cyclist’s intentions and can be further used for autonomous vehicle control algorithm development.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_07">16:00-18:00, Paper ThPS-SST7.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0170.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('170'); return false" title="Click to show or hide the keywords and abstract">Pedestrian Dynamic and Kinematic Information Obtained from Vision Sensors</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31077" title="Click to go to the Author Index">Gerling Konrad, Santiago</a></td><td class="r">Univ. Nacional Del Sur</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21940" title="Click to go to the Author Index">Shan, Mao</a></td><td class="r">Univ. of Sydney</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19419" title="Click to go to the Author Index">Masson, Favio</a></td><td class="r">Univ. Nacional Del Sur</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19330" title="Click to go to the Author Index">Worrall, Stewart</a></td><td class="r">Univ. of Sydney</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19135" title="Click to go to the Author Index">Nebot, Eduardo</a></td><td class="r">ACFR Univ. of Sydney</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab170" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> The estimation and prediction of pedestrian motion is of fundamental importance in ITS applications. Most existing solutions have utilized a particular type of sensor for perception such as cameras (stereo, monocular, infrared) or other modalities such as a laser or radar. The advent of wearable devices with inertial sensors have led to the development of systems capable of the robust inference of pedestrian intention. Unfortunately these devices do not have communications capabilities to broadcast this information to all vehicles in proximity, and also this strategy requires functioning devices on all pedestrians to work. This paper presents a robust perception method that is able to extract dynamic pedestrian information with accuracy comparable to typical gyros and accelerometers installed in wearable devices. Experimental results are presented to demonstrate the potential for obtaining very comprehensive dynamic information from limbs representing the skeleton of a pedestrian. This work also demonstrates the accuracy of vision based systems by comparing these results to the rotation and acceleration measured directly on the pedestrian using a wearable device. This contribution of this paper demonstrates that it is possible to significantly improve both the detection and estimation of pedestrian intention by incorporating dynamic information obtained from vision sensors.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst7_08">16:00-18:00, Paper ThPS-SST7.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0126.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('126'); return false" title="Click to show or hide the keywords and abstract">Motorcycle Inertial Parameters Identification Via Algorithmic Computation of State and Design Sensitivities</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36250" title="Click to go to the Author Index">Fouka, Majda Amina Aida</a></td><td class="r">Evry Val D'essonne Univ. (UEVE), IBISC LAB</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14106" title="Click to go to the Author Index">Nehaoua, Lamri</a></td><td class="r">IBISC Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#14111" title="Click to go to the Author Index">Arioui, Hichem</a></td><td class="r">Evry Val D'essonne Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10721" title="Click to go to the Author Index">Mammar, Said</a></td><td class="r">Univ. EVRY</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab126" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a></span><br>
                              <strong>Abstract:</strong> Recent advanced riding assistance safety systems (ARAS), such as electronic yaw stability control, adaptive cruise control, and lane-keeping systems, require good approximations of motorcycle inertial properties, such as yaw, roll and pitch moment of inertia, this parameter can vary significantly with	the rider's weight and heavy baggage placed on the luggage rack. This paper presents further research on parametric identification of two wheeler vehicles, carried out using a recursive Levenberg-Marquardt parameter identification formulation. This approach needs the use of sensitivity functions to identify acceleration responses in time domain by updating coupled inertial parameters value. The identification algorithm is implemented in MATLAB/Simulink software. Data and prior value are taken from the professional motorcycle simulation software BikeSim (based on high fidelity virtual motorcycle models).
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst8"><b>ThPS-SST8</b></a></td>
               <td class="r">RenHe Hall 1</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst8" title="Click to go to the Program at a Glance"><b>Collision Avoidance and Encounter</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_01">16:00-18:00, Paper ThPS-SST8.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0175.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('175'); return false" title="Click to show or hide the keywords and abstract">Estimating Reaction Time in Adaptive Cruise Control Systems</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36411" title="Click to go to the Author Index">Makridis, Michail</a></td><td class="r">Joint Res. Centre, European Commission</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36412" title="Click to go to the Author Index">Mattas, Konstantinos</a></td><td class="r">Joint Res. Centre, European Commission</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36413" title="Click to go to the Author Index">Borio, Daniele</a></td><td class="r">Joint Res. Centre, European Commission</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36414" title="Click to go to the Author Index">Giuliani, Raimondo</a></td><td class="r">Joint Res. Centre, European Commission</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20119" title="Click to go to the Author Index">Ciuffo, Biagio</a></td><td class="r">Joint Res. Centre - European Commission</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab175" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Vehicle automation and cooperation is progressively being introduced in traffic networks. As a consequence research to assess its impacts is currently on-going. Adaptive Cruise Control (ACC) is one of the first automated functionalities available for privately owned vehicles. An experimental study has been conducted to investigate the key features of the ACC controller using Global Navigation Satellite System data. The first remarks based on the data focus on the controller’s reaction time and desired time gap. Both parameters are essential in order to assess the influence of these technologies to safety and traffic flow. It is a common assumption that autonomous vehicles will have negligible reaction time and desired time gap comparable to that of a human driver. This paper presents an experimental study of an ACC-enabled vehicle on car following mode and a methodology for the estimation of the controller’s reaction time that can be used as benchmark in other scenarios. The results show the reaction time to be around 1.1s and the time gap to be distinctly larger than that of a human driver. This poses concern on the impact of ACC on traffic flow when a significant number of vehicles will have such systems operating on-board.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_02">16:00-18:00, Paper ThPS-SST8.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0169.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('169'); return false" title="Click to show or hide the keywords and abstract">A Virtual Reality Based Approach for Researching Pedestrian to Vehicle Collisions</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36096" title="Click to go to the Author Index">Holdgrün, Thomas</a></td><td class="r">Tech. Hochschule Ingolstadt, CARISSMA</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31826" title="Click to go to the Author Index">Doric, Igor</a></td><td class="r">Tech. Hochschule Ingolstadt</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36330" title="Click to go to the Author Index">Fuchs, Therese</a></td><td class="r">Univ. of Munich LMU</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36336" title="Click to go to the Author Index">Muehlbauer, Julia</a></td><td class="r">Univ. of Munich LMU</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36233" title="Click to go to the Author Index">Steinert, Philipp</a></td><td class="r">Univ. of Munich LMU</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36196" title="Click to go to the Author Index">Peldschus, Steffen</a></td><td class="r">Univ. of Munich LMU</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20558" title="Click to go to the Author Index">Brandmeier, Thomas</a></td><td class="r">Ingolstadt Univ. of Applied Sciences</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab169" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Passive_Safety" title="Click to go to the Keyword Index">Passive Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a></span><br>
                              <strong>Abstract:</strong> This paper presents a novel approach for the research on pedestrian to vehicle collisions in a realistic virtual environment. The reactions and crossing behavior of test subjects were examined using a pedestrian simulator, consisting of a full body wireless motion capture system and virtual reality glasses in which a virtual traffic environment is displayed. The crossing behavior of 23 participants was investigated in 9 different virtual traffic scenarios in which the velocities of the approaching vehicles, driving direction and traffic density is varied. In case of a virtual collision the body posture and impact position at the vehicle front was captured. Based on the variety of captured body postures at time of collision, typical pedestrian accident postures can be determined. The data was used as input to investigate the influence of the body posture on kinematics of the pedestrian during a car-pedestrian-impact using the Finite-Element Method (FEM). Therefore, the current standard SAE pedestrian test posture was compared to a non-standard posture which was recorded using the pedestrian simulator. In summary, the virtual reality based approach can be used for reproducible research on pedestrian to vehicle collisions, pre-collision pedestrian behavior and reactions and the optimization of pedestrian test models.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_03">16:00-18:00, Paper ThPS-SST8.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0166.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('166'); return false" title="Click to show or hide the keywords and abstract">Automatic Generation of Safety-Critical Test Scenarios for Collision Avoidance of Road Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13878" title="Click to go to the Author Index">Althoff, Matthias</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36402" title="Click to go to the Author Index">Lutz, Sebastian</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab166" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> It is apparent that one cannot rely solely on physical test drives for ensuring the correct functionality of autonomous vehicles. Since physical test drives are costly and time consuming, it is advantageous to accompany them with computer simulations. However, since most traffic scenarios are not challenging, even simulations are often too time consuming. To address this issue, we present an approach that creates automatically critical driving situations, i.e., situations with a small solution space for avoiding a collision. Our approach combines reachability analysis for determining the size of the solution space with optimization techniques to shrink it. The solution space is reduced by shifting the initial states of traffic participants, demanding an immediate and correct action of the vehicle under test. We demonstrate our approach by automatically increasing the criticality of several initially uncritical situations recorded from real traffic.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_04">16:00-18:00, Paper ThPS-SST8.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0251.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('251'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Effect of Vehicle-To-Vehicle Communication Latency on a Collision Avoidance Algorithm for Heavy Road Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36592" title="Click to go to the Author Index">Yellapantula, Venkata Ramani Shreya</a></td><td class="r">Indian Inst. of Tech. Madras</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36632" title="Click to go to the Author Index">Rao, Rakesh N</a></td><td class="r">Indian Inst. of Tech. Madras</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17703" title="Click to go to the Author Index">Subramanian, Shankar</a></td><td class="r">Indian Inst. of Tech. Madras</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab251" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0251.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Active safety is of utmost importance in heavy road vehicles due to the relatively higher number of fatalities encountered in their accidents. Vehicle-to-Vehicle (V2V) technology, which is seen as a future of connected vehicles, can potentially complement onboard sensing to reduce the time taken for detection, and to plan the path with the information available from road side units (RSU). This paper investigates the effect of latency in V2V communication on a collision avoidance algorithm developed for heavy road vehicles. Experiments performed on a Hardware-in-Loop (HiL) setup were used to evaluate the effect of latency for various scenarios. It was found that latency had a counterbalancing effect on vehicle spacing and relative longitudinal speed that led to insignificant changes in the final spacing. Further, a sensitivity analysis done at different host vehicle longitudinal speeds demonstrated the need of a variable time headway.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_05">16:00-18:00, Paper ThPS-SST8.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0320.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('320'); return false" title="Click to show or hide the keywords and abstract">A Linear Model Predictive Planning Approach for Overtaking Manoeuvres under Possible Collision Circumstances</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27680" title="Click to go to the Author Index">Lattarulo, Ray</a></td><td class="r">Tecnalia Res. and Innovation</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21657" title="Click to go to the Author Index">Hess, Daniel</a></td><td class="r">Deutsches Zentrum Für Luft Und Raumfahrt E.v</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19520" title="Click to go to the Author Index">Pérez Rastelli, Joshué</a></td><td class="r">Tecnalia</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab320" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> Overtaking is one of the most difficult tasks during driving. This manoeuvre demands good skills to accomplish it correctly. In the overtaking considering multiple vehicles (more than a couple) is necessary to understand, predict and coordinate future actions of the other participants. These reasons make it a significant scenario for testing in the connected and automated driving field, with the main goal of predicting safe future states. In this sense, this work presents an overtaking method based on a linear Model Predictive Control (MPC) approach, which considers multiple participants involved in the scenario. This method adapts dynamically the trajectory for the manoeuvre in case of unexpected situations. Some of these changes consider other vehicles coming on the opposite lane or variations on participants’ driving decisions. Additionally, the system considers passengers’ comfort, the vehicle physical constraints and lateral actions of the vehicle decoupled of the longitudinal ones to simplify the problem.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_06">16:00-18:00, Paper ThPS-SST8.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0630.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('630'); return false" title="Click to show or hide the keywords and abstract">Cooperative Collision Avoidance by Sharing Vehicular Subsystem Data</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35416" title="Click to go to the Author Index">Yavvari, Chaitanya</a></td><td class="r">GMU</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24553" title="Click to go to the Author Index">Duric, Zoran</a></td><td class="r">George Mason Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33565" title="Click to go to the Author Index">Wijesekera, Duminda</a></td><td class="r">George Mason Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab630" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Vehicular subsystems such as Anti-lock braking (ABS), Traction Control System (TCS) and Electronic Stability Program (ESP) use low-level sensory data to maintain the vehicle's stability. We show that sharing that information with neighboring vehicles could prevent potential collisions. Towards achieving this goal, we propose extensions to Dedicated Short Range Communication (DSRC)'s Basic Safety Message (BSM) set and propose a controller which can use the transmitted information to avoid collisions with vehicles experiencing instabilities. We show the utility of our methodology by using 3 common lane change scenarios under varying road surface conditions. Finally, we demonstrate some limitations of our methodology using similar scenarios.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_07">16:00-18:00, Paper ThPS-SST8.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0449.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('449'); return false" title="Click to show or hide the keywords and abstract">Cluster Naturalistic Driving Encounters Using Deep Unsupervised Learning</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37147" title="Click to go to the Author Index">Li, Sisi</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34803" title="Click to go to the Author Index">Wang, Wenshuo</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37159" title="Click to go to the Author Index">Mo, Zhaobin</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32861" title="Click to go to the Author Index">Zhao, Ding</a></td><td class="r">Univ. of Michigan, Ann Arbor</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab449" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> Learning knowledge from driving encounters could help self-driving cars make appropriate decisions when driving in complex settings with nearby vehicles engaged. This paper develops an unsupervised classifier to group naturalistic driving encounters into distinguishable clusters by combining an auto-encoder with k-means clustering (AE-kMC). The effectiveness of AE-kMC was validated using the data of 10,000 naturalistic driving encounters which were collected by the University of Michigan, Ann Arbor in the past five years. We compare our developed method with the k-means clustering methods and experimental results demonstrate that the AE-kMC method outperforms the original k-means clustering method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_08">16:00-18:00, Paper ThPS-SST8.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0365.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('365'); return false" title="Click to show or hide the keywords and abstract">Dynamic Space-Time Resource Allocation for Signal-Less Intersection Management in a Connected Autonomous Vehicle Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35966" title="Click to go to the Author Index">Wang, Nannan</a></td><td class="r">Fujitsu Labs of America</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36892" title="Click to go to the Author Index">Wang, Xi</a></td><td class="r">Fujitsu Labs of America</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36891" title="Click to go to the Author Index">Palacharla, Paparao</a></td><td class="r">Fujitsu Labs of America</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36914" title="Click to go to the Author Index">Ikeuchi, Tadashi</a></td><td class="r">Fujitsu Lab. of America, Inc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab365" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> In this paper, we consider the problem of dynamic space-time resource allocation for optimizing the movements of connected autonomous vehicles (CAVs) through intersections without traffic signals. We design a three-dimensional (3D) space-time resource model for maintaining the intersection resource information in both the two-dimensional (2D) space domain and the time domain. In the 3D resource model, the trajectory of a CAV through an intersection is assigned a specific parallelepiped resource that spans both 2D space and time domains. Moreover, the dynamic space-time resource allocation problem is simplified to a classic 3D container-packing problem. We propose a dynamic heuristic algorithm, Best Parallelepiped Fit (BPF), to maintain smooth traffic flow and maximize space-time resource usage by adjusting the speed and entry time of each approaching CAV through intersections. We evaluate the performance of the proposed algorithm under different traffic loads, and simulation results indicate that our algorithm can greatly reduce the average travel delay and fuel consumption of CAVs.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst8_09">16:00-18:00, Paper ThPS-SST8.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0538.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('538'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Adapting the Virtual Platooning Concept to Roundabout Crossing</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36030" title="Click to go to the Author Index">Masi, Stefano</a></td><td class="r">Univ. De Tech. De Compiègne</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34251" title="Click to go to the Author Index">Xu, Philippe</a></td><td class="r">Univ. of Tech. of Compiegne</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12522" title="Click to go to the Author Index">Bonnifait, Philippe</a></td><td class="r">Univ. of Tech. of Compiegne</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab538" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0538.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Despite the tremendous growth of research regarding fully autonomous vehicles in the past few years, many safety critical scenarios, such as the crossing of roundabouts, are still open issues. One of the main challenge is to deal with the lack of visibility in complex environments. Vehicle-to-vehicle and vehicle-to-infrastructure communications offer an appealing solution to handle these situations in a cooperative way. To avoid computing an ad hoc control strategy for every possible scenario, we propose in this paper to adapt the concept of virtual platooning to roundabout crossing. This idea allows a single platooning control law to handle complex scenarios such as intersection and roundabout crossings. This work combines the use of high definition maps and a curvilinear coordinates framework to deal with any kind of roundabouts. The proposed approach is not limited to communicating autonomous vehicles but can also be used with manually driven communicating vehicles or non-communicating vehicles with the help on the infrastructure. A formal proof of the correctness of this approach is given and simulations were carried out with a high definition map of a real roundabout. We also introduce a novel graphical representation called safety diagram to study de performances of our approach.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst9"><b>ThPS-SST9</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst9" title="Click to go to the Program at a Glance"><b>Lane Merging and Change Prediction</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_01">16:00-18:00, Paper ThPS-SST9.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0513.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('513'); return false" title="Click to show or hide the keywords and abstract">Data Collection and Processing Methods for the Evaluation of Vehicle Road Departure Detection Systems</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35613" title="Click to go to the Author Index">Shen, Dan</a></td><td class="r">Purdue School of Engineering and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27904" title="Click to go to the Author Index">Yi, Qiang</a></td><td class="r">Indiana Univ. Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17493" title="Click to go to the Author Index">Li, Lingxi</a></td><td class="r">Indiana Univ. Univ. Indianapolis</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#24123" title="Click to go to the Author Index">Chien, Stanley</a></td><td class="r">Indiana Univ. Univ. Indianapolis</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17557" title="Click to go to the Author Index">Chen, Yaobin</a></td><td class="r">Purdue School of Engineering and Tech. IUPUI</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#17407" title="Click to go to the Author Index">Sherony, Rini</a></td><td class="r">Toyota Motor Engineering and Manufacturing North America</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab513" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Active_and_Passive_Vehicle_Safety" title="Click to go to the Keyword Index">Active and Passive Vehicle Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> Road departure detection systems (RDDSs) for avoiding/mitigating road departure crashes have been developed and included on some production vehicles in recent years. In order to support and provide a standardized and objective performance evaluation of RDDSs, this paper describes the development of the data acquisition and data post-processing systems for testing RDDSs. Seven parameters are used to describe road departure test scenarios. The overall structure and specific components of data collection system and data post-processing system for evaluating vehicle RDDSs is devised and presented. Experimental results showed sensing system and data post-processing system could capture all needed signals and display vehicle motion profile from the testing vehicle accurately. Test track testing under different scenarios demonstrates the effective operations of the proposed data collection system.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_02">16:00-18:00, Paper ThPS-SST9.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0147.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('147'); return false" title="Click to show or hide the keywords and abstract">A Reinforcement Learning Based Approach for Automated Lane Change Maneuvers</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36345" title="Click to go to the Author Index">Wang, Pin</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10376" title="Click to go to the Author Index">Chan, Ching-Yao</a></td><td class="r">ITS, Univ. of California at Berkeley</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13613" title="Click to go to the Author Index">de La Fortelle, Arnaud</a></td><td class="r">MINES ParisTech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab147" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Lane change is a crucial vehicle maneuver which needs coordination with surrounding vehicles. Automated lane changing functions built on rule-based models may perform well under pre-defined operating conditions, but they may be prone to failure when unexpected situations are encountered. In our study, we proposed a Reinforcement Learning based approach to train the vehicle agent to learn an automated lane change behavior such that it can intelligently make a lane change under diverse and even unforeseen scenarios. Particularly, we treated both state space and action space as continuous, and designed a Q-function approximator that has a closed-form greedy policy, which contributes to the computation efficiency of our deep Q-learning algorithm. Extensive simulations are conducted for training the algorithm, and the results illustrate that the Reinforcement Learning based vehicle agent is capable of learning a smooth and efficient driving policy for lane change maneuvers.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_03">16:00-18:00, Paper ThPS-SST9.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0005.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('5'); return false" title="Click to show or hide the keywords and abstract">Learning to Predict Lane Changes in Highway Scenarios Using Dynamic Filters on a Generic Traffic Representation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35854" title="Click to go to the Author Index">Mänttäri, Joonatan</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31323" title="Click to go to the Author Index">Folkesson, John</a></td><td class="r">KTH -Royal Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31317" title="Click to go to the Author Index">Ward, Erik</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab5" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> In highway driving scenarios it is important for highly automated driving systems to be able to recognize and predict the intended maneuvers of other drivers in order to make robust and informed decisions. Many methods utilize the current kinematics of vehicles to make these predictions, but it is possible to examine the relations between vehicles as well to gain more information about the traffic scene and make more accurate predictions. The work presented in this paper proposes a novel method of predicting lane change maneuvers in highway scenarios using deep learning and a generic visual representation of the traffic scene. Experimental results suggest that by operating on the visual representation, the spacial relations between arbitrary vehicles can be captured by our method and used for more informed predictions without the need for explicit dynamic or driver interaction models. The proposed method is evaluated on highway driving scenarios using the Interstate-80 dataset and compared to a kinematics based prediction model, with results showing that the proposed method produces more robust predictions across the prediction horizon than the comparison model.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_04">16:00-18:00, Paper ThPS-SST9.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0552.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('552'); return false" title="Click to show or hide the keywords and abstract">Naturalistic Lane Change Analysis for Human-Like Trajectory Generation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28561" title="Click to go to the Author Index">Xu, Donghao</a></td><td class="r">Peking Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37480" title="Click to go to the Author Index">Ding, Zhezhang</a></td><td class="r">Peking Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12556" title="Click to go to the Author Index">Zhao, Huijing</a></td><td class="r">Peking Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36871" title="Click to go to the Author Index">Moze, Mathieu</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31507" title="Click to go to the Author Index">Aioun, Francois</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31505" title="Click to go to the Author Index">Guillemard, Franck</a></td><td class="r">PSA Peugeot Citroen, Velizy, France</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab552" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Human-like driving is of great significance for safety and comfort of autonomous vehicles, but existing trajectory planning methods for on-road vehicles rarely take the similarity with human behavior into consideration. From a representative trajectory-generation-based planning algorithm, this paper analyzes the systematic deviation of the generated trajectories from human trajectories, and proposes a new scheme of trajectory generation by compensating the deviation using a deviation profile learned from data. Experimental results show that the proposed trajectory generator is able to fit the human trajectories considerably better than the original one with only one additional degree of freedom. When used for online trajectory planning, with the same level of computational complexity, the proposed generator is able to generate trajectories that are more human-like than original generator does, which provides basis for autonomous vehicle to perform human-like trajectory planning.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_05">16:00-18:00, Paper ThPS-SST9.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0218.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('218'); return false" title="Click to show or hide the keywords and abstract">Behavior Decision Making of Lane Change Based on RCS for Automated Vehicles in the Real Environment</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13755" title="Click to go to the Author Index">Xiong, Guangming</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39044" title="Click to go to the Author Index">Kang, Ziyi</a></td><td class="r">BIT</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#39045" title="Click to go to the Author Index">Song, Weilong</a></td><td class="r">China North Vehicle Res. Inst</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15879" title="Click to go to the Author Index">Gong, Jianwei</a></td><td class="r">Beijing Inst. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab218" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> This paper proposes the decision-making framework of lane change behavior based on Hierarchical State Machine (HSM) and we build distributed control system architecture based on RCS (Real-Time Control System) to test the model. Environment perception module, decision planning module and execution control module are put into the distributed system architecture based on RCS to improve real-time and ensure that several modules run simultaneously. Besides, the decision-making framework of lane change behavior consists of two parts: miniature scene information model and decision-making model of lane change behavior based on multi-attribute decision-making. The decision-making model of lane change behavior is based on HSM and it sets two top-level state machines: free lane change model and mandatory lane change model. Free lane change model changes the state by using lane reward model to judge and assess driving condition of each lane, while mandatory lane change model uses strategy of multi-source information fusion to judge whether to lane change. In the end, the unmanned platform BYD Tang using vehicle embedded platform is used to verify the reliability and effectiveness of the lane change decision-making algorithm proposed in this paper in the real road environment.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_06">16:00-18:00, Paper ThPS-SST9.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0473.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('473'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Novel Lane Merging Framework with Probabilistic Risk Based Lane Selection Using Time Scaled Collision Cone</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36001" title="Click to go to the Author Index">Avula, Venkata Seetharama Sai Bhargav Kumar</a></td><td class="r">International Inst. of Information Tech. Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36254" title="Click to go to the Author Index">Modh, Adarsh</a></td><td class="r">International Inst. of Information Tech. Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36159" title="Click to go to the Author Index">Nallana, Mithun Babu</a></td><td class="r">IIIT Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30666" title="Click to go to the Author Index">Gopalakrishnan, Bharath</a></td><td class="r">Iiit Hyderabad</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23891" title="Click to go to the Author Index">Krishna, K Madhava</a></td><td class="r">IIIT Hyderabad</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab473" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0473.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Conventionally, planning frameworks for autonomous vehicles consider large safety margins and predefined paths for performing the merge maneuvers. These considerations often increase the wait time at the intersections leading to traffic disruption. In this paper, we present a motion planning framework for autonomous vehicles to perform merge maneuver in dense traffic. Our framework is divided into a two-layer structure, Lane Selection layer and Scale Optimization layer. The Lane Selection layer computes the likelihood of collision along the lanes. This likelihood represents the collision risk associated with each lane and is used for lane selection. Subsequently, the Scale Optimization layer solves the time scaled collision cone (TSCC) constraint reactively for collision-free velocities. Our framework guarantees a collision-free merging even in dense traffic with minimum disruption. Furthermore, we show the simulation results in different merging scenarios to demonstrate the efficacy of our framework.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst9_07">16:00-18:00, Paper ThPS-SST9.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0654.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('654'); return false" title="Click to show or hide the keywords and abstract">Learning Vehicle Surrounding-Aware Lane-Changing Behavior from Observed Trajectories</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37658" title="Click to go to the Author Index">Su, Shuang</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37563" title="Click to go to the Author Index">Muelling, Katharina</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15953" title="Click to go to the Author Index">Dolan, John</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37562" title="Click to go to the Author Index">Palanisamy, Praveen</a></td><td class="r">General Motors</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#20643" title="Click to go to the Author Index">Mudalige, Priyantha</a></td><td class="r">General Motors</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab654" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Predicting lane-changing intentions has long been a very active area of research in the autonomous driving community. However, most of the literature has focused on individual vehicles and did not consider both the vehicle's trajectory history and neighbor information when making the predictions. We propose to apply a surrounding-aware LSTM algorithm for predicting the intention of a vehicle to perform a lane change that takes advantage of both vehicle past trajectories and their neighbor's current states. We trained the model on real-world lane changing data and were able to show in simulation that these two components can lead not only to higher accuracy, but also to lower lane-changing prediction time, which plays an important role in potentially improving the autonomous vehicle's overall performance.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst10"><b>ThPS-SST10</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst10" title="Click to go to the Program at a Glance"><b>Lane and Road Detection</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_01">16:00-18:00, Paper ThPS-SST10.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0161.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('161'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Vision-Based Recognition of Road Regulation for Intelligent Vehicle</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25304" title="Click to go to the Author Index">Lim, Kwangyong</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36387" title="Click to go to the Author Index">Hong, Yong</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36393" title="Click to go to the Author Index">Ki, Min-song</a></td><td class="r">Yonsei Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25303" title="Click to go to the Author Index">Choi, Yeongwoo</a></td><td class="r">Sookmyung Women’s Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15752" title="Click to go to the Author Index">Byun, Hyeran</a></td><td class="r">Yonsei Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab161" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0161.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> In this paper, we present a new framework to detect and recognize entire lanes and symbolic marks on high resolution road images. The first part of the framework utilizes local threshold to overcome the limitations of fixed threshold determination in road marking segmentation. The second part of the framework handles false detections caused by nearby objects on the roads such as vehicles and buildings by re-moving the areas that are not related to road surface using semantic segmentation. It also boosts recognition performance with a cascaded classifier structure that combines CNN for symbolic mark recognition and SVM for lane verification. The proposed lane detection achieves average F1-score of 0.96 and symbol recognition achieves average F1-score of 0.91. The proposed method is expected to advance the vehicle industry; with a GPU device, the proposed method can easily be embedded in smart vehicles.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_02">16:00-18:00, Paper ThPS-SST10.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0248.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('248'); return false" title="Click to show or hide the keywords and abstract">Early Fusion of Camera and Lidar for Robust Road Detection Based on U-Net</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36641" title="Click to go to the Author Index">Wulff, Florian</a></td><td class="r">Fraunhofer Inst. for Open Communication Systems</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29094" title="Click to go to the Author Index">Becker, Daniel</a></td><td class="r">Daimler Center for Automotive Information Tech. Innovations</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30966" title="Click to go to the Author Index">Henke, Birgit</a></td><td class="r">Fraunhofer FOKUS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23243" title="Click to go to the Author Index">Schäufele, Bernd</a></td><td class="r">Fraunhofer FOKUS</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23242" title="Click to go to the Author Index">Sawade, Oliver</a></td><td class="r">Fraunhofer Inst. for Open Communication Systems</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#23245" title="Click to go to the Author Index">Radusch, Ilja</a></td><td class="r">Daimler Center for Automotive Information Tech. Innovations</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab248" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> Automated vehicles rely on the accurate and robust detection of the drivable area, often classified into free space, road area and lane information. Most current approaches use monocular or stereo cameras to detect these. However, LiDAR sensors are becoming more common and offer unique properties for road area detection such as precision and robustness to weather conditions. We therefore propose two approaches for a pixel-wise semantic binary segmentation of the road area based on a modified U-Net Fully Convolutional Network (FCN) architecture. The first approach UView-Cam employs a single camera image, whereas the second approach UGrid-Fused incorporates a early fusion of LiDAR and camera data into a multi-dimensional occupation grid representation as FCN input. The fusion of camera and LiDAR allows for efficient and robust leverage of individual sensor properties in a single FCN. For the training of UView-Cam, multiple publicly available datasets of street environments are used, while the UGrid-Fused is trained with the KITTI dataset. In the KITTI Road/Lane Detection benchmark, the proposed networks reach a MaxF score of 94.23% and 93.81% respectively. Both approaches achieve real-time performance with a detection rate of about 10 Hz.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_03">16:00-18:00, Paper ThPS-SST10.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0176.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('176'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Near-Optimal Online Motion Planning of Connected and Automated Vehicles at a Signal-Free and Lane-Free Intersection</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#26452" title="Click to go to the Author Index">Li, Bai</a></td><td class="r">Zhejiang Lab</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36902" title="Click to go to the Author Index">Zhang, Youmin</a></td><td class="r">Xi'an Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36901" title="Click to go to the Author Index">Zhang, Yue</a></td><td class="r">Center for Information and Systems Engineering, Boston Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35956" title="Click to go to the Author Index">Jia, Ning</a></td><td class="r">Tianjin Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36903" title="Click to go to the Author Index">Ge, Yuming</a></td><td class="r">China Acad. of Information and Communications Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab176" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0176.VD.mpg" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a></span><br>
                              <strong>Abstract:</strong> In this paper, we propose a cooperative motion planning method for a group of connected and automated vehicles (CAVs) crossing a lane-free intersection without using explicit traffic signaling. This multi-vehicle motion planning task is formulated as a centralized optimal control problem. However, the solution to this optimal control problem is numerically intractable due to the high dimensionality of the collision-avoidance constraints and the nonlinearity of the vehicle kinematics. A two-stage strategy is proposed for generating online solutions: at Stage 1, the CAVs are enforced to reach a standard formation before entering the intersection; at Stage 2, the vehicles cross the intersection. As the motion planning sub-problem at Stage 2 begins with a standard configuration, the optimal solution to this standard sub-problem can be computed offline in advance and used online directly. On the other hand, the formation reconfiguration sub-problem at Stage 1 is easy to solve online. By dividing the entire dynamic process into two periods, the difficulties in the original optimal control problem can be significantly reduced so that real-time performance is achieved.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_04">16:00-18:00, Paper ThPS-SST10.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0054.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('54'); return false" title="Click to show or hide the keywords and abstract">Exploring OpenStreetMap Capability for Road Perception</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#29461" title="Click to go to the Author Index">Zheng, Yang</a></td><td class="r">Aptiv</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35967" title="Click to go to the Author Index">Izzat, Izzat</a></td><td class="r">Aptiv Inc</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab54" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a></span><br>
                              <strong>Abstract:</strong> Drivable space detection or road perception is one of the most important tasks for autonomous driving. Sensor-based vision/laser systems may have limited performance in bad illumination/weather conditions, a prior knowledge of the road from the map data is expected to improve the effectiveness. This paper is to employ the map information extracted from the OpenStreetMap (OSM) data, and explore its capability for road perception. The OSM data can be used to render virtual street views, and further refined to provide the prior road mask. The OSM masks can be also combine with image processing and Lidar point clouding approaches to characterize the drivable space. Using a Fully Convolutional Neural Network (FCNN), the OSM availability for deep learning methods is also discussed.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_05">16:00-18:00, Paper ThPS-SST10.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0291.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('291'); return false" title="Click to show or hide the keywords and abstract">An Efficient Encoder-Decoder CNN Architecture for Reliable Multilane Detection in Real Time</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36753" title="Click to go to the Author Index">Chougule, Shriyash</a></td><td class="r">Visteon</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36492" title="Click to go to the Author Index">Ismail, Asad</a></td><td class="r">Visteon Electronics Germany GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36484" title="Click to go to the Author Index">Soni, Ajay</a></td><td class="r">Visteon</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36507" title="Click to go to the Author Index">Kozonek, Nora</a></td><td class="r">Visteon</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35809" title="Click to go to the Author Index">Narayan, Vikram</a></td><td class="r">Visteon Electronics Germany GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21281" title="Click to go to the Author Index">Schulze, Matthias</a></td><td class="r">Visteon Eiectronics Germany</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab291" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Image__Radar__Lidar_Signal_Processing" title="Click to go to the Keyword Index">Image, Radar, Lidar Signal Processing</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Multilane detection system is a vital prerequisite for realizing higher ADAS functionality of autonomous navigation. In this work, we present an efficient convolutional neural network (CNN) architecture for real time detection of multiple lane boundaries using a camera sensor. Our network has a simple encoder-decoder architecture and is a special two class semantic segmentation network designed to segment lane boundaries. Efficacy of our network stems from two key insights which are at the foundation of all our design decisions. Firstly, we term a lane boundary as a weak class object in the context of semantic segmentation. We show that the weak class objects which occupy relatively few pixels in the scene, also have a relatively low detection accuracy among the know segmentation methods. We present novel design choices and intuitions to improve the segmentation accuracy of weak class objects, which in turn reduces computation time. Our second insight lies in the manner we depict the ground truth information in our derived dataset. Instead of annotating just the visible lane markers, we accurately delineate the lane boundaries in the ground truth for challenging scenarios like occlusions, low light and degraded lane markings. We then leverage the CNN's ability to concisely summarize the global and local context in an image, for accurately inferring lane boundaries in these challenging cases. We evaluate our network against ENet and FCN-8, and found it performing notably better in terms of speed and accuracy. Our network achieves an encouraging 46 FPS performance on NVIDIA Drive PX2 platform and it has been validated on our test vehicle in highway driving conditions.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_06">16:00-18:00, Paper ThPS-SST10.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0325.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('325'); return false" title="Click to show or hide the keywords and abstract">A Reliable Road Segmentation and Edge Extraction for Sparse 3D Lidar Data</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36742" title="Click to go to the Author Index">Gu, Jianfeng</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36836" title="Click to go to the Author Index">Wang, Yuehui</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18409" title="Click to go to the Author Index">Chen, Long</a></td><td class="r">Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36743" title="Click to go to the Author Index">Zhao, Zhihao</a></td><td class="r">School of Data and Computer Science, Sun Yat-Sen Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34953" title="Click to go to the Author Index">Xuanyuan, Zhe</a></td><td class="r">Beijing Normal Univ. Kong Baptist Univ. United In</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34952" title="Click to go to the Author Index">Huang, Kai</a></td><td class="r">Sun Yat-Sen Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab325" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Precise segmentation of road areas using low laser Lidar is a tough and critical task for small intelligent vehicle due to data sparsity problem. The paper proposes a reliable approach for unstructured road segmentation and edge extraction with sparse 3D Lidar data. Muliti-frame point clouds, acquired from low lasers Lidar, are matched by fast Iterative Closest Point(ICP) algorithm to make the environment data denser, and VoxelGrid filter is assigned for removal of overlapping points. Besides, the methods, Linefit and RANSAC, are applied for rough and precise road segmentation, respectively. Furthermore, border points are extracted from refined road plane and fitted by polynomial curve fitting method to generate the road edges. The accuracies of extensive experiments on three kinds of roads demonstrate that the proposed approach obtains great precision and reliability.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_07">16:00-18:00, Paper ThPS-SST10.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0414.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('414'); return false" title="Click to show or hide the keywords and abstract">Coding Pavement Lanes for Accurate Self-Localization of Intelligent Vehicles</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40329" title="Click to go to the Author Index">Tao, Qianwen</a></td><td class="r">Wuhan Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37072" title="Click to go to the Author Index">Hu, Zhaozheng</a></td><td class="r">Wuhan Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab414" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> Self-localization is a key technology for intelligent vehicles. This paper demonstrates a practical and easy solution to vehicle self-localization by simply coding pavement lane lines. Especially, the coding of pavement lane lines makes it possible to distinguish unique pavement markings within certain ranges, which is crucial for vehicle localization. Based on the coded pavement lane lines, we proposed a multi-scale strategy for accurate vehicle localization. The localization method consists of coarse localization with Real-time Locating Systems (RTLS), marking-level localization with marking matching, and metric localization by matching distinctive visual feature points around the marking area. The proposed method has been tested by using the actual data collected in the field, where we encoded the pavement lane lines with two different colors (i.e., white and yellow). The results demonstrate that the proposed method can achieve sub-meter localization accuracy by referring to the coded pavement lane lines. The results also demonstrate that advanced road infrastructure could greatly support intelligent vehicles with low-cost and reliable solutions.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_08">16:00-18:00, Paper ThPS-SST10.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0490.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('490'); return false" title="Click to show or hide the keywords and abstract">A Novel Approach for Detecting Road Based on Two-Stream Fusion Fully Convolutional Network</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37288" title="Click to go to the Author Index">Lv, Xin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36465" title="Click to go to the Author Index">Liu, Ziyi</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30143" title="Click to go to the Author Index">Xin, Jingmin</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10618" title="Click to go to the Author Index">Zheng, Nanning</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab490" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a></span><br>
                              <strong>Abstract:</strong> Road detection is one of the most basic tasks of autonomous driving systems. At present, researches on this issue mainly take two kinds of data as input, i.e., LIDAR point clouds and RGB images from cameras. To make best use of the advantages and bypass the disadvantages of these two kinds of data, we propose a novel network, namely two-stream fusion fully convolutional network (TSF-FCN), which can take advantage of both the accurate location information from LIDAR point clouds and rich appearance information from RGB images. One stream of this network is LIDAR stream which aggregates multi-scale contextual information from LIDAR point clouds. The other stream is RGB stream which is used for extracting features from RGB images. To fuse the two streams, the feature maps of RGB stream are converted to a bird-view representation to concatenate with that of LIDAR stream. In this way, the two kinds of data can complement each other for detecting road. To verify the efficacy of our TSF-FCN, experiments are carried on KITTI-ROAD benchmark and competitive performance is achieved compared with state-of-the-art methods.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_09">16:00-18:00, Paper ThPS-SST10.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0601.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('601'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Robust Lane Detection Using Multiple Features</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37565" title="Click to go to the Author Index">Gupta, Tejus</a></td><td class="r">Indian Inst. of Tech. Kharagpur</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36307" title="Click to go to the Author Index">Sikchi, Harshit</a></td><td class="r">IIT Kharagpur</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37634" title="Click to go to the Author Index">Chakravarty, Debashish</a></td><td class="r">Indian Inst. of Tech. Kharagpur</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab601" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0601.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Environment_Perception" title="Click to go to the Keyword Index">Vehicle Environment Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Lane marker detection is a crucial challenge in developing self-driving cars. Despite significant research, large gaps remain between research and needs for fully autonomous driving. We highlight the limitations of present work and present a unified approach for robust and real-time lane marker detection. We present a multi-feature lane detection algorithm and give evidence why relying on one type of features can be harmful. We design a lane model using geometric constraints on lane shape and fit the lane model to the visual cues extracted. We improve the robustness of our algorithm by tracking lane markers temporally. We test our algorithm on KITTI dataset and show results that our algorithm can detect lane markers in presence of occlusions, sharp curves, and shadows.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst10_10">16:00-18:00, Paper ThPS-SST10.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0205.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('205'); return false" title="Click to show or hide the keywords and abstract">An Accurate and Computational Efficient System for Detecting and Classifying Ego and Sides Lanes Using LiDAR</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#32885" title="Click to go to the Author Index">de Paula Veronese, Lucas</a></td><td class="r">Visteon Electronics Germany GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36492" title="Click to go to the Author Index">Ismail, Asad</a></td><td class="r">Visteon Electronics Germany GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35809" title="Click to go to the Author Index">Narayan, Vikram</a></td><td class="r">Visteon Electronics Germany GmbH</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#21281" title="Click to go to the Author Index">Schulze, Matthias</a></td><td class="r">Visteon Eiectronics Germany</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab205" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a>, <a href="IV2018_KeywordIndexMedia.html#Lidar_Sensing_and_Perception" title="Click to go to the Keyword Index">Lidar Sensing and Perception</a></span><br>
                              <strong>Abstract:</strong> In this work, we are proposing a computationally efficient LiDAR based lane detection system that detects both ego and side lanes using 3D LiDARs. Our solution relies on the construction of local gird map around the ego vehichle using the infrared reflectance of combination of LiDARs. To fuse the information of the LiDARs into a map, the vehicle ego-motion is taken into account. The system is built using image processing by binarizing the map to extract the lane markers. The evaluation of computational performance of the final solution is realized on a single ARM core of the NVIDIA Drive PX2 without the need for the GPUs, and achieved a frame rate of 40 Hz. In the absence of a publicly available annotated dataset for LiDAR based lane detection, we evaluate the proposed solution against our proprietary camera based lane detection system. We observed a good correlation between the two in terms of Jaccard and Dice Coefficients.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst11"><b>ThPS-SST11</b></a></td>
               <td class="r">RenHe Hall 2</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst11" title="Click to go to the Program at a Glance"><b>Traffic Management</b></a></td>
               <td class="r">Poster Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_01">16:00-18:00, Paper ThPS-SST11.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0141.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('141'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Feasibility Study on a Traffic Management System for Autonomous Driving Services Based on Dynamic Map</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30582" title="Click to go to the Author Index">Akagi, Yasuhiro</a></td><td class="r">Tokyo Univ. of Agriculture and Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#18324" title="Click to go to the Author Index">Raksincharoensak, Pongsathorn</a></td><td class="r">Tokyo Univ. of Agriculture and Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab141" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0141.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a></span><br>
                              <strong>Abstract:</strong> An autonomous driving service in urban areas is still a challenging problem. It is necessary to simultaneously solve tactical planning problems and operation planning problems, such as in a scenario for passing through an intersection with oncoming vehicles. To achieve a service that uses a centralized management system, it is necessary to guide thousands of vehicles in real-time. Therefore, efficiency and robustness are required for the system. In this study, we propose a centralized path planning system to make tactical level decisions to manage the path plans of multiple vehicles based on Dynamic Map. The proposed system selects an operational-level planner to generate a detailed motion plan for each vehicle on the basis of a decision-making algorithm. The conflicts of the motion plans between vehicles are then detected and resolved by using a spatio-temporal reservation map on Dynamic Map. We report the results of the 10,000-hour simulation test to evaluate the efficiency and robustness of the proposed system using the map data of an existing city.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_02">16:00-18:00, Paper ThPS-SST11.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0145.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('145'); return false" title="Click to show or hide the keywords and abstract">Modeling the Special Intersection for Enhanced Digital Map</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#13784" title="Click to go to the Author Index">Li, Xu</a></td><td class="r">Southeast Univ. of China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36311" title="Click to go to the Author Index">Xia, Liang</a></td><td class="r">Southeast Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36313" title="Click to go to the Author Index">Song, Xianghui</a></td><td class="r">Key Lab. of Tech. on Intelligent Transportation Syste</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28413" title="Click to go to the Author Index">Xu, Qimin</a></td><td class="r">Southeast Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab145" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Mapping_and_Localization" title="Click to go to the Keyword Index">Mapping and Localization</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> The enhanced digital map is of great significance for various Intelligent Transportation System (ITS) applications and services, especially at the lane-level. This requirement motives the development of road modeling in the enhanced digital map at the lane-level. However, previous enhanced digital maps do not provide detailed modeling of special intersections which are covered by vegetation in the central region. In this paper we propose a novel lane-level road model for this special intersection scenario. The proposed intersection model can be considered into two levels: topological structure and geometrical structure. Topological structure of this model helps describe the connectivity, turn restrictions and other attributes of the special intersection in the real world. Geometrical structure of this model helps describe the virtual lanes of the internal part of the special intersection using cardinal spline, which better approximates the real vehicle trajectory at the intersection. The proposed intersection model has been verified and evaluated through experiments. The results demonstrate the effectiveness of the proposed intersection model in representing the lane-level topological and geometrical details of special intersection which is covered by vegetation in its central region.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_03">16:00-18:00, Paper ThPS-SST11.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0578.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('578'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Traffic Scene Prediction Via Deep Learning: Introduction of Multi-Channel Occupancy Grid Map As a Scene Representation</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35262" title="Click to go to the Author Index">Jeon, Hyeongseok</a></td><td class="r">Korea Advanced Inst. of Science & Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28241" title="Click to go to the Author Index">Kum, Dongsuk</a></td><td class="r">Korea Advanced Inst. of Science & Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36940" title="Click to go to the Author Index">Jeong, Wooyeol</a></td><td class="r">Hyundai Motor Group</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab578" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              <strong>Attachments:</strong> <span style=""><a href="./files/0578.VD.mp4" title="Click to open">Video demonstration</a></span><br>

                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> When predicting future motions of surrounding vehicles for autonomous vehicles, the inter-vehicular interaction must be considered in order to predict future risks and to make safe and intelligent decisions. This becomes critical when it comes to conflicting driving situation such as lane merge, tollgate area, and unsignalized intersections. Previously developed future prediction algorithms show limited performance when handling interactions and conflicts between vehicles because they focused on predicting individual vehicle motion and/or interaction between a single pair of vehicles rather than the entire traffic scene. In this paper, a scene representation method, namely multi-channel Occupancy Grid Map (OGM), is proposed to describe the entire traffic scene, which is then utilized for the deep learning architecture that predicts the future traffic scene or OGM. Multi-channel OGM represents entire traffic scene as a manner of image-like structure from bird’s eye view composed with dynamic layer and static layer depicting the occupancy of the dynamic and static objects. By using this 2D traffic scene representation, future prediction can be modeled as a video processing problem, where future time-serial image sequence need to be predicted. In order to predict future traffic scenes based on past traffic scenes, a deep learning architecture is proposed using Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) Networks. With the proposed deep learning architecture, future prediction accuracy in highly conflicting traffic situation is guaranteed up to 90 percent with 3 seconds of prediction horizon. A video of the traffic scene prediction results is available online [1].
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_04">16:00-18:00, Paper ThPS-SST11.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0108.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('108'); return false" title="Click to show or hide the keywords and abstract">Chaos Theory in Urban Traffic Flow: Is Crowd Sensed Data Driving the Macro-Traffic Behavior to Oscillation or Equilibrium?</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36203" title="Click to go to the Author Index">Liang, Huanghuang</a></td><td class="r">Univ. of Electronic Science and Tech. of China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36206" title="Click to go to the Author Index">Yang, Lu</a></td><td class="r">Univ. of Electronic Science and Tech. of China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36194" title="Click to go to the Author Index">Wei, Jiacheng</a></td><td class="r">Univ. of Electronic Science and Tech. of China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40908" title="Click to go to the Author Index">Zhao, Yang</a></td><td class="r">Univ. of Electronic Science and Tech. of China</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10616" title="Click to go to the Author Index">Cheng, Hong</a></td><td class="r">Univ. of Electronics Science and Tech. of China</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab108" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Traffic_Flow_and_Management" title="Click to go to the Keyword Index">Traffic Flow and Management</a></span><br>
                              <strong>Abstract:</strong> Stability theory tells us that a dynamic system will eventually converge to its stable state, in which the system's overall energy is at its minimum. On the other hand, chaos theory states that small perturbations of the system are able to drive itself from previously-stable state to another state. This phenomenon has been observed in many fields like cosmetology, physics, biology and chemistry. Our research question is whether chaos theory also applies to the transportation domain. Specifically, when we are given imperfect or delayed crowd-sensed data, will we observe the cyclic/oscillatory transition between different traffic states? This paper aims at investigating this chaotic phenomenon (oscillatory traffic behavior in this paper) on urban transportation with imperfect or delayed crowd-sensed information and delivering recommendations for crowdsensing-based traffic applications to avoid the undesirable oscillations.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_05">16:00-18:00, Paper ThPS-SST11.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0650.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('650'); return false" title="Click to show or hide the keywords and abstract">On-Road I-Vics Management for Blockage of Abreast Low Speed Vehicles Near Signalized Intersections</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35939" title="Click to go to the Author Index">Hou, Kaizhe</a></td><td class="r">Tsinghua Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10704" title="Click to go to the Author Index">Hu, Jianming</a></td><td class="r">Tsinghua</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10742" title="Click to go to the Author Index">Zhang, Yi</a></td><td class="r">Tsinghua Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab650" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Driving at velocity much lower than the speed limit greatly restraints the movements of the followers, resulting in considerable waste of road resource every day. When two slow vehicles are moving abreast on the city road, the result is devastating for their followers. To solve such problem by the idea the i-Vics (intelligent vehicular infrastructure cooperative systems), this paper proposes a dedicated on-road traffic management plan for abreast low speed connected vehicles, especially for scenarios near signalized intersections. The management plan takes three steps to erase the traffic blockage caused by those abreast slow drivers, including detection of abreast slow drivers, appraisal of environmental factors, and traffic guidance notification for all relevant drivers. In the proposed plan, the detection model initially evaluates the whole street and picks out all the abreast low speed vehicles based on the i-Vics. For every blockage target, a appraisal model of the surroundings determines the strategy how to dissolve the traffic blockage, including the feasibility of overtaking, and possibility of passing through the intersection and etc. Finaly, once the dissolving strategy is approved by the appraisal program, the management system sends notification of speed guidance to all relevant drivers to guide them to pass through the signalized intersection within the remaining green time.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_06">16:00-18:00, Paper ThPS-SST11.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0386.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('386'); return false" title="Click to show or hide the keywords and abstract">The Relationship between Different Safety Indicators in Car-Following Situations</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36053" title="Click to go to the Author Index">Liu, Tong</a></td><td class="r">Chang'an Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31894" title="Click to go to the Author Index">Selpi, Selpi</a></td><td class="r">Chalmers Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27939" title="Click to go to the Author Index">Fu, Rui</a></td><td class="r">Chang'an Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab386" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Advanced_Driver_Assistance_Systems" title="Click to go to the Keyword Index">Advanced Driver Assistance Systems</a></span><br>
                              <strong>Abstract:</strong> Studying different aspects of car-following behavior is still of strong interests for many researchers, due to its usefulness in many applications, such as for further development of traffic simulators and active safety systems (e.g., Adaptive Cruise Control and Autonomous Emergency Braking). This paper investigates the relationships between several safety indicators (e.g., time gap, gap distance, time to collision) in car-following situations, and analyzes which of these indicators affect driver’s behavior in car-following situations and how. All analyses are done using real driving data collected in China. The paper also suggests parameters that can be used for defining, identifying, and extracting car-following events from real driving data. Results indicate that time gap is less sensitive to the variations in speed and road condition compared with gap distance in this test. TTC in the low speed range of subject-vehicle is found to be steady compared with other speed ranges, so is the time gap in the high speed range. Therefore, time gap is more suitable to be the safety indicator compared with gap distance in the future car-following research. Time gap is found to be more appropriate for the analysis of car following behavior in the high speed ranges, but both TTC and time gap should be used as part of the safety indicator for the low speed ranges.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_07">16:00-18:00, Paper ThPS-SST11.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0093.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('93'); return false" title="Click to show or hide the keywords and abstract">Using Time-To-React Based on Naturalistic Traffic Object Behavior for Scenario-Based Risk Assessment of Automated Driving</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36007" title="Click to go to the Author Index">Wagner, Sebastian</a></td><td class="r">Tech. Univ. Munich</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34986" title="Click to go to the Author Index">Groh, Korbinian</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34989" title="Click to go to the Author Index">Kuehbeck, Thomas</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36093" title="Click to go to the Author Index">Doerfel, Michael</a></td><td class="r">BMW Group</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19222" title="Click to go to the Author Index">Knoll, Alois</a></td><td class="r">Tech. Univ. München</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab93" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> The steady improvement of advanced driving assis- tance systems (ADAS) and the leap towards automated driving (AD) require novel methods for assessing the safety of those, which is a major subject for current research. Different propos- als cope with the massive testing effort to assure the safety of such systems. These proposals include virtualization of testing, usage of stochastic methods and reduction of the necessary real world driving tests. Despite these different approaches, they all rely on the same basis: The behavior assessment of the vehicle under test, which results in a measurement of risk. This paper presents a novel approach to measure the criti- cality of a given driving scenario fitted on the requirements of testing. A Monte-Carlo simulation, which uses the input of a motion prediction model as variation parameters, determines the possible evolutions of a scenario at every time step. The distributions of these parameters have been fitted to data obtained by a large-scale field tests. These evolutions are then analyzed individually by considering the Time-To-React (TTR) measure. Finally a single value of accident risk between 0 and 1 can be assigned to the scenario.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_08">16:00-18:00, Paper ThPS-SST11.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0341.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('341'); return false" title="Click to show or hide the keywords and abstract">Following Cars with Elastic Bands</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31307" title="Click to go to the Author Index">Ulbrich, Fritz</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27191" title="Click to go to the Author Index">Langner, Tobias</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36880" title="Click to go to the Author Index">Sundermann, Stephan</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#31305" title="Click to go to the Author Index">Goehring, Daniel</a></td><td class="r">Freie Univ. Berlin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16677" title="Click to go to the Author Index">Rojas, Raúl</a></td><td class="r">Berlin Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab341" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Self_Driving_Vehicles" title="Click to go to the Keyword Index">Self-Driving Vehicles</a></span><br>
                              <strong>Abstract:</strong> We propose a trajectory planning approach for autonomous vehicles in highly dynamic traffic scenarios, using elastic bands to follow the observed trajectories of other vehicles. The focus of this paper is on the initialization of the elastic band. The proposed method does not rely on a map. We tested our method using recorded urban traffic data. The results show that the presented approach is valid and the proposed initialization process is clearly superior to naive initialization.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_09">16:00-18:00, Paper ThPS-SST11.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0254.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('254'); return false" title="Click to show or hide the keywords and abstract">Social Force Based Microscopic Modeling of Vehicle-Crowd Interaction</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35753" title="Click to go to the Author Index">Yang, Dongfang</a></td><td class="r">The Ohio State Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10356" title="Click to go to the Author Index">Ozguner, Umit</a></td><td class="r">Ohio State Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#10197" title="Click to go to the Author Index">Redmill, Keith</a></td><td class="r">Ohio State Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab254" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vulnerable_Road_User_Safety" title="Click to go to the Keyword Index">Vulnerable Road-User Safety</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Situation_Analysis_and_Planning" title="Click to go to the Keyword Index">Situation Analysis and Planning</a></span><br>
                              <strong>Abstract:</strong> Pedestrian safety is of paramount importance for intelligent transportation systems. This study focuses on the scenarios where pedestrians appear as crowds and interact with moving vehicles in a relatively-free space. Based on social force model (SFM), a vehicle-crowd interaction (VCI) model is proposed to describe both the behavior of crowd pedestrians and vehicle. Specifically, a heuristic-based and effective modeling of vehicle influence on pedestrians is designed and incorporated into the crowd-only SFM. Qualitative analysis of systematic simulations of various VCI scenarios demonstrates the effectiveness of the proposed model. This model can effectively describe the VCI scenarios such as vehicle approaching from different directions, vehicle zigzagging, and vehicle sharply turning.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst11_10">16:00-18:00, Paper ThPS-SST11.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0624.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('624'); return false" title="Click to show or hide the keywords and abstract">Traffic Sensory Data Classification by Quantifying Scenario Complexity*</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37026" title="Click to go to the Author Index">Wang, Jiajie</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25523" title="Click to go to the Author Index">Zhang, Chi</a></td><td class="r">Inst. of Artificial Intelligence and Robotics, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#25095" title="Click to go to the Author Index">Liu, Yuehu</a></td><td class="r">Inst. of Artificial Intelligence and Robotics, Xi'an Jiaoton</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37355" title="Click to go to the Author Index">Zhang, Qilin</a></td><td class="r">HERE Tech. Chicago, Illinois</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab624" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Intelligent_Vehicle_Software_Infrastructure" title="Click to go to the Keyword Index">Intelligent Vehicle Software Infrastructure</a>, <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> Massive traffic scenario data play an important role in development of the unmanned ground vehicle (UGV), which contribute to the evaluation of the performance of tested algorithms. Current description on the scenario complexity mainly include I) different types of roads II) different scene content III) scene characteristics that are challenging to cognitive algorithms. However, these released datasets are suffered from lacking the expression of scenario complexity and the quantitative description of the scene features. To avoid the aforementioned problems, we propose a data classification paradigm based on quantifying scenario complexity. Scene complexity is quantified according to two different semantic levels. Firstly, the Road complexity which is predicted based on SVM Regression. Then traffic element complexity is obtained by 8-nearest neighbour analysis. Results prove that our methods to be of correctness and effectiveness. Besides the study case of unmanned off-line testing is also a valid proof of our method.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst12"><b>ThPS-SST12</b></a></td>
               <td class="r">ZhaoWen Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst12" title="Click to go to the Program at a Glance"><b>CAV Test & Evaluation</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst12_01">16:00-18:00, Paper ThPS-SST12.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0167.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('167'); return false" title="Click to show or hide the keywords and abstract">An Augmented Reality Environment for Connected and Automated Vehicle Testing and Evaluation (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36394" title="Click to go to the Author Index">Feng, Yiheng</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36395" title="Click to go to the Author Index">Yu, Chunhui</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#27347" title="Click to go to the Author Index">Xu, Shaobing</a></td><td class="r">Univ. of Michigan, Ann Arbor</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#40591" title="Click to go to the Author Index">Liu, Henry X.</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#12018" title="Click to go to the Author Index">Peng, Huei</a></td><td class="r">Univ. of Michigan</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab167" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Cooperative_Systems__V2X_" title="Click to go to the Keyword Index">Cooperative Systems (V2X)</a>, <a href="IV2018_KeywordIndexMedia.html#V2X_Communication" title="Click to go to the Keyword Index">V2X Communication</a></span><br>
                              <strong>Abstract:</strong> Testing and evaluation are critical steps in the development of connected and automated vehicle (CAV) technology. One limitation of closed CAV testing facilities is that they merely provide empty roadways, in which testing CAVs can only interact with a limited number of other CAVs and infrastructure. This paper presents an augmented reality environment for CAV testing and evaluation. A real-world testing facility and a simulation platform are combined together. Movements of testing CAVs in the real world are synchronized with simulation and information of background traffic is fed back to testing CAVs. Testing CAVs can interact with virtual background traffic as if in a realistic traffic environment. The proposed system mainly consists of three components: a simulation platform, testing CAVs, and a communication network. Testing scenarios that have safety concerns and/or require interactions with other vehicles can be performed. Two exemplary test scenarios are designed and implemented to demonstrate the capabilities of the system.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst12_02">16:00-18:00, Paper ThPS-SST12.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0602.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('602'); return false" title="Click to show or hide the keywords and abstract">Simulation-Based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#33440" title="Click to go to the Author Index">Tuncali, Cumhur Erkan</a></td><td class="r">Arizona State Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#22507" title="Click to go to the Author Index">Fainekos, Georgios</a></td><td class="r">Arizona State Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37359" title="Click to go to the Author Index">Ito, Hisahiro</a></td><td class="r">Toyota Motor North America</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37358" title="Click to go to the Author Index">Kapinski, James</a></td><td class="r">Toyota Motor North America</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab602" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vision_Sensing_and_Perception" title="Click to go to the Keyword Index">Vision Sensing and Perception</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Collision_Avoidance" title="Click to go to the Keyword Index">Collision Avoidance</a></span><br>
                              <strong>Abstract:</strong> Many organizations are developing autonomous driving systems, which are expected to be deployed at a large scale in the near future. Despite this, there is a lack of agreement on appropriate methods to test, debug, and certify the performance of these systems. One of the main challenges is that many autonomous driving systems have machine learning (ML) components, such as deep neural networks, for which formal properties are difficult to characterize. We present a testing framework that is compatible with test case generation and automatic falsification methods, which are used to evaluate cyber-physical systems. We demonstrate how the framework can be used to evaluate closed-loop properties of an autonomous driving system model that includes the ML components, all within a virtual environment. We demonstrate how to use test case generation methods, such as covering arrays, as well as requirement falsification methods to automatically identify problematic test scenarios. The resulting framework can be used to increase the reliability of autonomous driving systems.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst13"><b>ThPS-SST13</b></a></td>
               <td class="r">LongLiQi Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst13" title="Click to go to the Program at a Glance"><b>Intelligent Electric Vehicle State Estimation and Dynamics Control</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst13_01">16:00-18:00, Paper ThPS-SST13.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0363.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('363'); return false" title="Click to show or hide the keywords and abstract">Automated Vehicle Attitude and Lateral Velocity Estimation Using a 6-D IMU Aided by Vehicle Dynamics (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36542" title="Click to go to the Author Index">Xia, Xin</a></td><td class="r">School of Automotive Studies, Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36322" title="Click to go to the Author Index">Lu, Xiong</a></td><td class="r">Tongji Unviersity</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35862" title="Click to go to the Author Index">Liu, Wei</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15432" title="Click to go to the Author Index">Yu, Zhuoping</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab363" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Sensor_and_Data_Fusion" title="Click to go to the Keyword Index">Sensor and Data Fusion</a></span><br>
                              <strong>Abstract:</strong> In this paper, an estimation method for attitude and lateral velocity for automated vehicle has been proposed using a six degree of freedom inertial measurement unit(IMU) aided by vehicle dynamics. This estimation method makes full use of the advantage of the IMU and vehicle dynamics and could run autonomously without aid from other outside information such as GNSS or camera. Based on Kalman filter theory, three observers have been developed: a kinematic model based attitude observer for pitch angle and roll angle using IMU, a kinematic model based lateral velocity observer for lateral velocity using IMU and a dynamic model based observer using vehicle dynamics. In small excitation condition, the estimated lateral velocity from dynamic model based observer is more reliable and it is forwarded to the two kinematic model based observers to prevent the accumulated estimation error. In larger excitation condition, the two kinematic model based observers run in open-loop mode. Slalom maneuver and double lane change(DLC) maneuvers have been conducted to validated the estimation method. The experiment results have proved the effectiveness of this estimation method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst13_02">16:00-18:00, Paper ThPS-SST13.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0543.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('543'); return false" title="Click to show or hide the keywords and abstract">Research on Modeling of Distributed Compound Braking System and Braking Force Allocation Strategy (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37454" title="Click to go to the Author Index">Yao, Jun</a></td><td class="r">State Key Lab. of Automotive Simulation and Control, Jilin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36620" title="Click to go to the Author Index">Chen, Guoying</a></td><td class="r">Jilin Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36621" title="Click to go to the Author Index">Zong, Changfu</a></td><td class="r">State Key Lab. of Automotive Simulation and Control, Jilin</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37456" title="Click to go to the Author Index">Guo, Yaohua</a></td><td class="r">Zhengzhou Yutong Bus Co. Ltd</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab543" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Eco_driving_and_Energy_efficient_Vehicles" title="Click to go to the Keyword Index">Eco-driving and Energy-efficient Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Electric_and_Hybrid_Technologies" title="Click to go to the Keyword Index">Electric and Hybrid Technologies</a></span><br>
                              <strong>Abstract:</strong> This paper proposes a novel braking force allocation strategy for electric vehicles with four in-wheel motors (IWM). Based on the principle of maximizing the regenerative braking torque of four motors, the motor and the hydraulic braking force of both front and rear axles have been allocated to four wheels respectively, and the fuzzy control method is adopted to control the vehicle’s slip rate under the emergency braking. The paper first analyzes two common regenerative braking control methods of IWM: half-bridge modulation and full-bridge modulation, and compares the modulation effect of the two control methods. Then, the braking force allocation strategy is proposed, which can be divided into two parts: under normal braking and under emergency braking. Finally, the braking force allocation strategy is verified through the co-simulation platform of Carsim, Matlab/Simulink and AMEsim. The simulation results show that under normal braking, the proposed braking force allocation strategy can preferentially adopt the motor braking force according to the maximum motor braking force. When the total braking force provided by the motors cannot meet the vehicle's braking severity requirements, EHB system involves in braking to provide the remaining braking force. Under emergency braking, the slip rate can be well controlled by the fuzzy control method.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst13_03">16:00-18:00, Paper ThPS-SST13.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0275.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('275'); return false" title="Click to show or hide the keywords and abstract">State of Charge Estimation Based on State of Health Correction for Lithium-Ion Batteries (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36681" title="Click to go to the Author Index">Zhu, Yiduo</a></td><td class="r">Wuhan Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36683" title="Click to go to the Author Index">Yan, Fuwu</a></td><td class="r">Wuhan Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36685" title="Click to go to the Author Index">Kang, Jianqiang</a></td><td class="r">Wuhan Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35977" title="Click to go to the Author Index">Du, Changqing</a></td><td class="r">Wuhan Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab275" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Electric_and_Hybrid_Technologies" title="Click to go to the Keyword Index">Electric and Hybrid Technologies</a></span><br>
                              <strong>Abstract:</strong> In most studies, the state of health (SOH) effect is rarely considered in state of charge (SOC) estimation of the battery. The estimation error gradually increases in the late decline of lithium battery. In this study, the SOC estimation method based on SOH correction and the back-propagation neural network optimized by mind evolutionary algorithm (MEA) is proposed. First, SOH is estimated based on Thevenin battery model and BP neural network. Then, together with the current, voltage and temperature, the SOH is added to the input of the BP neural network, battery capacity can be estimated as the output of the neural network. The next, the initial weights and thresholds of the BP neural network are optimized by the MEA algorithm to achieve better estimation results. The application range of SOC estimation method is further broadened for both the new and old battery.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst13_04">16:00-18:00, Paper ThPS-SST13.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0210.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('210'); return false" title="Click to show or hide the keywords and abstract">Intelligent Vehicle Sideslip Angle Estimation Considering Measurement Signals Delay (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35862" title="Click to go to the Author Index">Liu, Wei</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36322" title="Click to go to the Author Index">Lu, Xiong</a></td><td class="r">Tongji Unviersity</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36542" title="Click to go to the Author Index">Xia, Xin</a></td><td class="r">School of Automotive Studies, Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15432" title="Click to go to the Author Index">Yu, Zhuoping</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab210" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Information_Fusion" title="Click to go to the Keyword Index">Information Fusion</a></span><br>
                              <strong>Abstract:</strong> Considering vehicle sideslip angle estimation difficulty under severe driving conditions with dynamic model based method due to vehicle nonlinear characteristic and parameter uncertainty, a novel kinematic model based method is proposed with the fusion of intelligent vehicle sensors. The state space models of the vehicle yaw angle and the roll angle are constructed based on the IMU and the lateral arrangement of the dual-GPS. In order to reduce the weight of the previously estimated value, an adaptive fading Kalman filtering algorithm is adopted to improve the filtering effect on the yaw and roll angle. A nonlinear adaptive observer is constructed to estimate the vehicle sideslip angle with the integration of the road line information from the camera, velocity from the GPS and acceleration/ angular velocity from the IMU. Furthermore, compared with the IMU, the information obtained from the GPS and the camera can’t be utilized directly as large measurement delay. Thus, an observer-predictor is developed with multi-sensor fusion to handle the measurement delay problem. Finally, the proposed algorithm is validated through co-simulation under different maneuvers.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst13_05">16:00-18:00, Paper ThPS-SST13.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0638.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('638'); return false" title="Click to show or hide the keywords and abstract">Tire-Model-Free Control for Steering of Skid Steering Vehicle (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37283" title="Click to go to the Author Index">Meng, Haolan</a></td><td class="r">TONGJI Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36322" title="Click to go to the Author Index">Lu, Xiong</a></td><td class="r">Tongji Unviersity</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37627" title="Click to go to the Author Index">Gao, Letian</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15432" title="Click to go to the Author Index">Yu, Zhuoping</a></td><td class="r">Tongji Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37628" title="Click to go to the Author Index">Zhang, Renxie</a></td><td class="r">Tongji Univ</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab638" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a></span><br>
                              <strong>Abstract:</strong> Skid steering vehicle has larger wheel slip ratio when steering than straight driving. Due to the lack of accurate tire model in practical application, the slip coefficient is difficult to be quantitatively studied. However, there would be a large steady state error on the skid steering vehicle, which using the wheel speed difference control if the coefficient was ignored. Aiming at this problem, a controller overcoming integral saturation for steering control based on proportional integral control method is designed, which can correct the wheel slip coefficient in real time without the tire model. In the end, the effectiveness of the algorithm is verified by real vehicle test.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thps-sst14"><b>ThPS-SST14</b></a></td>
               <td class="r">BoSiDeng Hall</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thps-sst14" title="Click to go to the Program at a Glance"><b>Parallel-Driving-Based Control Optimization of Intelligent Electrified
<br>Vehicles: From CPS to CPSS</b></a></td>
               <td class="r">Special Session</td>
             </tr>
            


<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst14_01">16:00-18:00, Paper ThPS-SST14.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0398.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('398'); return false" title="Click to show or hide the keywords and abstract">A Novel Control Framework of Haptic Take-Over System for Automated Vehicles (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30472" title="Click to go to the Author Index">Lv, Chen</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37011" title="Click to go to the Author Index">Wang, Huaji</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34596" title="Click to go to the Author Index">Cao, Dongpu</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37012" title="Click to go to the Author Index">Zhao, Yifan</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37015" title="Click to go to the Author Index">Sullman, Mark</a></td><td class="r">Middle East Tech. Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37016" title="Click to go to the Author Index">Auger, Daniel</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37018" title="Click to go to the Author Index">Brighton, James</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37019" title="Click to go to the Author Index">Matthias, Rebecca</a></td><td class="r">Jaguar Land Rover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#34557" title="Click to go to the Author Index">Skrypchuk, Lee</a></td><td class="r">Jaguar Land Rover</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#16351" title="Click to go to the Author Index">Mouzakitis, Alexandros</a></td><td class="r">Jaguar Land Rover</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab398" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a>, <a href="IV2018_KeywordIndexMedia.html#Human_Factors_and_Human_Machine_Interaction" title="Click to go to the Keyword Index">Human Factors and Human Machine Interaction</a>, <a href="IV2018_KeywordIndexMedia.html#Driver_State_and_Intent_Recognition" title="Click to go to the Keyword Index">Driver State and Intent Recognition</a></span><br>
                              <strong>Abstract:</strong> Autonomous driving presents an exciting new development in vehicle technology. It poses a new challenge in driver-automation collaboration particularly during handover transitions between human and machine. In order to deal with this problem, this paper proposes a novel control framework for the haptic take-over system. The high-level framework of the haptic take-over control system, which takes driver cognitive workload, neuromuscular dynamics and optimal trajectory planning into consideration, is developed. Under the proposed framework, the determination approach of the optimal input sequence is introduced. The model of the allowed driver take-over authority, which is associated with driver’s cognitive workload, as well as muscle readiness during take-over, is investigated and developed. The haptic feedback torque controller is then designed so as to minimize the deviation between the allowed control authority and driver’s current degree of participation. A handover process, along with the proposed take-over control method, is also simulated. The simulation results validate the feasibility and effectiveness of the proposed approach.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst14_02">16:00-18:00, Paper ThPS-SST14.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0670.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('670'); return false" title="Click to show or hide the keywords and abstract">Local Path Planning for Autonomous Vehicles: Crash Mitigation (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37765" title="Click to go to the Author Index">Wang, Hong</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#35934" title="Click to go to the Author Index">Huang, Yanjun</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#28368" title="Click to go to the Author Index">Khajepour, Amir</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36040" title="Click to go to the Author Index">Liu, Teng</a></td><td class="r">Univ. of Waterloo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36433" title="Click to go to the Author Index">Qin, Yechen</a></td><td class="r">Beijing Inst. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#37806" title="Click to go to the Author Index">Zhang, Yubiao</a></td><td class="r">Univ. of Waterloo</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab670" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Autonomous___Intelligent_Robotic_Vehicles" title="Click to go to the Keyword Index">Autonomous / Intelligent Robotic Vehicles</a></span><br>
                              <strong>Abstract:</strong> A path planning approach to generate a path which mitigates the effects of an inevitable crash for autonomous vehicles is presented in this brief. The model predictive control algorithm is adopted here for path planning. The artificial potential field, which describes the obstacles and the potential crash severity, are added to the control objectives to avoid the obstacle, and also to mitigate the inevitable crash. The vehicle dynamic is also considered as an optimal control objective. Based on the analysis above, the model predictive controller can guarantee the command following, obstacle avoidance, vehicle dynamics, and mitigate the inevitable crash. Simulation results verified that the proposed MPC has the abilities of obstacles avoidance and mitigation of the inevitable crash.
                           </div>
                        </td>
                     </tr>
                  
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="thps-sst14_03">16:00-18:00, Paper ThPS-SST14.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0065.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('65'); return false" title="Click to show or hide the keywords and abstract">Intelligent Synthesis of Driving Cycle for Advanced Design and Control of Powertrains (I)</a></span></td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#36025" title="Click to go to the Author Index">Zhao, Bolin</a></td><td class="r">Ricardo</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#19073" title="Click to go to the Author Index">Hofman, Theo</a></td><td class="r">Eindhoven Univ. of Tech</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#30472" title="Click to go to the Author Index">Lv, Chen</a></td><td class="r">Cranfield Univ</td></tr>
<tr><td><a href="IV2018_AuthorIndexMedia.html#15486" title="Click to go to the Author Index">Steinbuch, M.</a></td><td class="r">Eindhoven Univ. of Tech</td></tr>

                     <tr>
                        <td colspan="2" style="padding: 0px">
                           <div id="Ab65" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                              
                              <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IV2018_KeywordIndexMedia.html#Electric_and_Hybrid_Technologies" title="Click to go to the Keyword Index">Electric and Hybrid Technologies</a>, <a href="IV2018_KeywordIndexMedia.html#Vehicle_Control" title="Click to go to the Keyword Index">Vehicle Control</a>, <a href="IV2018_KeywordIndexMedia.html#Automated_Vehicles" title="Click to go to the Keyword Index">Automated Vehicles</a></span><br>
                              <strong>Abstract:</strong> As a important input for the simulation and design process of powertrain, a driving cycle needs to be representative of real-world driving behavior. For the purpose of reducing the time consumption in the simulation, a novel modeling method is required to get a representative short driving cycle from the driving datasets. In this paper, a stochastic model based driving cycle synthesis is introduced. The Markov Chain process is combined with a transition probability extracted from the input driving data to determine the next possible state of the vehicle. Speci&#64257;cally, the velocity and slope are generated simultaneously using a three-dimensional Markov Chain model. After the generation process, the result is validated by selected criteria. Furthermore, this synthesis can generate the driving cycle with the desired length to compress the original driving cycle. The results show that the successful compression of the driving cycle can be tested for the fuel economic in the powertrain simulation. At last, the standard deviation of acceleration is found that has a positive correlation of the compression capability of the driving cycle.
                           </div>
                        </td>
                     </tr>
                  
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thcbd"><b>ThCBD</b></a></td>
               <td class="r">Conference Center</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thcbd" title="Click to go to the Program at a Glance"><b>Conference Banquet</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="thed"><b>ThED</b></a></td>
               <td class="r">Room T20</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IV2018_ProgramAtAGlanceMedia.html#thed" title="Click to go to the Program at a Glance"><b>Exhibition & Demonstration-28June</b></a></td>
               <td class="r">Conference Event</td>
             </tr>
            


</table>
</div>

<p>&nbsp;<br>&nbsp;</p><p>&nbsp;<br>&nbsp;</p>


</div>

</body>

</html>
